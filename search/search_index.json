{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fed-PyDESeq2 documentation","text":"<p>This package is a federated python implementation of the DESeq2 method <sup>1</sup> for differential expression analysis (DEA) with bulk RNA-seq data, originally in R. This federated implementation is based on Substra, an open source federated learning software.</p> <p>This package is based on (and benchmarked against) PyDESeq2 <sup>2</sup>, which is a python re-implementation of DESeq2.</p> <p>Currently, available features broadly correspond to the default settings of DESeq2 (v1.34.0) for single-factor and multi-factor analysis (with categorical or continuous factors) using Wald tests, without the LFC shrinkage step.</p>"},{"location":"#citing-this-work","title":"Citing this work","text":"<pre><code>@article{muzellec2024fedpydeseq2,\n  title={FedPyDESeq2: a federated framework for bulk RNA-seq differential expression analysis},\n  author={Muzellec, Boris and Marteau-Ferey, Ulysse and Marchand, Tanguy},\n  journal={bioRxiv},\n  pages={2024--12},\n  year={2024},\n  publisher={Cold Spring Harbor Laboratory}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>FedPyDESeq2 is released under an MIT license.</p> <ol> <li> <p>Michael I Love, Wolfgang Huber, and Simon Anders. Moderated estimation of fold change and dispersion for rna-seq data with deseq2. Genome biology, 15(12):1\u201321, 2014. doi:10.1186/s13059-014-0550-8.\u00a0\u21a9</p> </li> <li> <p>Boris Muzellec, Maria Telenczuk, Vincent Cabeli, and Mathieu Andreux. Pydeseq2: a python package for bulk rna-seq differential expression analysis. Bioinformatics, 2023. doi:10.1093/bioinformatics/btad547.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/","title":"The <code>fedpydeseq2</code> main module","text":"<p>In this part of the documentation, you will find all references to the API, in the different sections on the left.</p>"},{"location":"api/fedpydeseq2_pipeline/","title":"Running an experiment","text":""},{"location":"api/fedpydeseq2_pipeline/#fedpydeseq2.fedpydeseq2_pipeline.run_fedpydeseq2_experiment","title":"<code>run_fedpydeseq2_experiment(n_centers=2, backend='subprocess', register_data=False, simulate=True, asset_directory=None, centers_root_directory=None, compute_plan_name='FedPyDESeq2Experiment', dataset_name='MyDatasetName', remote_timeout=86400, clean_models=True, save_filepath=None, credentials_path=None, dataset_datasamples_keys_path=None, cp_id_path=None, parameter_file=None, fedpydeseq2_wheel_path=None, logging_configuration_file_path=None, **kwargs)</code>","text":"<p>Run a federated experiment using the DESeq2 strategy.</p> <p>Parameters:</p> Name Type Description Default <code>n_centers</code> <code>int</code> <p>Number of centers to use in the federated experiment.</p> <code>2</code> <code>backend</code> <code>BackendType</code> <p>Backend to use for the experiment. Should be one of \"subprocess\", \"docker\" or \"remote\".</p> <code>'subprocess'</code> <code>register_data</code> <code>bool</code> <p>Whether to register the data on the substra platform. Can be True only when using the remote backend.</p> <code>False</code> <code>simulate</code> <code>bool</code> <p>Whether to simulate the experiment. If True, the experiment will be simulated and no data will be sent to the centers. This can be True only in subprocess backend.</p> <code>True</code> <code>asset_directory</code> <code>Path</code> <p>Path to the directory containing the assets (opener.py and description.md).</p> <code>None</code> <code>centers_root_directory</code> <code>Path</code> <p>Path to the directory containing the centers data. Can be None only in remote mode when register_data is False. The centers data should be organized as follows: <pre><code>&lt;centers_root_directory&gt;\n\u251c\u2500\u2500 center_0\n\u2502   \u251c\u2500\u2500 counts_data.csv\n\u2502   \u2514\u2500\u2500 metadata.csv\n\u251c\u2500\u2500 center_1\n\u2502   \u251c\u2500\u2500 counts_data.csv\n\u2502   \u2514\u2500\u2500 metadata.csv\n\u2514\u2500\u2500\n</code></pre> where the metadata.csv file is indexed by sample barcodes and contains all columns needed to build the design matrix, and the counts_data.csv file represents a dataframe with gene names as columns and sample barcodes as rows, in the \"barcode\" column.</p> <code>None</code> <code>compute_plan_name</code> <code>str</code> <p>Name of the compute plan to use for the experiment.</p> <code>'FedPyDESeq2Experiment'</code> <code>dataset_name</code> <code>str</code> <p>Name of the dataset to fill in the Dataset schema.</p> <code>'MyDatasetName'</code> <code>remote_timeout</code> <code>int</code> <p>Timeout in seconds for the remote backend.</p> <code>86400</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the experiment.</p> <code>True</code> <code>save_filepath</code> <code>str or Path</code> <p>Path to save the results of the experiment.</p> <code>None</code> <code>credentials_path</code> <code>str or Path</code> <p>Path to the file containing the credentials to use for the remote backend.</p> <code>None</code> <code>dataset_datasamples_keys_path</code> <code>str or Path</code> <p>Path to the file containing the datasamples keys of the dataset. Only used for the remote backend. Is filled in if register_data is True, and read if register_data is False.</p> <code>None</code> <code>cp_id_path</code> <code>str or Path</code> <p>Path to the file containing the compute plan id. This file is a yaml file with the following structure: <pre><code>algo_org_name: str\ncredentials_path: str\ncompute_plan_key: str\n</code></pre></p> <code>None</code> <code>parameter_file</code> <code>str or Path</code> <p>If not None, yaml file containing the parameters to pass to the DESeq2Strategy. If None, the default parameters are used.</p> <code>None</code> <code>fedpydeseq2_wheel_path</code> <code>str or Path</code> <p>Path to the wheel file of the fedpydeseq2 package. If provided and the backend is remote, this wheel will be added to the dependencies.</p> <code>None</code> <code>logging_configuration_file_path</code> <code>str or Path or None</code> <p>Path to the logging configuration file. Only used in subprocess backend.</p> <code>None</code> <code>**kwargs</code> <p>Arguments to pass to the DESeq2Strategy. They will overwrite those specified in the parameter_file if the file is not None.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Result of the strategy, which are assumed to be contained in the results attribute of the last round of the aggregation node.</p> Source code in <code>fedpydeseq2/fedpydeseq2_pipeline.py</code> <pre><code>def run_fedpydeseq2_experiment(\n    n_centers: int = 2,\n    backend: BackendType = \"subprocess\",\n    register_data: bool = False,\n    simulate: bool = True,\n    asset_directory: Path | None = None,\n    centers_root_directory: Path | None = None,\n    compute_plan_name: str = \"FedPyDESeq2Experiment\",\n    dataset_name: str = \"MyDatasetName\",\n    remote_timeout: int = 86400,  # 24 hours\n    clean_models: bool = True,\n    save_filepath: str | Path | None = None,\n    credentials_path: str | Path | None = None,\n    dataset_datasamples_keys_path: str | Path | None = None,\n    cp_id_path: str | Path | None = None,\n    parameter_file: str | Path | None = None,\n    fedpydeseq2_wheel_path: str | Path | None = None,\n    logging_configuration_file_path: str | Path | None = None,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"Run a federated experiment using the DESeq2 strategy.\n\n    Parameters\n    ----------\n    n_centers : int\n        Number of centers to use in the federated experiment.\n\n    backend : BackendType\n        Backend to use for the experiment. Should be one of \"subprocess\", \"docker\"\n        or \"remote\".\n\n    register_data : bool\n        Whether to register the data on the substra platform. Can be True only\n        when using the remote backend.\n\n    simulate : bool\n        Whether to simulate the experiment. If True, the experiment will be simulated\n        and no data will be sent to the centers. This can be True only in subprocess\n        backend.\n\n    asset_directory : Path\n        Path to the directory containing the assets (opener.py and description.md).\n\n    centers_root_directory : Path, optional\n        Path to the directory containing the centers data. Can be None only in remote\n        mode when register_data is False.\n        The centers data should be organized as follows:\n        ```\n        &lt;centers_root_directory&gt;\n        \u251c\u2500\u2500 center_0\n        \u2502   \u251c\u2500\u2500 counts_data.csv\n        \u2502   \u2514\u2500\u2500 metadata.csv\n        \u251c\u2500\u2500 center_1\n        \u2502   \u251c\u2500\u2500 counts_data.csv\n        \u2502   \u2514\u2500\u2500 metadata.csv\n        \u2514\u2500\u2500\n\n        ```\n        where the metadata.csv file is indexed by sample barcodes and contains\n        all columns needed to build the design matrix, and the counts_data.csv file\n        represents a dataframe with gene names as columns and sample barcodes as rows,\n        in the \"barcode\" column.\n\n    compute_plan_name : str\n        Name of the compute plan to use for the experiment.\n\n    dataset_name : str\n        Name of the dataset to fill in the Dataset schema.\n\n    remote_timeout : int\n        Timeout in seconds for the remote backend.\n\n    clean_models : bool\n        Whether to clean the models after the experiment.\n\n    save_filepath : str or Path\n        Path to save the results of the experiment.\n\n    credentials_path : str or Path\n        Path to the file containing the credentials to use for the remote backend.\n\n    dataset_datasamples_keys_path : str or Path\n        Path to the file containing the datasamples keys of the dataset.\n        Only used for the remote backend.\n        Is filled in if register_data is True, and read if register_data is False.\n\n    cp_id_path : str or Path, optional\n        Path to the file containing the compute plan id.\n        This file is a yaml file with the following structure:\n        ```\n        algo_org_name: str\n        credentials_path: str\n        compute_plan_key: str\n        ```\n\n    parameter_file : str or Path, optional\n        If not None, yaml file containing the parameters to pass to the DESeq2Strategy.\n        If None, the default parameters are used.\n\n    fedpydeseq2_wheel_path : str or Path, optional\n        Path to the wheel file of the fedpydeseq2 package. If provided and the backend\n        is remote, this wheel will be added to the dependencies.\n\n    logging_configuration_file_path : str or Path or None\n        Path to the logging configuration file.\n        Only used in subprocess backend.\n\n    **kwargs\n        Arguments to pass to the DESeq2Strategy. They will overwrite those specified\n        in the parameter_file if the file is not None.\n\n    Returns\n    -------\n    dict\n        Result of the strategy, which are assumed to be contained in the\n        results attribute of the last round of the aggregation node.\n    \"\"\"\n    if backend != \"subprocess\" and logging_configuration_file_path is not None:\n        logger.warning(\n            \"logging_configuration_file_path is only used in subprocess backend.\"\n            \"It will be ignored.\"\n        )\n        logging_configuration_file_path = None\n\n    set_log_config_path(logging_configuration_file_path)\n\n    if parameter_file is not None:\n        with open(parameter_file, \"rb\") as file:\n            parameters = yaml.load(file, Loader=yaml.FullLoader)\n    else:\n        parameters = {}\n    parameters.update(kwargs)\n    strategy = DESeq2Strategy(**parameters)\n\n    return run_federated_experiment(\n        strategy=strategy,\n        n_centers=n_centers,\n        backend=backend,\n        register_data=register_data,\n        simulate=simulate,\n        centers_root_directory=centers_root_directory,\n        assets_directory=asset_directory,\n        compute_plan_name=compute_plan_name,\n        dataset_name=dataset_name,\n        remote_timeout=remote_timeout,\n        clean_models=clean_models,\n        save_filepath=save_filepath,\n        credentials_path=credentials_path,\n        dataset_datasamples_keys_path=dataset_datasamples_keys_path,\n        cp_id_path=cp_id_path,\n        fedpydeseq2_wheel_path=fedpydeseq2_wheel_path,\n    )\n</code></pre>"},{"location":"api/substra_utils/","title":"Substra","text":""},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.federated_experiment","title":"<code>federated_experiment</code>","text":""},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.federated_experiment.run_federated_experiment","title":"<code>run_federated_experiment(strategy, n_centers=2, backend='subprocess', register_data=False, simulate=True, centers_root_directory=None, assets_directory=None, compute_plan_name='FedPyDESeq2Experiment', dataset_name='TCGA', remote_timeout=86400, clean_models=True, save_filepath=None, credentials_path=None, dataset_datasamples_keys_path=None, cp_id_path=None, fedpydeseq2_wheel_path=None)</code>","text":"<p>Run a federated experiment with the given strategy.</p> <p>In remote mode, if the data is already registered, the assets_directory and centers_root_directory are not used (register_data=False).</p> <p>Otherwise, the assets_directory and centers_root_directory must be provided. The assets_directory is expected to contain the opener.py and description.md files, used to create the dataset for all centers. The centers_root_directory is expected to contain a subdirectory for each center, in the following form:</p> <pre><code>&lt;centers_root_directory&gt;\n\u251c\u2500\u2500 center_0\n\u251c\u2500\u2500 center_1\n</code></pre> <p>These directories contain the necessary data for each center and are passed to the DataSampleSpec object to register the data to substra.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>ComputePlanBuilder</code> <p>The strategy to use for the federated experiment.</p> required <code>n_centers</code> <code>int</code> <p>The number of centers to use in the experiment.</p> <code>2</code> <code>backend</code> <code>BackendType</code> <p>The backend to use for the experiment. Can be one of \"subprocess\", \"docker\", or \"remote\".</p> <code>'subprocess'</code> <code>register_data</code> <code>bool</code> <p>Whether to register the data. If True, the assets_directory and centers_root_directory must be provided. Can be False only in \"remote\" mode.</p> <code>False</code> <code>simulate</code> <code>bool</code> <p>Whether to simulate the experiment. If True, the experiment must be run in subprocess mode.</p> <code>True</code> <code>centers_root_directory</code> <code>Optional[Path]</code> <p>The path to the root directory containing the data for each center. This is only used if register_data is True.</p> <code>None</code> <code>assets_directory</code> <code>Optional[Path]</code> <p>The path to the assets directory. It must contain the opener.py file and the description.md file. This is only used if register_data is True.</p> <code>None</code> <code>compute_plan_name</code> <code>str</code> <p>The name of the compute plan.</p> <code>'FedPyDESeq2Experiment'</code> <code>dataset_name</code> <code>str</code> <p>The name of the dataset to use, to be passed to the DatasetSpec object and used to create the path of the yaml file storing the dataset and datasample keys.</p> <code>'TCGA'</code> <code>remote_timeout</code> <code>int</code> <p>The timeout for the remote backend in seconds.</p> <code>86400</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the experiment.</p> <code>True</code> <code>save_filepath</code> <code>Optional[Union[str, Path]]</code> <p>The path to save the results. If None, the results are not saved.</p> <code>None</code> <code>credentials_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to the credentials file. By default, will be set to Path(file).parent / \"credentials/credentials.yaml\" This file is used only in remote mode, and is expected to be a dictionary with the following structure: <pre><code>org1:\n    url: \"****\"\n    token: \"****\"\norg2:\n    url: \"****\"\n    token: \"****\"\n...\n</code></pre> The first organization is assumed to be the algorithm provider. The other organizations are the data providers.</p> <code>None</code> <code>dataset_datasamples_keys_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to the file containing the dataset and datasamples keys. If None, and if backend is \"remote\", will be set to Path(file).parent / \"credentials/-datasamples-keys.yaml\" This file is used only in remote mode, and is expected to be a dictionary with the following structure: <pre><code>org_id:\n    dataset_key: \"****\"\n    datasample_key: \"****\"\n...\n</code></pre> Where all data provider org ids are present, and there is only one datasample key per org id. This file is generated if register_data is True and backend is \"remote\". This file is loaded if register_data is False and backend is \"remote\". <code>None</code> <code>cp_id_path</code> <code>str or Path</code> <p>The path to a file where we save the necessary information to retrieve the compute plan. This parameter is only used in remote mode. If None, this information is not saved. If a path is provided, the information is saved in a yaml file with the following structure: <pre><code>compute_plan_key: \"****\"\ncredentials_path: \"****\"\nalgo_org_name: \"****\"\n</code></pre></p> <code>None</code> <code>fedpydeseq2_wheel_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to the wheel file of the fedpydeseq2 package. If provided and the backend is remote, this wheel will be added to the dependencies.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Result of the strategy, which are assumed to be contained in the results attribute of the last round of the aggregation node.</p> Source code in <code>fedpydeseq2/substra_utils/federated_experiment.py</code> <pre><code>def run_federated_experiment(\n    strategy: ComputePlanBuilder,\n    n_centers: int = 2,\n    backend: BackendType = \"subprocess\",\n    register_data: bool = False,\n    simulate: bool = True,\n    centers_root_directory: Path | None = None,\n    assets_directory: Path | None = None,\n    compute_plan_name: str = \"FedPyDESeq2Experiment\",\n    dataset_name: str = \"TCGA\",\n    remote_timeout: int = 86400,  # 24 hours\n    clean_models: bool = True,\n    save_filepath: str | Path | None = None,\n    credentials_path: str | Path | None = None,\n    dataset_datasamples_keys_path: str | Path | None = None,\n    cp_id_path: str | Path | None = None,\n    fedpydeseq2_wheel_path: str | Path | None = None,\n) -&gt; dict:\n    \"\"\"Run a federated experiment with the given strategy.\n\n    In remote mode, if the data is already registered,\n    the assets_directory and centers_root_directory\n    are not used (register_data=False).\n\n    Otherwise, the assets_directory and centers_root_directory must be\n    provided. The assets_directory is expected to contain the opener.py\n    and description.md files, used to create the dataset for all centers.\n    The centers_root_directory is expected to contain a subdirectory for each center,\n    in the following form:\n\n    ```\n    &lt;centers_root_directory&gt;\n    \u251c\u2500\u2500 center_0\n    \u251c\u2500\u2500 center_1\n\n    ```\n\n    These directories contain the necessary data for each center and are passed\n    to the DataSampleSpec object to register the data to substra.\n\n    Parameters\n    ----------\n    strategy : ComputePlanBuilder\n        The strategy to use for the federated experiment.\n\n    n_centers : int\n        The number of centers to use in the experiment.\n\n    backend : BackendType\n        The backend to use for the experiment. Can be one of \"subprocess\",\n        \"docker\", or \"remote\".\n\n    register_data : bool\n        Whether to register the data. If True, the assets_directory and\n        centers_root_directory must be provided.\n        Can be False only in \"remote\" mode.\n\n    simulate : bool\n        Whether to simulate the experiment. If True, the experiment must be run\n        in subprocess mode.\n\n    centers_root_directory : Optional[Path]\n        The path to the root directory containing the data for each center.\n        This is only used if register_data is True.\n\n    assets_directory : Optional[Path]\n        The path to the assets directory. It must contain the opener.py file\n        and the description.md file. This is only used if register_data is True.\n\n    compute_plan_name : str\n        The name of the compute plan.\n\n    dataset_name : str\n        The name of the dataset to use, to be passed to the DatasetSpec object and used\n        to create the path of the yaml file storing the dataset and datasample keys.\n\n    remote_timeout : int\n        The timeout for the remote backend in seconds.\n\n    clean_models : bool\n        Whether to clean the models after the experiment.\n\n    save_filepath : Optional[Union[str, Path]]\n        The path to save the results. If None, the results are not saved.\n\n    credentials_path : Optional[Union[str, Path]]\n        The path to the credentials file. By default, will be set to\n        Path(__file__).parent / \"credentials/credentials.yaml\"\n        This file is used only in remote mode, and is expected to be a dictionary with\n        the following structure:\n        ```\n        org1:\n            url: \"****\"\n            token: \"****\"\n        org2:\n            url: \"****\"\n            token: \"****\"\n        ...\n        ```\n        The first organization is assumed to be the algorithm provider.\n        The other organizations are the data providers.\n\n\n    dataset_datasamples_keys_path : Optional[Union[str, Path]]\n        The path to the file containing the dataset and datasamples keys.\n        If None, and if backend is \"remote\", will be set to\n        Path(__file__).parent / \"credentials/&lt;dataset&gt;-datasamples-keys.yaml\"\n        This file is used only in remote mode, and is expected to be a dictionary with\n        the following structure:\n        ```\n        org_id:\n            dataset_key: \"****\"\n            datasample_key: \"****\"\n        ...\n        ```\n        Where all data provider org ids are present, and there is only one\n        datasample key per org id.\n        This file is generated if register_data is True and backend is \"remote\".\n        This file is loaded if register_data is False and backend is \"remote\".\n\n    cp_id_path : str or Path, optional\n        The path to a file where we save the necessary information to\n        retrieve the compute plan. This parameter\n        is only used in remote mode.\n        If None, this information is not saved.\n        If a path is provided, the information is saved in a yaml file with the\n        following structure:\n        ```\n        compute_plan_key: \"****\"\n        credentials_path: \"****\"\n        algo_org_name: \"****\"\n        ```\n\n    fedpydeseq2_wheel_path : Optional[Union[str, Path]]\n        The path to the wheel file of the fedpydeseq2 package. If provided and the\n        backend is remote, this wheel will be added to the dependencies.\n\n    Returns\n    -------\n    dict\n        Result of the strategy, which are assumed to be contained in the\n        results attribute of the last round of the aggregation node.\n    \"\"\"\n    # %%\n    # Setup\n    # *****\n    # In the following code cell, we define the different\n    #  organizations needed for our FL experiment.\n    # Every computation will run in `subprocess` mode,\n    # where everything runs locally in Python\n    # subprocesses.\n    # Others backend_types are:\n    # \"docker\" mode where computations run locally in docker\n    #  containers\n    # \"remote\" mode where computations run remotely (you need to\n    # have a deployed platform for that)\n    logger.info(\"Setting up organizations...\")\n    n_clients = n_centers + 1\n    if backend == \"remote\":\n        clients_ = [\n            get_client(\n                backend_type=backend,\n                org_name=f\"org{i}\",\n                credentials_path=credentials_path,\n            )\n            for i in range(1, n_clients + 1)\n        ]\n    else:\n        clients_ = [get_client(backend_type=backend) for _ in range(n_clients)]\n\n    clients = {\n        client.organization_info().organization_id: client for client in clients_\n    }\n\n    # Store organization IDs\n    all_orgs_id = list(clients.keys())\n    algo_org_id = all_orgs_id[0]  # Algo provider is defined as the first organization.\n    data_providers_ids = all_orgs_id[\n        1:\n    ]  # Data providers orgs are the remaining organizations.\n\n    # %%\n    # Dataset registration\n    # ====================\n    #\n    # A :ref:`documentation/concepts:Dataset` is composed of an **opener**,\n    # which is a Python script that can load\n    # the data from the files in memory and a description markdown file.\n    # The :ref:`documentation/concepts:Dataset` object itself does not contain\n    #  the data. The proper asset that contains the\n    # data is the **datasample asset**.\n    #\n    # A **datasample** contains a local path to the data. A datasample can be\n    #  linked to a dataset in order to add data to a\n    # dataset.\n    #\n    # Data privacy is a key concept for Federated Learning experiments.\n    # That is why we set\n    # :ref:`documentation/concepts:Permissions` for :ref:`documentation/concepts:Assets`\n    #  to determine how each organization can access a specific asset.\n    #\n    # Note that metadata such as the assets' creation date and the asset owner are\n    #  visible to all the organizations of a\n    # network.\n\n    # Define the path to the asset.\n    if register_data:\n        logger.info(\"Registering the datasets...\")\n    else:\n        logger.info(\"Using pre-registered datasets...\")\n\n    dataset_keys = {}\n    train_datasample_keys = {}\n\n    if dataset_datasamples_keys_path is None:\n        dataset_datasamples_keys_path = (\n            Path(__file__).parent / f\"credentials/{dataset_name}-datasamples-keys.yaml\"\n        )\n    else:\n        dataset_datasamples_keys_path = Path(dataset_datasamples_keys_path)\n\n    if not register_data:\n        # Check that we are in remote mode\n        assert backend == \"remote\", (\n            \"register_data must be True if backend is not remote,\"\n            \"as the datasets can be saved and reused only in remote mode.\"\n            \"If register_data is False, the dataset_datasamples_keys_path \"\n            \"provides the necessary information to load the data which is \"\n            \"already present on each remote organization.\"\n        )\n        # Load the dataset and datasample keys from the file\n        with open(dataset_datasamples_keys_path) as file:\n            dataset_datasamples_keys = yaml.load(file, Loader=yaml.FullLoader)\n        for org_id in data_providers_ids:\n            dataset_keys[org_id] = dataset_datasamples_keys[org_id][\"dataset_key\"]\n            train_datasample_keys[org_id] = dataset_datasamples_keys[org_id][\n                \"datasample_key\"\n            ]\n        logger.info(\"Datasets fetched.\")\n    else:\n        for i, org_id in enumerate(data_providers_ids):\n            client = clients[org_id]\n\n            # In this case, check that the assets_directory is provided\n            assert (\n                assets_directory is not None\n            ), \"assets_directory must be provided if register_data is True\"\n            # In this case, check that the centers_root_directory is provided\n            assert centers_root_directory is not None, (\n                \"centers_root_directory must be provided if\" \"register_data is True\"\n            )\n\n            permissions_dataset = Permissions(public=True, authorized_ids=all_orgs_id)\n\n            # DatasetSpec is the specification of a dataset. It makes sure every field\n            # is well-defined, and that our dataset is ready to be registered.\n            # The real dataset object is created in the add_dataset method.\n            dataset = DatasetSpec(\n                name=dataset_name,\n                data_opener=assets_directory / \"opener.py\",\n                description=assets_directory / \"description.md\",\n                permissions=permissions_dataset,\n                logs_permission=permissions_dataset,\n            )\n            logger.info(\n                f\"Adding dataset to client \"\n                f\"{str(client.organization_info().organization_id)}\"\n            )\n            dataset_keys[org_id] = client.add_dataset(dataset)\n            logger.info(f\"Dataset added. Key: {dataset_keys[org_id]}\")\n            assert dataset_keys[org_id], \"Missing dataset key\"\n            data_sample = DataSampleSpec(\n                data_manager_keys=[dataset_keys[org_id]],\n                path=centers_root_directory / f\"center_{i}\",\n            )\n            if backend == \"remote\":\n                check_datasample_folder(data_sample.path)\n            train_datasample_keys[org_id] = client.add_data_sample(data_sample)\n\n        # Create the dataset and datasample keys file if the backend is remote\n        if backend == \"remote\":\n            dataset_datasamples_dico = {\n                org_id: {\n                    \"dataset_key\": dataset_keys[org_id],\n                    \"datasample_key\": train_datasample_keys[org_id],\n                }\n                for org_id in data_providers_ids\n            }\n            with open(dataset_datasamples_keys_path, \"w\") as file:\n                yaml.dump(dataset_datasamples_dico, file)\n        logger.info(\"Datasets registered.\")\n\n    logger.info(f\"Dataset keys: {dataset_keys}\")\n\n    # %%\n    # Where to train where to aggregate\n    # =================================\n    #\n    # We specify on which data we want to train our model, using\n    # the :ref:`substrafl_doc/api/nodes:TrainDataNode` object.\n    # Here we train on the two datasets that we have registered earlier.\n    #\n    # The :ref:`substrafl_doc/api/nodes:AggregationNode` specifies the\n    #  organization on which the aggregation operation\n    # will be computed.\n\n    aggregation_node = AggregationNode(algo_org_id)\n\n    train_data_nodes = []\n\n    for org_id in data_providers_ids:\n        # Create the Train Data Node (or training task) and save it in a list\n        train_data_node = TrainDataNode(\n            organization_id=org_id,\n            data_manager_key=dataset_keys[org_id],\n            data_sample_keys=[train_datasample_keys[org_id]],\n        )\n        train_data_nodes.append(train_data_node)\n\n    # %%\n    # Running the experiment\n    # **********************\n    #\n    # We now have all the necessary objects to launch our experiment.\n    # Please see a summary below of all the objects we created so far:\n    #\n    # - A :ref:`documentation/references/sdk:Client` to add or retrieve\n    #  the assets of our experiment, using their keys to\n    #   identify them.\n    # - A `Federated Strategy &lt;substrafl_doc/api/strategies:Strategies&gt;`_,\n    #  implementing the pipeline that will be run.\n    # - `Train data nodes &lt;substrafl_doc/api/nodes:TrainDataNode&gt;`_ to\n    # indicate on which data to train.\n    # - An :ref:`substrafl_doc/api/nodes:AggregationNode`, to specify the\n    #  organization on which the aggregation operation\n    #   will be computed.\n    # - An **experiment folder** to save a summary of the operation made.\n    # - The :ref:`substrafl_doc/api/dependency:Dependency` to define the\n    # libraries on which the experiment needs to run.\n\n    # The Dependency object is instantiated in order to install the right\n    #  libraries in the Python environment of each organization.\n\n    algo_deps = get_dependencies(\n        backend_type=backend, fedpydeseq2_wheel_path=fedpydeseq2_wheel_path\n    )\n\n    exp_path = tempfile.mkdtemp()\n\n    if simulate:\n        if backend != \"subprocess\":\n            raise ValueError(\"Simulated experiment can only be run in subprocess mode.\")\n        _, intermediate_train_state, intermediate_state_agg = simulate_experiment(\n            client=clients[algo_org_id],\n            strategy=strategy,\n            train_data_nodes=train_data_nodes,\n            evaluation_strategy=None,\n            aggregation_node=aggregation_node,\n            clean_models=clean_models,\n            num_rounds=strategy.num_round,\n            experiment_folder=exp_path,\n        )\n\n        # Gather results from the aggregation node\n\n        agg_client_id_mask = [\n            w == clients[algo_org_id].organization_info().organization_id\n            for w in intermediate_state_agg.worker\n        ]\n\n        agg_round_id_mask = [\n            r == max(intermediate_state_agg.round_idx)\n            for r in intermediate_state_agg.round_idx\n        ]\n\n        agg_state_idx = np.where(\n            [\n                r and w\n                for r, w in zip(agg_round_id_mask, agg_client_id_mask, strict=False)\n            ]\n        )[0][0]\n\n        fl_results = intermediate_state_agg.state[agg_state_idx].results\n    else:\n        algo_client = clients[algo_org_id]\n\n        compute_plan = execute_experiment(\n            client=algo_client,\n            strategy=strategy,\n            train_data_nodes=train_data_nodes,\n            evaluation_strategy=None,\n            aggregation_node=aggregation_node,\n            num_rounds=strategy.num_round,\n            experiment_folder=exp_path,\n            dependencies=algo_deps,\n            clean_models=clean_models,\n            name=compute_plan_name,\n        )\n\n        compute_plan_key = compute_plan.key\n\n        # Extract the results. The method used here downloads the results from the\n        # training nodes, as we cannot download\n        # results from the aggregation node. Note that it implies an extra step\n        # for the aggregation node to share the result with the training nodes.\n\n        if cp_id_path is not None:\n            cp_id_path = Path(cp_id_path)\n            cp_id_path.parent.mkdir(parents=True, exist_ok=True)\n            with cp_id_path.open(\"w\") as f:\n                yaml.dump(\n                    {\n                        \"compute_plan_key\": compute_plan_key,\n                        \"credentials_path\": credentials_path,\n                        \"algo_org_name\": \"org1\",\n                    },\n                    f,\n                )\n\n        if backend == \"remote\":\n            sleep_time = 60\n            t1 = time.time()\n            finished = False\n            while (time.time() - t1) &lt; remote_timeout:\n                status = algo_client.get_compute_plan(compute_plan_key).status\n                logger.info(\n                    f\"Compute plan status is {status}, after {(time.time() - t1):.2f}s\"\n                )\n                if status == ComputePlanStatus.done:\n                    logger.info(\"Compute plan has finished successfully\")\n                    finished = True\n                    break\n                elif (\n                    status == ComputePlanStatus.failed\n                    or status == ComputePlanStatus.canceled\n                ):\n                    raise ValueError(\"Compute plan has failed\")\n                elif (\n                    status == ComputePlanStatus.doing\n                    or status == ComputePlanStatus.created\n                ):\n                    pass\n                else:\n                    logger.info(\n                        f\"Compute plan status is {status}, this shouldn't \"\n                        f\"happen, sleeping {sleep_time} and retrying \"\n                        f\"until timeout {remote_timeout}\"\n                    )\n                time.sleep(sleep_time)\n            if not finished:\n                raise ValueError(\n                    f\"Compute plan did not finish after {remote_timeout} seconds\"\n                )\n\n        fl_results = download_aggregate_shared_state(\n            client=algo_client,\n            compute_plan_key=compute_plan_key,\n            round_idx=None,\n        )\n    if save_filepath is not None:\n        pkl_save_filepath = Path(save_filepath) / \"fl_result.pkl\"\n        with pkl_save_filepath.open(\"wb\") as f:\n            pkl.dump(fl_results, f)\n\n    return fl_results\n</code></pre>"},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils","title":"<code>utils</code>","text":""},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils.cancel_compute_plan","title":"<code>cancel_compute_plan(cp_id_path)</code>","text":"<p>Cancel a compute plan.</p> <p>We assume that we are in the remote setting.</p> <p>Parameters:</p> Name Type Description Default <code>cp_id_path</code> <code>str or Path</code> <p>Path to the file containing the compute plan id. This file is a yaml file with the following structure: <pre><code>algo_org_name: str\ncredentials_path: str\ncompute_plan_key: str\n</code></pre></p> required Source code in <code>fedpydeseq2/substra_utils/utils.py</code> <pre><code>def cancel_compute_plan(cp_id_path: str | Path):\n    \"\"\"Cancel a compute plan.\n\n    We assume that we are in the remote setting.\n\n    Parameters\n    ----------\n    cp_id_path : str or Path\n        Path to the file containing the compute plan id.\n        This file is a yaml file with the following structure:\n        ```\n        algo_org_name: str\n        credentials_path: str\n        compute_plan_key: str\n        ```\n    \"\"\"\n    try:\n        with open(cp_id_path) as file:\n            conf = yaml.load(file, Loader=yaml.FullLoader)\n\n        algo_org_name = conf[\"algo_org_name\"]\n        credentials_path = conf[\"credentials_path\"]\n        client = get_client(\n            backend_type=\"remote\",\n            org_name=algo_org_name,\n            credentials_path=credentials_path,\n        )\n        compute_plan_key = conf[\"compute_plan_key\"]\n        client.cancel_compute_plan(compute_plan_key)\n    except Exception as e:  # noqa : BLE001\n        print(\n            f\"An error occured while cancelling the compute plan: {e}.\"\n            f\"Maybe it was already cancelled, or never launched ?\"\n        )\n</code></pre>"},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils.check_datasample_folder","title":"<code>check_datasample_folder(datasample_folder)</code>","text":"<p>Sanity check for the datasample folder.</p> <p>Check if the datasample folder contains only two csv files: counts_data.csv and metadata.csv and nothing else.</p> <p>Parameters:</p> Name Type Description Default <code>datasample_folder</code> <code>Path</code> <p>Path to the datasample folder.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the datasample folder does not contain exactly two files named 'counts_data.csv' and 'metadata.csv'.</p> Source code in <code>fedpydeseq2/substra_utils/utils.py</code> <pre><code>def check_datasample_folder(datasample_folder: Path) -&gt; None:\n    \"\"\"Sanity check for the datasample folder.\n\n    Check if the datasample folder contains only two csv files: counts_data.csv\n    and metadata.csv and nothing else.\n\n    Parameters\n    ----------\n    datasample_folder : Path\n        Path to the datasample folder.\n\n    Raises\n    ------\n    ValueError\n        If the datasample folder does not contain exactly two files named\n        'counts_data.csv' and 'metadata.csv'.\n    \"\"\"\n    if not datasample_folder.is_dir():\n        raise ValueError(f\"{datasample_folder} is not a directory.\")\n    files = list(datasample_folder.iterdir())\n    if len(files) != 2:\n        raise ValueError(\n            \"Datasample folder should contain exactly two files, \"\n            f\"found {len(files)}: {files}.\"\n        )\n    if {file.name for file in files} != {\"counts_data.csv\", \"metadata.csv\"}:\n        raise ValueError(\n            \"Datasample folder should contain two csv files named 'counts_data.csv'\"\n            \" and 'metadata.csv'.\"\n        )\n\n    return\n</code></pre>"},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils.get_client","title":"<code>get_client(backend_type, org_name=None, credentials_path=None)</code>","text":"<p>Return a substra client for a given organization.</p> <p>Parameters:</p> Name Type Description Default <code>backend_type</code> <code>str</code> <p>Name of the backend to connect to. Should be \"subprocess\", \"docker\" or \"remote\"</p> required <code>org_name</code> <code>str, optional.</code> <p>Name of the organization to connect to. Required when using remote backend.</p> <code>None</code> <code>credentials_path</code> <code>str or Path</code> <p>Path to the credentials file. By default, will be set to Path(file).parent / \"credentials/credentials.yaml\"</p> <code>None</code> Source code in <code>fedpydeseq2/substra_utils/utils.py</code> <pre><code>def get_client(\n    backend_type: BackendType,\n    org_name: str | None = None,\n    credentials_path: str | Path | None = None,\n) -&gt; Client:\n    \"\"\"Return a substra client for a given organization.\n\n    Parameters\n    ----------\n    backend_type : str\n        Name of the backend to connect to. Should be \"subprocess\", \"docker\" or \"remote\"\n    org_name : str, optional.\n        Name of the organization to connect to. Required when using remote backend.\n    credentials_path : str or Path\n        Path to the credentials file. By default, will be set to\n        Path(__file__).parent / \"credentials/credentials.yaml\"\n    \"\"\"\n    if backend_type not in (\"subprocess\", \"docker\", \"remote\"):\n        raise ValueError(\n            f\"Backend type {backend_type} not supported. Should be one of 'subprocess',\"\n            f\" 'docker' or 'remote'.\"\n        )\n    if backend_type == \"remote\":\n        assert (\n            org_name is not None\n        ), \"Organization name must be provided when using remote backend.\"\n        if credentials_path is not None:\n            credential_path = Path(credentials_path)\n        else:\n            credential_path = Path(__file__).parent / \"credentials/credentials.yaml\"\n\n        with open(credential_path) as file:\n            conf = yaml.load(file, Loader=yaml.FullLoader)\n        if org_name not in conf.keys():\n            raise ValueError(f\"Organization {org_name} not found in credentials file.\")\n        url = conf[org_name][\"url\"]\n        token = conf[org_name][\"token\"]\n\n        logger.info(\n            f\"Connecting to {org_name} \"\n            f\"at {url} using credentials \"\n            f\"from {credential_path}.\"\n        )\n        return Client(url=url, token=token, backend_type=\"remote\")\n    else:\n        return Client(backend_type=backend_type)\n</code></pre>"},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils.get_dependencies","title":"<code>get_dependencies(backend_type, fedpydeseq2_wheel_path=None)</code>","text":"<p>Return a substra Dependency in regard to the backend_type.</p> <p>Parameters:</p> Name Type Description Default <code>backend_type</code> <code>BackendType</code> <p>Name of the backend to connect to. Should be \"subprocess\", \"docker\" or \"remote\"</p> required <code>fedpydeseq2_wheel_path</code> <code>str | Path | None</code> <p>Path to the wheel file of the fedpydeseq2 package. If provided and the backend is remote or docker, this wheel will be used instead of downloading it.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the wheel file cannot be downloaded or found.</p> Source code in <code>fedpydeseq2/substra_utils/utils.py</code> <pre><code>def get_dependencies(\n    backend_type: BackendType,\n    fedpydeseq2_wheel_path: str | Path | None = None,\n) -&gt; Dependency:\n    \"\"\"Return a substra Dependency in regard to the backend_type.\n\n    Parameters\n    ----------\n    backend_type : BackendType\n        Name of the backend to connect to. Should be \"subprocess\", \"docker\" or \"remote\"\n    fedpydeseq2_wheel_path : str | Path | None, optional\n        Path to the wheel file of the fedpydeseq2 package. If provided and the backend\n        is remote or docker, this wheel will be used instead of downloading it.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the wheel file cannot be downloaded or found.\n    \"\"\"\n    # in subprocess the dependency are not used, no need to build the wheel.\n    if backend_type == BackendType.LOCAL_SUBPROCESS:\n        return Dependency()\n\n    if fedpydeseq2_wheel_path:\n        wheel_path = Path(fedpydeseq2_wheel_path)\n        if not wheel_path.exists():\n            raise FileNotFoundError(f\"Provided wheel file not found: {wheel_path}\")\n        logger.info(f\"Using provided wheel path: {wheel_path}\")\n        return Dependency(local_installable_dependencies=[wheel_path])\n    else:\n        raise FileNotFoundError(\n            \"You must provide a wheel path when using a remote backend.\"\n        )\n</code></pre>"},{"location":"api/substra_utils/#fedpydeseq2.substra_utils.utils.get_n_centers_from_datasamples_file","title":"<code>get_n_centers_from_datasamples_file(datasamples_file)</code>","text":"<p>Return the number of centers from a datasamples file.</p> <p>Parameters:</p> Name Type Description Default <code>datasamples_file</code> <code>str | Path</code> <p>Path to the yaml file containing the datasamples keys of the dataset.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of centers in the datasamples file.</p> Source code in <code>fedpydeseq2/substra_utils/utils.py</code> <pre><code>def get_n_centers_from_datasamples_file(datasamples_file: str | Path) -&gt; int:\n    \"\"\"Return the number of centers from a datasamples file.\n\n    Parameters\n    ----------\n    datasamples_file: str or Path\n        Path to the yaml file containing the datasamples keys of the dataset.\n\n    Returns\n    -------\n    int\n        Number of centers in the datasamples file.\n    \"\"\"\n    with open(datasamples_file) as file:\n        dataset_datasamples_keys = yaml.load(file, Loader=yaml.FullLoader)\n    return len(dataset_datasamples_keys)\n</code></pre>"},{"location":"api/core/strategy/","title":"Strategy","text":""},{"location":"api/core/strategy/#fedpydeseq2.core.deseq2_strategy.DESeq2Strategy","title":"<code>DESeq2Strategy</code>","text":"<p>               Bases: <code>ComputePlanBuilder</code>, <code>DESeq2FullPipe</code></p> <p>DESeq2 strategy.</p> <p>This strategy is an implementation of the DESeq2 algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>design_factors</code> <code>str or list</code> <p>Name of the columns of metadata to be used as design variables. If you are using categorical and continuous factors, you must put all of them here.</p> required <code>ref_levels</code> <code>dict</code> <p>An optional dictionary of the form <code>{\"factor\": \"test_level\"}</code> specifying for each factor the reference (control) level against which we're testing, e.g. <code>{\"condition\", \"A\"}</code>. Factors that are left out will be assigned random reference levels. (default: <code>None</code>).</p> <code>None</code> <code>continuous_factors</code> <code>list</code> <p>An optional list of continuous (as opposed to categorical) factors. Any factor not in <code>continuous_factors</code> will be considered categorical (default: <code>None</code>).</p> <code>None</code> <code>contrast</code> <code>list</code> <p>A list of three strings, in the following format: <code>['variable_of_interest', 'tested_level', 'ref_level']</code>. Names must correspond to the metadata data passed to the DeseqDataSet. E.g., <code>['condition', 'B', 'A']</code> will measure the LFC of 'condition B' compared to 'condition A'. For continuous variables, the last two strings should be left empty, e.g. <code>['measurement', '', ''].</code> If None, the last variable from the design matrix is chosen as the variable of interest, and the reference level is picked alphabetically. (default: <code>None</code>).</p> <code>None</code> <code>lfc_null</code> <code>float</code> <p>The (log2) log fold change under the null hypothesis. (default: <code>0</code>).</p> <code>0.0</code> <code>alt_hypothesis</code> <code>str</code> <p>The alternative hypothesis for computing wald p-values. By default, the normal Wald test assesses deviation of the estimated log fold change from the null hypothesis, as given by <code>lfc_null</code>. One of <code>[\"greaterAbs\", \"lessAbs\", \"greater\", \"less\"]</code> or <code>None</code>. The alternative hypothesis corresponds to what the user wants to find rather than the null hypothesis. (default: <code>None</code>).</p> <code>None</code> <code>min_replicates</code> <code>int</code> <p>Minimum number of replicates a condition should have to allow refitting its samples. (default: <code>7</code>).</p> <code>7</code> <code>min_disp</code> <code>float</code> <p>Lower threshold for dispersion parameters. (default: <code>1e-8</code>).</p> <code>1e-08</code> <code>max_disp</code> <code>float</code> <p>Upper threshold for dispersion parameters. Note: The threshold that is actually enforced is max(max_disp, len(counts)). (default: <code>10</code>).</p> <code>10.0</code> <code>grid_batch_size</code> <code>int</code> <p>The number of genes to put in each batch for local parallel processing. (default: <code>100</code>).</p> <code>250</code> <code>grid_depth</code> <code>int</code> <p>The number of grid interval selections to perform (if using GridSearch). (default: <code>3</code>).</p> <code>3</code> <code>grid_length</code> <code>int</code> <p>The number of grid points to use for the grid search (if using GridSearch). (default: <code>100</code>).</p> <code>100</code> <code>num_jobs</code> <code>int</code> <p>The number of jobs to use for local parallel processing in MLE tasks. (default: <code>8</code>).</p> <code>8</code> <code>independent_filter</code> <code>bool</code> <p>Whether to perform independent filtering to correct p-value trends. (default: <code>True</code>).</p> <code>True</code> <code>alpha</code> <code>float</code> <p>P-value and adjusted p-value significance threshold (usually 0.05). (default: <code>0.05</code>).</p> <code>0.05</code> <code>min_mu</code> <code>float</code> <p>The minimum value of the mean parameter mu. (default: <code>0.5</code>).</p> <code>0.5</code> <code>beta_tol</code> <code>float</code> <p>The tolerance for the beta parameter. (default: <code>1e-8</code>). This is used in the IRLS algorithm to stop the iterations when the relative change in the beta parameter is smaller than beta_tol.</p> <code>1e-08</code> <code>max_beta</code> <code>float</code> <p>The maximum value for the beta parameter. (default: <code>30</code>).</p> <code>30</code> <code>irls_num_iter</code> <code>int</code> <p>The number of iterations to perform in the IRLS algorithm. (default: <code>20</code>).</p> <code>20</code> <code>joblib_backend</code> <code>str</code> <p>The backend to use for parallel processing. (default: <code>loky</code>).</p> <code>'loky'</code> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity level of joblib. (default: <code>3</code>).</p> <code>0</code> <code>irls_batch_size</code> <code>int</code> <p>The number of genes to put in each batch for local parallel processing in the IRLS algorithm. (default: <code>100</code>).</p> <code>100</code> <code>PQN_c1</code> <code>float</code> <p>The Armijo line search constant for the prox newton.</p> <code>0.0001</code> <code>PQN_ftol</code> <code>float</code> <p>The functional stopping criterion for the prox newton method (relative error smaller than ftol).</p> <code>1e-07</code> <code>PQN_num_iters_ls</code> <code>int</code> <p>The number of iterations performed in the line search at each prox newton step.</p> <code>20</code> <code>PQN_num_iters</code> <code>int</code> <p>The number of iterations in the prox newton catch of IRLS.</p> <code>100</code> <code>PQN_min_mu</code> <code>float</code> <p>The minimum value for mu in the prox newton method.</p> <code>0.0</code> <code>refit_cooks</code> <code>bool</code> <p>Whether to refit the model after computation of Cooks distance. (default: <code>True</code>).</p> <code>True</code> <code>cooks_filter</code> <code>bool</code> <p>Whether to filter out genes with high Cooks distance in the pvalue computation. (default: <code>True</code>).</p> <code>True</code> <code>save_layers_to_disk</code> <code>bool</code> <p>Whether to save the layers to disk. (default: <code>False</code>). If True, the layers will be saved to disk.</p> <code>False</code> <code>trimmed_mean_num_iter</code> <code>int</code> <p>The number of iterations to use when computing the trimmed mean in a federated way, i.e. the number of dichotomy steps. The default is 40.</p> <code>40</code> Source code in <code>fedpydeseq2/core/deseq2_strategy.py</code> <pre><code>class DESeq2Strategy(ComputePlanBuilder, DESeq2FullPipe):\n    \"\"\"DESeq2 strategy.\n\n    This strategy is an implementation of the DESeq2 algorithm.\n\n    Parameters\n    ----------\n    design_factors : str or list\n        Name of the columns of metadata to be used as design variables.\n        If you are using categorical and continuous factors, you must put\n        all of them here.\n\n    ref_levels : dict, optional\n        An optional dictionary of the form ``{\"factor\": \"test_level\"}``\n        specifying for each factor the reference (control) level against which\n        we're testing, e.g. ``{\"condition\", \"A\"}``. Factors that are left out\n        will be assigned random reference levels. (default: ``None``).\n\n    continuous_factors : list, optional\n        An optional list of continuous (as opposed to categorical) factors. Any factor\n        not in ``continuous_factors`` will be considered categorical\n        (default: ``None``).\n\n    contrast : list, optional\n        A list of three strings, in the following format:\n        ``['variable_of_interest', 'tested_level', 'ref_level']``.\n        Names must correspond to the metadata data passed to the DeseqDataSet.\n        E.g., ``['condition', 'B', 'A']`` will measure the LFC of 'condition B' compared\n        to 'condition A'.\n        For continuous variables, the last two strings should be left empty, e.g.\n        ``['measurement', '', ''].``\n        If None, the last variable from the design matrix is chosen\n        as the variable of interest, and the reference level is picked alphabetically.\n        (default: ``None``).\n\n    lfc_null : float\n        The (log2) log fold change under the null hypothesis. (default: ``0``).\n\n    alt_hypothesis : str, optional\n        The alternative hypothesis for computing wald p-values. By default, the normal\n        Wald test assesses deviation of the estimated log fold change from the null\n        hypothesis, as given by ``lfc_null``.\n        One of ``[\"greaterAbs\", \"lessAbs\", \"greater\", \"less\"]`` or ``None``.\n        The alternative hypothesis corresponds to what the user wants to find rather\n        than the null hypothesis. (default: ``None``).\n\n    min_replicates : int\n        Minimum number of replicates a condition should have\n        to allow refitting its samples. (default: ``7``).\n\n    min_disp : float\n        Lower threshold for dispersion parameters. (default: ``1e-8``).\n\n    max_disp : float\n        Upper threshold for dispersion parameters.\n        Note: The threshold that is actually enforced is max(max_disp, len(counts)).\n        (default: ``10``).\n\n    grid_batch_size : int\n        The number of genes to put in each batch for local parallel processing.\n        (default: ``100``).\n\n    grid_depth : int\n        The number of grid interval selections to perform (if using GridSearch).\n        (default: ``3``).\n\n    grid_length : int\n        The number of grid points to use for the grid search (if using GridSearch).\n        (default: ``100``).\n\n    num_jobs : int\n        The number of jobs to use for local parallel processing in MLE tasks.\n        (default: ``8``).\n\n    independent_filter : bool\n        Whether to perform independent filtering to correct p-value trends.\n        (default: ``True``).\n\n    alpha : float\n        P-value and adjusted p-value significance threshold (usually 0.05).\n        (default: ``0.05``).\n\n    min_mu : float\n        The minimum value of the mean parameter mu. (default: ``0.5``).\n\n    beta_tol : float\n        The tolerance for the beta parameter. (default: ``1e-8``). This is used\n        in the IRLS algorithm to stop the iterations when the relative change in\n        the beta parameter is smaller than beta_tol.\n\n    max_beta : float\n        The maximum value for the beta parameter. (default: ``30``).\n\n    irls_num_iter : int\n        The number of iterations to perform in the IRLS algorithm. (default: ``20``).\n\n    joblib_backend : str\n        The backend to use for parallel processing. (default: ``loky``).\n\n    joblib_verbosity : int\n        The verbosity level of joblib. (default: ``3``).\n\n    irls_batch_size : int\n        The number of genes to put in each batch for local parallel processing in the\n        IRLS algorithm. (default: ``100``).\n\n    PQN_c1 : float\n        The Armijo line search constant for the prox newton.\n\n    PQN_ftol : float\n        The functional stopping criterion for the prox newton method (relative error\n        smaller than ftol).\n\n    PQN_num_iters_ls : int\n        The number of iterations performed in the line search at each prox newton step.\n\n    PQN_num_iters : int\n        The number of iterations in the prox newton catch of IRLS.\n\n    PQN_min_mu : float\n        The minimum value for mu in the prox newton method.\n\n    refit_cooks : bool\n        Whether to refit the model after computation of Cooks distance.\n        (default: ``True``).\n\n    cooks_filter : bool\n        Whether to filter out genes with high Cooks distance in the pvalue computation.\n        (default: ``True``).\n\n    save_layers_to_disk : bool\n        Whether to save the layers to disk. (default: ``False``).\n        If True, the layers will be saved to disk.\n\n    trimmed_mean_num_iter: int\n        The number of iterations to use when computing the trimmed mean\n        in a federated way, i.e. the number of dichotomy steps. The default is\n        40.\n    \"\"\"\n\n    def __init__(\n        self,\n        design_factors: str | list[str],\n        ref_levels: dict[str, str] | None = None,\n        continuous_factors: list[str] | None = None,\n        contrast: list[str] | None = None,\n        lfc_null: float = 0.0,\n        alt_hypothesis: (\n            Literal[\"greaterAbs\", \"lessAbs\", \"greater\", \"less\"] | None\n        ) = None,\n        min_replicates: int = 7,\n        min_disp: float = 1e-8,\n        max_disp: float = 10.0,\n        grid_batch_size: int = 250,\n        grid_depth: int = 3,\n        grid_length: int = 100,\n        num_jobs=8,\n        min_mu: float = 0.5,\n        beta_tol: float = 1e-8,\n        max_beta: float = 30,\n        irls_num_iter: int = 20,\n        joblib_backend: str = \"loky\",\n        joblib_verbosity: int = 0,\n        irls_batch_size: int = 100,\n        independent_filter: bool = True,\n        alpha: float = 0.05,\n        PQN_c1: float = 1e-4,\n        PQN_ftol: float = 1e-7,\n        PQN_num_iters_ls: int = 20,\n        PQN_num_iters: int = 100,\n        PQN_min_mu: float = 0.0,\n        refit_cooks: bool = True,\n        cooks_filter: bool = True,\n        save_layers_to_disk: bool = False,\n        trimmed_mean_num_iter: int = 40,\n        *args,\n        **kwargs,\n    ):\n        # Add all arguments to super init so that they can be retrieved by nodes.\n        super().__init__(\n            design_factors=design_factors,\n            ref_levels=ref_levels,\n            continuous_factors=continuous_factors,\n            contrast=contrast,\n            lfc_null=lfc_null,\n            alt_hypothesis=alt_hypothesis,\n            min_replicates=min_replicates,\n            min_disp=min_disp,\n            max_disp=max_disp,\n            grid_batch_size=grid_batch_size,\n            grid_depth=grid_depth,\n            grid_length=grid_length,\n            num_jobs=num_jobs,\n            min_mu=min_mu,\n            beta_tol=beta_tol,\n            max_beta=max_beta,\n            irls_num_iter=irls_num_iter,\n            joblib_backend=joblib_backend,\n            joblib_verbosity=joblib_verbosity,\n            irls_batch_size=irls_batch_size,\n            independent_filter=independent_filter,\n            alpha=alpha,\n            PQN_c1=PQN_c1,\n            PQN_ftol=PQN_ftol,\n            PQN_num_iters_ls=PQN_num_iters_ls,\n            PQN_num_iters=PQN_num_iters,\n            PQN_min_mu=PQN_min_mu,\n            refit_cooks=refit_cooks,\n            cooks_filter=cooks_filter,\n            trimmed_mean_num_iter=trimmed_mean_num_iter,\n        )\n\n        #### Define hyper parameters ####\n\n        self.min_disp = min_disp\n        self.max_disp = max_disp\n        self.grid_batch_size = grid_batch_size\n        self.grid_depth = grid_depth\n        self.grid_length = grid_length\n        self.min_mu = min_mu\n        self.beta_tol = beta_tol\n        self.max_beta = max_beta\n\n        # Parameters of the IRLS algorithm\n        self.irls_num_iter = irls_num_iter\n        self.min_replicates = min_replicates\n        self.PQN_c1 = PQN_c1\n        self.PQN_ftol = PQN_ftol\n        self.PQN_num_iters_ls = PQN_num_iters_ls\n        self.PQN_num_iters = PQN_num_iters\n        self.PQN_min_mu = PQN_min_mu\n\n        # Parameters for the trimmed mean computation\n        self.trimmed_mean_num_iter = trimmed_mean_num_iter\n\n        #### Stat parameters\n        self.independent_filter = independent_filter\n        self.alpha = alpha\n\n        #### Define job parallelization parameters ####\n\n        self.num_jobs = num_jobs\n        self.joblib_verbosity = joblib_verbosity\n        self.joblib_backend = joblib_backend\n        self.irls_batch_size = irls_batch_size\n\n        #### Define quantities to set the design ####\n\n        # Convert design_factors to list if a single string was provided.\n        self.design_factors = (\n            [design_factors] if isinstance(design_factors, str) else design_factors\n        )\n\n        self.ref_levels = ref_levels\n        self.continuous_factors = continuous_factors\n\n        if self.continuous_factors is not None:\n            self.categorical_factors = [\n                factor\n                for factor in self.design_factors\n                if factor not in self.continuous_factors\n            ]\n        else:\n            self.categorical_factors = self.design_factors\n\n        self.contrast = contrast\n\n        #### Set test parameters ####\n        self.lfc_null = lfc_null\n        self.alt_hypothesis = alt_hypothesis\n\n        #### If we want to refit cooks outliers\n        self.refit_cooks = refit_cooks\n\n        #### Define quantities to compute statistics\n        self.cooks_filter = cooks_filter\n\n        #### Set attributes to be registered / saved later on ####\n        self.local_adata: ad.AnnData | None = None\n        self.refit_adata: ad.AnnData | None = None\n        self.results: dict | None = None\n\n        #### Save layers to disk\n        self.save_layers_to_disk = save_layers_to_disk\n\n    def build_compute_plan(\n        self,\n        train_data_nodes: list[TrainDataNode],\n        aggregation_node: AggregationNode,\n        evaluation_strategy=None,\n        num_rounds=None,\n        clean_models=True,\n    ):\n        \"\"\"Build the computation graph to run a FedDESeq2 pipe.\n\n        Parameters\n        ----------\n        train_data_nodes : list[TrainDataNode]\n            List of the train nodes.\n        aggregation_node : AggregationNode\n            Aggregation node.\n        evaluation_strategy : EvaluationStrategy\n            Not used.\n        num_rounds : int\n            Number of rounds. Not used.\n        clean_models : bool\n            Whether to clean the models after the computation. (default: ``True``).\n        \"\"\"\n        round_idx = 0\n        local_states: dict[str, LocalStateRef] = {}\n\n        self.run_deseq_pipe(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n    @log_save_local_state\n    def save_local_state(self, path: Path) -&gt; None:\n        \"\"\"Save the local state of the strategy.\n\n        Parameters\n        ----------\n        path : Path\n            Path to the file where to save the state. Automatically handled by subtrafl.\n        \"\"\"\n        state_to_save = {\n            \"local_adata\": self.local_adata,\n            \"refit_adata\": self.refit_adata,\n            \"results\": self.results,\n        }\n        with open(path, \"wb\") as file:\n            pkl.dump(state_to_save, file)\n\n    def load_local_state(self, path: Path) -&gt; Any:\n        \"\"\"Load the local state of the strategy.\n\n        Parameters\n        ----------\n        path : Path\n            Path to the file where to load the state from. Automatically handled by\n            subtrafl.\n        \"\"\"\n        with open(path, \"rb\") as file:\n            state_to_load = pkl.load(file)\n\n        self.local_adata = state_to_load[\"local_adata\"]\n        self.refit_adata = state_to_load[\"refit_adata\"]\n        self.results = state_to_load[\"results\"]\n\n        return self\n\n    @property\n    def num_round(self):\n        \"\"\"Return the number of round in the strategy.\n\n        TODO do something clever with this.\n\n        Returns\n        -------\n        int\n            Number of round in the strategy.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"api/core/strategy/#fedpydeseq2.core.deseq2_strategy.DESeq2Strategy.num_round","title":"<code>num_round</code>  <code>property</code>","text":"<p>Return the number of round in the strategy.</p> <p>TODO do something clever with this.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of round in the strategy.</p>"},{"location":"api/core/strategy/#fedpydeseq2.core.deseq2_strategy.DESeq2Strategy.build_compute_plan","title":"<code>build_compute_plan(train_data_nodes, aggregation_node, evaluation_strategy=None, num_rounds=None, clean_models=True)</code>","text":"<p>Build the computation graph to run a FedDESeq2 pipe.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <code>list[TrainDataNode]</code> <p>List of the train nodes.</p> required <code>aggregation_node</code> <code>AggregationNode</code> <p>Aggregation node.</p> required <code>evaluation_strategy</code> <code>EvaluationStrategy</code> <p>Not used.</p> <code>None</code> <code>num_rounds</code> <code>int</code> <p>Number of rounds. Not used.</p> <code>None</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the computation. (default: <code>True</code>).</p> <code>True</code> Source code in <code>fedpydeseq2/core/deseq2_strategy.py</code> <pre><code>def build_compute_plan(\n    self,\n    train_data_nodes: list[TrainDataNode],\n    aggregation_node: AggregationNode,\n    evaluation_strategy=None,\n    num_rounds=None,\n    clean_models=True,\n):\n    \"\"\"Build the computation graph to run a FedDESeq2 pipe.\n\n    Parameters\n    ----------\n    train_data_nodes : list[TrainDataNode]\n        List of the train nodes.\n    aggregation_node : AggregationNode\n        Aggregation node.\n    evaluation_strategy : EvaluationStrategy\n        Not used.\n    num_rounds : int\n        Number of rounds. Not used.\n    clean_models : bool\n        Whether to clean the models after the computation. (default: ``True``).\n    \"\"\"\n    round_idx = 0\n    local_states: dict[str, LocalStateRef] = {}\n\n    self.run_deseq_pipe(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        local_states=local_states,\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n</code></pre>"},{"location":"api/core/strategy/#fedpydeseq2.core.deseq2_strategy.DESeq2Strategy.load_local_state","title":"<code>load_local_state(path)</code>","text":"<p>Load the local state of the strategy.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file where to load the state from. Automatically handled by subtrafl.</p> required Source code in <code>fedpydeseq2/core/deseq2_strategy.py</code> <pre><code>def load_local_state(self, path: Path) -&gt; Any:\n    \"\"\"Load the local state of the strategy.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the file where to load the state from. Automatically handled by\n        subtrafl.\n    \"\"\"\n    with open(path, \"rb\") as file:\n        state_to_load = pkl.load(file)\n\n    self.local_adata = state_to_load[\"local_adata\"]\n    self.refit_adata = state_to_load[\"refit_adata\"]\n    self.results = state_to_load[\"results\"]\n\n    return self\n</code></pre>"},{"location":"api/core/strategy/#fedpydeseq2.core.deseq2_strategy.DESeq2Strategy.save_local_state","title":"<code>save_local_state(path)</code>","text":"<p>Save the local state of the strategy.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file where to save the state. Automatically handled by subtrafl.</p> required Source code in <code>fedpydeseq2/core/deseq2_strategy.py</code> <pre><code>@log_save_local_state\ndef save_local_state(self, path: Path) -&gt; None:\n    \"\"\"Save the local state of the strategy.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the file where to save the state. Automatically handled by subtrafl.\n    \"\"\"\n    state_to_save = {\n        \"local_adata\": self.local_adata,\n        \"refit_adata\": self.refit_adata,\n        \"results\": self.results,\n    }\n    with open(path, \"wb\") as file:\n        pkl.dump(state_to_save, file)\n</code></pre>"},{"location":"api/core/utils/","title":"Core","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.aggregation","title":"<code>aggregation</code>","text":"<p>Aggregation functions.</p> <p>Copy-pasted from the CancerLINQ repo.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.aggregation.aggregate_means","title":"<code>aggregate_means(local_means, n_local_samples, filter_nan=False)</code>","text":"<p>Aggregate local means.</p> <p>Aggregate the local means into a global mean by using the local number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>local_means</code> <code>list[Any]</code> <p>list of local means. Could be array, float, Series.</p> required <code>n_local_samples</code> <code>list[int]</code> <p>list of number of samples used for each local mean.</p> required <code>filter_nan</code> <code>bool</code> <p>Filter NaN values in the local means, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Aggregated mean. Same type of the local means</p> Source code in <code>fedpydeseq2/core/utils/aggregation.py</code> <pre><code>def aggregate_means(\n    local_means: list[Any], n_local_samples: list[int], filter_nan: bool = False\n):\n    \"\"\"Aggregate local means.\n\n    Aggregate the local means into a global mean by using the local number of samples.\n\n    Parameters\n    ----------\n    local_means : list[Any]\n        list of local means. Could be array, float, Series.\n    n_local_samples : list[int]\n        list of number of samples used for each local mean.\n    filter_nan : bool, optional\n        Filter NaN values in the local means, by default False.\n\n    Returns\n    -------\n    Any\n        Aggregated mean. Same type of the local means\n    \"\"\"\n    tot_samples = 0\n    tot_mean = np.zeros_like(local_means[0])\n    for mean, n_sample in zip(local_means, n_local_samples, strict=False):\n        if filter_nan:\n            mean = np.nan_to_num(mean, nan=0, copy=False)\n        tot_mean += mean * n_sample\n        tot_samples += n_sample\n\n    return tot_mean / tot_samples\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.compute_lfc_utils","title":"<code>compute_lfc_utils</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.compute_lfc_utils.get_lfc_utils_from_gene_mask_adata","title":"<code>get_lfc_utils_from_gene_mask_adata(adata, gene_mask, disp_param_name, beta=None, lfc_param_name=None)</code>","text":"<p>Get the necessary data for LFC computations from the local adata and genes.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>gene_mask</code> <code>ndarray</code> <p>The mask of genes to use for the IRLS algorithm. This mask identifies the genes in the non_zero_gene_names. If None, all non zero genes are used.</p> required <code>disp_param_name</code> <code>str</code> <p>The name of the dispersion parameter in the adata.varm.</p> required <code>beta</code> <code>Optional[ndarray]</code> <p>The log fold change values, of shape (n_non_zero_genes,).</p> <code>None</code> <code>lfc_param_name</code> <code>str | None</code> <p>The name of the lfc parameter in the adata.varm. Is incompatible with beta.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>gene_names</code> <code>list[str]</code> <p>The names of the genes to use for the IRLS algorithm.</p> <code>design_matrix</code> <code>ndarray</code> <p>The design matrix.</p> <code>size_factors</code> <code>ndarray</code> <p>The size factors.</p> <code>counts</code> <code>ndarray</code> <p>The count matrix from the local adata.</p> <code>dispersions</code> <code>ndarray</code> <p>The dispersions from the local adata.</p> <code>beta_on_mask</code> <code>ndarray</code> <p>The log fold change values on the mask.</p> Source code in <code>fedpydeseq2/core/utils/compute_lfc_utils.py</code> <pre><code>def get_lfc_utils_from_gene_mask_adata(\n    adata: ad.AnnData,\n    gene_mask: np.ndarray | None,\n    disp_param_name: str,\n    beta: np.ndarray | None = None,\n    lfc_param_name: str | None = None,\n) -&gt; tuple[list[str], np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Get the necessary data for LFC computations from the local adata and genes.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local AnnData object.\n\n    gene_mask : np.ndarray, optional\n        The mask of genes to use for the IRLS algorithm.\n        This mask identifies the genes in the non_zero_gene_names.\n        If None, all non zero genes are used.\n\n    disp_param_name : str\n        The name of the dispersion parameter in the adata.varm.\n\n    beta : Optional[np.ndarray]\n        The log fold change values, of shape (n_non_zero_genes,).\n\n    lfc_param_name: Optional[str]\n        The name of the lfc parameter in the adata.varm.\n        Is incompatible with beta.\n\n    Returns\n    -------\n    gene_names : list[str]\n        The names of the genes to use for the IRLS algorithm.\n    design_matrix : np.ndarray\n        The design matrix.\n    size_factors : np.ndarray\n        The size factors.\n    counts : np.ndarray\n        The count matrix from the local adata.\n    dispersions : np.ndarray\n        The dispersions from the local adata.\n    beta_on_mask : np.ndarray\n        The log fold change values on the mask.\n    \"\"\"\n    # Check that one of beta or lfc_param_name is not None\n    assert (beta is not None) ^ (\n        lfc_param_name is not None\n    ), \"One of beta or lfc_param_name must be not None\"\n\n    # Get non zero genes\n    non_zero_genes_names = adata.var_names[adata.varm[\"non_zero\"]]\n\n    # Get the irls genes\n    if gene_mask is None:\n        gene_names = non_zero_genes_names\n    else:\n        gene_names = non_zero_genes_names[gene_mask]\n\n    # Get beta\n    if lfc_param_name is not None:\n        beta_on_mask = adata[:, gene_names].varm[lfc_param_name].to_numpy()\n    elif gene_mask is not None:\n        assert beta is not None  # for mypy\n        beta_on_mask = beta[gene_mask]\n    else:\n        assert beta is not None  # for mypy\n        beta_on_mask = beta.copy()\n\n    design_matrix = adata.obsm[\"design_matrix\"].values\n    size_factors = adata.obsm[\"size_factors\"]\n    counts = adata[:, gene_names].X\n    dispersions = adata[:, gene_names].varm[disp_param_name]\n\n    return gene_names, design_matrix, size_factors, counts, dispersions, beta_on_mask\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.design_matrix","title":"<code>design_matrix</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.design_matrix.build_design_matrix","title":"<code>build_design_matrix(metadata, design_factors='stage', levels=None, continuous_factors=None, ref_levels=None)</code>","text":"<p>Build design_matrix matrix for DEA.</p> <p>Unless specified, the reference factor is chosen alphabetically. Copied from PyDESeq2, with some modifications specific to fedomics to ensure that all centers have the same columns</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>DataFrame containing metadata information. Must be indexed by sample barcodes.</p> required <code>design_factors</code> <code>str or list</code> <p>Name of the columns of metadata to be used as design_matrix variables. (default: <code>\"condition\"</code>).</p> <code>'stage'</code> <code>levels</code> <code>dict</code> <p>An optional dictionary of lists of strings specifying the levels of each factor in the global design, e.g. <code>{\"condition\": [\"A\", \"B\"]}</code>. (default: <code>None</code>).</p> <code>None</code> <code>ref_levels</code> <code>dict</code> <p>An optional dictionary of the form <code>{\"factor\": \"test_level\"}</code> specifying for each factor the reference (control) level against which we're testing, e.g. <code>{\"condition\", \"A\"}</code>. Factors that are left out will be assigned random reference levels. (default: <code>None</code>).</p> <code>None</code> <code>continuous_factors</code> <code>list</code> <p>An optional list of continuous (as opposed to categorical) factors, that should also be in <code>design_factors</code>. Any factor in <code>design_factors</code> but not in <code>continuous_factors</code> will be considered categorical (default: <code>None</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with experiment design information (to split cohorts). Indexed by sample barcodes.</p> Source code in <code>fedpydeseq2/core/utils/design_matrix.py</code> <pre><code>def build_design_matrix(\n    metadata: pd.DataFrame,\n    design_factors: str | list[str] = \"stage\",\n    levels: dict[str, list[str]] | None = None,\n    continuous_factors: list[str] | None = None,\n    ref_levels: dict[str, str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Build design_matrix matrix for DEA.\n\n    Unless specified, the reference factor is chosen alphabetically.\n    Copied from PyDESeq2, with some modifications specific to fedomics to ensure that\n    all centers have the same columns\n\n    Parameters\n    ----------\n    metadata : pandas.DataFrame\n        DataFrame containing metadata information.\n        Must be indexed by sample barcodes.\n\n    design_factors : str or list\n        Name of the columns of metadata to be used as design_matrix variables.\n        (default: ``\"condition\"``).\n\n    levels : dict, optional\n        An optional dictionary of lists of strings specifying the levels of each factor\n        in the global design, e.g. ``{\"condition\": [\"A\", \"B\"]}``. (default: ``None``).\n\n    ref_levels : dict, optional\n        An optional dictionary of the form ``{\"factor\": \"test_level\"}``\n        specifying for each factor the reference (control) level against which\n        we're testing, e.g. ``{\"condition\", \"A\"}``. Factors that are left out\n        will be assigned random reference levels. (default: ``None``).\n\n    continuous_factors : list, optional\n        An optional list of continuous (as opposed to categorical) factors, that should\n        also be in ``design_factors``. Any factor in ``design_factors`` but not in\n        ``continuous_factors`` will be considered categorical (default: ``None``).\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame with experiment design information (to split cohorts).\n        Indexed by sample barcodes.\n    \"\"\"\n    if isinstance(\n        design_factors, str\n    ):  # if there is a single factor, convert to singleton list\n        design_factors = [design_factors]\n\n    # Check that factors in the design don't contain underscores. If so, convert\n    # them to hyphens\n    if np.any([\"_\" in factor for factor in design_factors]):\n        warnings.warn(\n            \"\"\"Same factor names in the design contain underscores ('_'). They will\n            be converted to hyphens ('-').\"\"\",\n            UserWarning,\n            stacklevel=2,\n        )\n        design_factors = [factor.replace(\"_\", \"-\") for factor in design_factors]\n\n    # Check that level factors in the design don't contain underscores. If so, convert\n    # them to hyphens\n    warning_issued = False\n    for factor in design_factors:\n        if ptypes.is_numeric_dtype(metadata[factor]):\n            continue\n        if np.any([\"_\" in value for value in metadata[factor]]):\n            if not warning_issued:\n                warnings.warn(\n                    \"\"\"Some factor levels in the design contain underscores ('_').\n                    They will be converted to hyphens ('-').\"\"\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                warning_issued = True\n            metadata[factor] = metadata[factor].apply(lambda x: x.replace(\"_\", \"-\"))\n\n    if continuous_factors is not None:\n        for factor in continuous_factors:\n            if factor not in design_factors:\n                raise ValueError(\n                    f\"Continuous factor '{factor}' not in design factors: \"\n                    f\"{design_factors}.\"\n                )\n        categorical_factors = [\n            factor for factor in design_factors if factor not in continuous_factors\n        ]\n    else:\n        categorical_factors = design_factors\n\n    if levels is None:\n        levels = {factor: np.unique(metadata[factor]) for factor in categorical_factors}\n\n    # Check that there is at least one categorical factor\n    if len(categorical_factors) &gt; 0:\n        design_matrix = pd.get_dummies(metadata[categorical_factors], drop_first=False)\n        # Check if there missing levels. If so, add them and set to 0.\n        for factor in categorical_factors:\n            for level in levels[factor]:\n                if f\"{factor}_{level}\" not in design_matrix.columns:\n                    design_matrix[f\"{factor}_{level}\"] = 0\n\n        # Pick the first level as reference. Then, drop the column.\n        for factor in categorical_factors:\n            if ref_levels is not None and factor in ref_levels:\n                ref = ref_levels[factor]\n            else:\n                ref = levels[factor][0]\n\n            ref_level_name = f\"{factor}_{ref}\"\n            design_matrix.drop(ref_level_name, axis=\"columns\", inplace=True)\n\n            # Add reference level as column name suffix\n            design_matrix.columns = [\n                f\"{col}_vs_{ref}\" if col.startswith(factor) else col\n                for col in design_matrix.columns\n            ]\n    else:\n        # There is no categorical factor in the design\n        design_matrix = pd.DataFrame(index=metadata.index)\n\n    # Add the intercept column\n    design_matrix.insert(0, \"intercept\", 1)\n\n    # Convert categorical factors one-hot encodings to int\n    design_matrix = design_matrix.astype(\"int\")\n\n    # Add continuous factors\n    if continuous_factors is not None:\n        for factor in continuous_factors:\n            # This factor should be numeric\n            design_matrix[factor] = pd.to_numeric(metadata[factor])\n    return design_matrix\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers","title":"<code>layers</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers","title":"<code>build_layers</code>","text":"<p>Module to construct the layers.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.cooks","title":"<code>cooks</code>","text":"<p>Module to set the cooks layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.cooks.can_set_cooks_layer","title":"<code>can_set_cooks_layer(adata, shared_state, raise_error=False)</code>","text":"<p>Check if the Cook's distance can be set.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local adata.</p> required <code>shared_state</code> <code>Optional[dict]</code> <p>The shared state containing the Cook's dispersion values.</p> required <code>raise_error</code> <code>bool</code> <p>Whether to raise an error if the Cook's distance cannot be set.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the Cook's distance can be set.</p> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If the Cook's distance cannot be set and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/cooks.py</code> <pre><code>def can_set_cooks_layer(\n    adata: ad.AnnData, shared_state: dict | None, raise_error: bool = False\n) -&gt; bool:\n    \"\"\"Check if the Cook's distance can be set.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local adata.\n\n    shared_state : Optional[dict]\n        The shared state containing the Cook's dispersion values.\n\n    raise_error : bool\n        Whether to raise an error if the Cook's distance cannot be set.\n\n    Returns\n    -------\n    bool:\n        Whether the Cook's distance can be set.\n\n    Raises\n    ------\n    ValueError:\n        If the Cook's distance cannot be set and raise_error is True.\n    \"\"\"\n    if \"cooks\" in adata.layers.keys():\n        return True\n    if shared_state is None:\n        if raise_error:\n            raise ValueError(\n                \"To set cooks layer, there should be \" \"an input shared state\"\n            )\n        else:\n            return False\n    has_non_zero = \"non_zero\" in adata.varm.keys()\n    try:\n        has_hat_diagonals = can_set_hat_diagonals_layer(\n            adata, shared_state, raise_error\n        )\n    except ValueError as hat_diagonals_error:\n        raise ValueError(\n            \"The Cook's distance cannot be set because the hat diagonals cannot be set.\"\n        ) from hat_diagonals_error\n    try:\n        has_mu_LFC = can_set_mu_layer(\n            local_adata=adata,\n            lfc_param_name=\"LFC\",\n            mu_param_name=\"_mu_LFC\",\n        )\n    except ValueError as mu_LFC_error:\n        raise ValueError(\n            \"The Cook's distance cannot be set because the mu_LFC layer cannot be set.\"\n        ) from mu_LFC_error\n    has_X = adata.X is not None\n    has_cooks_dispersions = \"cooks_dispersions\" in shared_state.keys()\n    has_all = (\n        has_non_zero\n        and has_hat_diagonals\n        and has_mu_LFC\n        and has_X\n        and has_cooks_dispersions\n    )\n    if not has_all and raise_error:\n        raise ValueError(\n            \"The Cook's distance cannot be set because \"\n            \"the following conditions are not met:\"\n            f\"\\n- has_non_zero: {has_non_zero}\"\n            f\"\\n- has_hat_diagonals: {has_hat_diagonals}\"\n            f\"\\n- has_mu_LFC: {has_mu_LFC}\"\n            f\"\\n- has_X: {has_X}\"\n            f\"\\n- has_cooks_dispersions: {has_cooks_dispersions}\"\n        )\n    return has_all\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.cooks.set_cooks_layer","title":"<code>set_cooks_layer(adata, shared_state)</code>","text":"<p>Compute the Cook's distance from the shared state.</p> <p>This function computes the Cook's distance from the shared state and stores it in the \"cooks\" layer of the local adata.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local adata.</p> required <code>shared_state</code> <code>dict</code> <p>The shared state containing the Cook's dispersion values.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_layers/cooks.py</code> <pre><code>def set_cooks_layer(\n    adata: ad.AnnData,\n    shared_state: dict | None,\n):\n    \"\"\"Compute the Cook's distance from the shared state.\n\n    This function computes the Cook's distance from the shared state and stores it\n    in the \"cooks\" layer of the local adata.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local adata.\n\n    shared_state : dict\n        The shared state containing the Cook's dispersion values.\n    \"\"\"\n    can_set_cooks_layer(adata, shared_state, raise_error=True)\n    if \"cooks\" in adata.layers.keys():\n        return\n    # set all necessary layers\n    assert isinstance(shared_state, dict)\n    set_mu_layer(adata, lfc_param_name=\"LFC\", mu_param_name=\"_mu_LFC\")\n    set_hat_diagonals_layer(adata, shared_state)\n    num_vars = adata.uns[\"n_params\"]\n    cooks_dispersions = shared_state[\"cooks_dispersions\"]\n    V = (\n        adata[:, adata.varm[\"non_zero\"]].layers[\"_mu_LFC\"]\n        + cooks_dispersions[None, adata.varm[\"non_zero\"]]\n        * adata[:, adata.varm[\"non_zero\"]].layers[\"_mu_LFC\"] ** 2\n    )\n    squared_pearson_res = (\n        adata[:, adata.varm[\"non_zero\"]].X\n        - adata[:, adata.varm[\"non_zero\"]].layers[\"_mu_LFC\"]\n    ) ** 2 / V\n    diag_mul = (\n        adata[:, adata.varm[\"non_zero\"]].layers[\"_hat_diagonals\"]\n        / (1 - adata[:, adata.varm[\"non_zero\"]].layers[\"_hat_diagonals\"]) ** 2\n    )\n    adata.layers[\"cooks\"] = np.full((adata.n_obs, adata.n_vars), np.NaN)\n    adata.layers[\"cooks\"][:, adata.varm[\"non_zero\"]] = (\n        squared_pearson_res / num_vars * diag_mul\n    )\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.fit_lin_mu_hat","title":"<code>fit_lin_mu_hat</code>","text":"<p>Module to reconstruct the fit_lin_mu_hat layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.fit_lin_mu_hat.can_get_fit_lin_mu_hat","title":"<code>can_get_fit_lin_mu_hat(local_adata, raise_error=False)</code>","text":"<p>Check if the fit_lin_mu_hat layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the fit_lin_mu_hat layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the fit_lin_mu_hat layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the fit_lin_mu_hat layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/fit_lin_mu_hat.py</code> <pre><code>def can_get_fit_lin_mu_hat(local_adata: ad.AnnData, raise_error: bool = False) -&gt; bool:\n    \"\"\"Check if the fit_lin_mu_hat layer can be reconstructed.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n\n    raise_error : bool, optional\n        If True, raise an error if the fit_lin_mu_hat layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the fit_lin_mu_hat layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the fit_lin_mu_hat layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"_fit_lin_mu_hat\" in local_adata.layers.keys():\n        return True\n    try:\n        y_hat_ok = can_get_y_hat(local_adata, raise_error=raise_error)\n    except ValueError as y_hat_error:\n        raise ValueError(\n            f\"Error while checking if y_hat can be reconstructed: {y_hat_error}\"\n        ) from y_hat_error\n\n    has_size_factors = \"size_factors\" in local_adata.obsm.keys()\n    has_non_zero = \"non_zero\" in local_adata.varm.keys()\n    if not has_size_factors or not has_non_zero:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain the size_factors obsm \"\n                \"and the non_zero varm to compute the fit_lin_mu_hat layer.\"\n                \" Here are the keys present in the local adata: \"\n                f\"obsm : {local_adata.obsm.keys()} and varm : {local_adata.varm.keys()}\"\n            )\n        return False\n    return y_hat_ok\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.fit_lin_mu_hat.set_fit_lin_mu_hat","title":"<code>set_fit_lin_mu_hat(local_adata, min_mu=0.5)</code>","text":"<p>Calculate the _fit_lin_mu_hat layer using the provided local data.</p> <p>Checks are performed to ensure necessary keys are present in the data.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local anndata object containing necessary keys for computation.</p> required <code>min_mu</code> <code>float</code> <p>The minimum value for mu, defaults to 0.5.</p> <code>0.5</code> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/fit_lin_mu_hat.py</code> <pre><code>def set_fit_lin_mu_hat(local_adata: ad.AnnData, min_mu: float = 0.5):\n    \"\"\"Calculate the _fit_lin_mu_hat layer using the provided local data.\n\n    Checks are performed to ensure necessary keys are present in the data.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local anndata object containing necessary keys for computation.\n    min_mu : float, optional\n        The minimum value for mu, defaults to 0.5.\n    \"\"\"\n    can_get_fit_lin_mu_hat(local_adata, raise_error=True)\n    if \"_fit_lin_mu_hat\" in local_adata.layers.keys():\n        return\n    set_y_hat(local_adata)\n    mu_hat = local_adata.obsm[\"size_factors\"][:, None] * local_adata.layers[\"_y_hat\"]\n    fit_lin_mu_hat = np.maximum(mu_hat, min_mu)\n\n    fit_lin_mu_hat[:, ~local_adata.varm[\"non_zero\"]] = np.nan\n    local_adata.layers[\"_fit_lin_mu_hat\"] = fit_lin_mu_hat\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.hat_diagonals","title":"<code>hat_diagonals</code>","text":"<p>Module to set the hat diagonals layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.hat_diagonals.can_set_hat_diagonals_layer","title":"<code>can_set_hat_diagonals_layer(adata, shared_state, raise_error=False)</code>","text":"<p>Check if the hat diagonals layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object.</p> required <code>shared_state</code> <code>Optional[dict]</code> <p>The shared state dictionary.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the hat diagonals layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the hat diagonals layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the hat diagonals layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/hat_diagonals.py</code> <pre><code>def can_set_hat_diagonals_layer(\n    adata: ad.AnnData, shared_state: dict | None, raise_error: bool = False\n) -&gt; bool:\n    \"\"\"Check if the hat diagonals layer can be reconstructed.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The AnnData object.\n\n    shared_state : Optional[dict]\n        The shared state dictionary.\n\n    raise_error : bool, optional\n        If True, raise an error if the hat diagonals layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the hat diagonals layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the hat diagonals layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"_hat_diagonals\" in adata.layers.keys():\n        return True\n\n    if shared_state is None:\n        if raise_error:\n            raise ValueError(\n                \"To set the _hat_diagonals layer, there\" \"should be a shared state.\"\n            )\n        else:\n            return False\n\n    has_design_matrix = \"design_matrix\" in adata.obsm.keys()\n    has_lfc_param = \"LFC\" in adata.varm.keys()\n    has_size_factors = \"size_factors\" in adata.obsm.keys()\n    has_non_zero = \"non_zero\" in adata.varm.keys()\n    has_dispersion = \"dispersions\" in adata.varm.keys()\n    has_global_hat_matrix_inv = \"global_hat_matrix_inv\" in shared_state.keys()\n\n    has_all = (\n        has_design_matrix\n        and has_lfc_param\n        and has_size_factors\n        and has_non_zero\n        and has_global_hat_matrix_inv\n        and has_dispersion\n    )\n    if not has_all:\n        if raise_error:\n            raise ValueError(\n                \"Adata must contain the design matrix obsm\"\n                \", the LFC varm, the dispersions varm, \"\n                \"the size_factors obsm, the non_zero varm \"\n                \"and the global_hat_matrix_inv \"\n                \"in the shared state to compute the hat diagonals layer.\"\n                \" Here are the keys present in the adata: \"\n                f\"obsm : {adata.obsm.keys()} and varm : {adata.varm.keys()}, and the \"\n                f\"shared state keys: {shared_state.keys()}\"\n            )\n        return False\n    return True\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.hat_diagonals.make_hat_diag_batch","title":"<code>make_hat_diag_batch(beta, global_hat_matrix_inv, design_matrix, size_factors, dispersions, min_mu=0.5)</code>","text":"<p>Compute the H matrix for a batch of LFC estimates.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>ndarray</code> <p>Current LFC estimate, of shape (batch_size, n_params).</p> required <code>global_hat_matrix_inv</code> <code>ndarray</code> <p>The inverse of the global hat matrix, of shape (batch_size, n_params, n_params).</p> required <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <code>dispersions</code> <code>ndarray</code> <p>The dispersions, of shape (batch_size).</p> required <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability. (default: <code>0.5</code>).</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The H matrix, of shape (batch_size, n_obs).</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/hat_diagonals.py</code> <pre><code>def make_hat_diag_batch(\n    beta: np.ndarray,\n    global_hat_matrix_inv: np.ndarray,\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n    dispersions: np.ndarray,\n    min_mu: float = 0.5,\n) -&gt; np.ndarray:\n    \"\"\"Compute the H matrix for a batch of LFC estimates.\n\n    Parameters\n    ----------\n    beta : np.ndarray\n        Current LFC estimate, of shape (batch_size, n_params).\n    global_hat_matrix_inv : np.ndarray\n        The inverse of the global hat matrix, of shape (batch_size, n_params, n_params).\n    design_matrix : np.ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : np.ndarray\n        The size factors, of shape (n_obs).\n    dispersions : np.ndarray\n        The dispersions, of shape (batch_size).\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n        (default: ``0.5``).\n\n    Returns\n    -------\n    np.ndarray\n        The H matrix, of shape (batch_size, n_obs).\n    \"\"\"\n    mu = size_factors[:, None] * np.exp(design_matrix @ beta.T)\n    mu_clipped = np.maximum(\n        mu,\n        min_mu,\n    )\n\n    # W of shape (n_obs, batch_size)\n    W = mu_clipped / (1.0 + mu_clipped * dispersions[None, :])\n\n    # W_sq Of shape (batch_size, n_obs)\n    W_sq = np.sqrt(W).T\n\n    # Inside the diagonal operator is of shape (batch_size, n_obs, n_obs)\n    # The diagonal operator takes the diagonal per gene in the batch\n    # H is therefore of shape (batch_size, n_obs)\n    H = np.diagonal(\n        design_matrix @ global_hat_matrix_inv @ design_matrix.T,\n        axis1=1,\n        axis2=2,\n    )\n\n    H = W_sq * H * W_sq\n\n    return H\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.hat_diagonals.set_hat_diagonals_layer","title":"<code>set_hat_diagonals_layer(adata, shared_state, n_jobs=1, joblib_verbosity=0, joblib_backend='loky', batch_size=100, min_mu=0.5)</code>","text":"<p>Compute the hat diagonals layer from the adata and the shared state.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object.</p> required <code>shared_state</code> <code>Optional[dict]</code> <p>The shared state dictionary. This dictionary must contain the global hat matrix inverse.</p> required <code>n_jobs</code> <code>int</code> <p>The number of jobs to use for parallel processing.</p> <code>1</code> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity level of joblib.</p> <code>0</code> <code>joblib_backend</code> <code>str</code> <p>The joblib backend to use.</p> <code>'loky'</code> <code>batch_size</code> <code>int</code> <p>The batch size for parallel processing.</p> <code>100</code> <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The hat diagonals layer, of shape (n_obs, n_params).</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/hat_diagonals.py</code> <pre><code>def set_hat_diagonals_layer(\n    adata: ad.AnnData,\n    shared_state: dict | None,\n    n_jobs: int = 1,\n    joblib_verbosity: int = 0,\n    joblib_backend: str = \"loky\",\n    batch_size: int = 100,\n    min_mu: float = 0.5,\n):\n    \"\"\"Compute the hat diagonals layer from the adata and the shared state.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The AnnData object.\n\n    shared_state : Optional[dict]\n        The shared state dictionary.\n        This dictionary must contain the global hat matrix inverse.\n\n    n_jobs : int\n        The number of jobs to use for parallel processing.\n\n    joblib_verbosity : int\n        The verbosity level of joblib.\n\n    joblib_backend : str\n        The joblib backend to use.\n\n    batch_size : int\n        The batch size for parallel processing.\n\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n\n    Returns\n    -------\n    np.ndarray\n        The hat diagonals layer, of shape (n_obs, n_params).\n    \"\"\"\n    can_set_hat_diagonals_layer(adata, shared_state, raise_error=True)\n    if \"_hat_diagonals\" in adata.layers.keys():\n        return\n\n    assert shared_state is not None, (\n        \"To construct the _hat_diagonals layer, \" \"one must have a shared state.\"\n    )\n\n    gene_names = adata.var_names[adata.varm[\"non_zero\"]]\n    beta = adata.varm[\"LFC\"].loc[gene_names].to_numpy()\n    design_matrix = adata.obsm[\"design_matrix\"].values\n    size_factors = adata.obsm[\"size_factors\"]\n\n    dispersions = adata[:, gene_names].varm[\"dispersions\"]\n\n    # ---- Step 1: Compute the mu and the diagonal of the hat matrix ---- #\n\n    with parallel_backend(joblib_backend):\n        res = Parallel(n_jobs=n_jobs, verbose=joblib_verbosity)(\n            delayed(make_hat_diag_batch)(\n                beta[i : i + batch_size],\n                shared_state[\"global_hat_matrix_inv\"][i : i + batch_size],\n                design_matrix,\n                size_factors,\n                dispersions[i : i + batch_size],\n                min_mu,\n            )\n            for i in range(0, len(beta), batch_size)\n        )\n\n    H = np.concatenate(res)\n\n    H_layer = np.full(adata.shape, np.NaN)\n\n    H_layer[:, adata.var_names.get_indexer(gene_names)] = H.T\n\n    adata.layers[\"_hat_diagonals\"] = H_layer\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_hat","title":"<code>mu_hat</code>","text":"<p>Module to build the mu_hat layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_hat.can_get_mu_hat","title":"<code>can_get_mu_hat(local_adata, raise_error=False)</code>","text":"<p>Check if the mu_hat layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the mu_hat layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the mu_hat layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mu_hat layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/mu_hat.py</code> <pre><code>def can_get_mu_hat(local_adata: ad.AnnData, raise_error: bool = False) -&gt; bool:\n    \"\"\"Check if the mu_hat layer can be reconstructed.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n\n    raise_error : bool, optional\n        If True, raise an error if the mu_hat layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the mu_hat layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the mu_hat layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"_mu_hat\" in local_adata.layers.keys():\n        return True\n    has_num_replicates = \"num_replicates\" in local_adata.uns\n    has_n_params = \"n_params\" in local_adata.uns\n    if not has_num_replicates or not has_n_params:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain num_replicates in uns field \"\n                \"and n_params in uns field to compute mu_hat.\"\n                \" Here are the keys present in the local adata: \"\n                f\"uns : {local_adata.uns.keys()}\"\n            )\n        return False\n    # If the number of replicates is not equal to the number of parameters,\n    # we need to reconstruct mu_hat from the adata.\n    if len(local_adata.uns[\"num_replicates\"]) != local_adata.uns[\"n_params\"]:\n        try:\n            mu_hat_LFC_ok = can_set_mu_layer(\n                local_adata=local_adata,\n                lfc_param_name=\"_mu_hat_LFC\",\n                mu_param_name=\"_irls_mu_hat\",\n                raise_error=raise_error,\n            )\n        except ValueError as mu_hat_LFC_error:\n            raise ValueError(\n                \"Error while checking if mu_hat_LFC can \"\n                f\"be reconstructed: {mu_hat_LFC_error}\"\n            ) from mu_hat_LFC_error\n        return mu_hat_LFC_ok\n    else:\n        try:\n            fit_lin_mu_hat_ok = can_get_fit_lin_mu_hat(\n                local_adata=local_adata,\n                raise_error=raise_error,\n            )\n        except ValueError as fit_lin_mu_hat_error:\n            raise ValueError(\n                \"Error while checking if fit_lin_mu_hat can be \"\n                f\"reconstructed: {fit_lin_mu_hat_error}\"\n            ) from fit_lin_mu_hat_error\n        return fit_lin_mu_hat_ok\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_hat.set_mu_hat_layer","title":"<code>set_mu_hat_layer(local_adata)</code>","text":"<p>Reconstruct the mu_hat layer.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_layers/mu_hat.py</code> <pre><code>def set_mu_hat_layer(local_adata: ad.AnnData):\n    \"\"\"Reconstruct the mu_hat layer.\n\n    Parameters\n    ----------\n    local_adata: ad.AnnData\n        The local AnnData object.\n    \"\"\"\n    can_get_mu_hat(local_adata, raise_error=True)\n    if \"_mu_hat\" in local_adata.layers.keys():\n        return\n\n    if len(local_adata.uns[\"num_replicates\"]) != local_adata.uns[\"n_params\"]:\n        set_mu_layer(\n            local_adata=local_adata,\n            lfc_param_name=\"_mu_hat_LFC\",\n            mu_param_name=\"_irls_mu_hat\",\n        )\n        local_adata.layers[\"_mu_hat\"] = local_adata.layers[\"_irls_mu_hat\"].copy()\n        return\n    set_fit_lin_mu_hat(\n        local_adata=local_adata,\n    )\n    local_adata.layers[\"_mu_hat\"] = local_adata.layers[\"_fit_lin_mu_hat\"].copy()\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_layer","title":"<code>mu_layer</code>","text":"<p>Module to construct mu layer from LFC estimates.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_layer.can_set_mu_layer","title":"<code>can_set_mu_layer(local_adata, lfc_param_name, mu_param_name, raise_error=False)</code>","text":"<p>Check if the mu layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>lfc_param_name</code> <code>str</code> <p>The name of the log fold changes parameter in the adata.</p> required <code>mu_param_name</code> <code>str</code> <p>The name of the mu parameter in the adata.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the mu layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the mu layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the mu layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/mu_layer.py</code> <pre><code>def can_set_mu_layer(\n    local_adata: ad.AnnData,\n    lfc_param_name: str,\n    mu_param_name: str,\n    raise_error: bool = False,\n) -&gt; bool:\n    \"\"\"Check if the mu layer can be reconstructed.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n\n    lfc_param_name : str\n        The name of the log fold changes parameter in the adata.\n\n    mu_param_name : str\n        The name of the mu parameter in the adata.\n\n    raise_error : bool, optional\n        If True, raise an error if the mu layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the mu layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the mu layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if mu_param_name in local_adata.layers.keys():\n        return True\n\n    has_design_matrix = \"design_matrix\" in local_adata.obsm.keys()\n    has_lfc_param = lfc_param_name in local_adata.varm.keys()\n    has_size_factors = \"size_factors\" in local_adata.obsm.keys()\n    has_non_zero = \"non_zero\" in local_adata.varm.keys()\n\n    has_all = has_design_matrix and has_lfc_param and has_size_factors and has_non_zero\n    if not has_all:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain the design matrix obsm\"\n                f\", the {lfc_param_name} varm to compute the mu layer, \"\n                f\"the size_factors obsm and the non_zero varm. \"\n                \" Here are the keys present in the local adata: \"\n                f\"obsm : {local_adata.obsm.keys()} and varm : {local_adata.varm.keys()}\"\n            )\n        return False\n    return True\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_layer.make_mu_batch","title":"<code>make_mu_batch(beta, design_matrix, size_factors)</code>","text":"<p>Compute the mu matrix for a batch of LFC estimates.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>ndarray</code> <p>Current LFC estimate, of shape (batch_size, n_params).</p> required <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <p>Returns:</p> Name Type Description <code>mu</code> <code>ndarray</code> <p>The mu matrix, of shape (n_obs, batch_size).</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/mu_layer.py</code> <pre><code>def make_mu_batch(\n    beta: np.ndarray,\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Compute the mu matrix for a batch of LFC estimates.\n\n    Parameters\n    ----------\n    beta : np.ndarray\n        Current LFC estimate, of shape (batch_size, n_params).\n    design_matrix : np.ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : np.ndarray\n        The size factors, of shape (n_obs).\n\n    Returns\n    -------\n    mu : np.ndarray\n        The mu matrix, of shape (n_obs, batch_size).\n    \"\"\"\n    mu = size_factors[:, None] * np.exp(design_matrix @ beta.T)\n\n    return mu\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.mu_layer.set_mu_layer","title":"<code>set_mu_layer(local_adata, lfc_param_name, mu_param_name, n_jobs=1, joblib_verbosity=0, joblib_backend='loky', batch_size=100)</code>","text":"<p>Reconstruct a mu layer from the adata and a given LFC field.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>lfc_param_name</code> <code>str</code> <p>The name of the log fold changes parameter in the adata.</p> required <code>mu_param_name</code> <code>str</code> <p>The name of the mu parameter in the adata.</p> required <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel.</p> <code>1</code> <code>joblib_verbosity</code> <code>int</code> <p>Verbosity level of joblib.</p> <code>0</code> <code>joblib_backend</code> <code>str</code> <p>Joblib backend to use.</p> <code>'loky'</code> <code>batch_size</code> <code>int</code> <p>Batch size for parallelization.</p> <code>100</code> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/mu_layer.py</code> <pre><code>def set_mu_layer(\n    local_adata: ad.AnnData,\n    lfc_param_name: str,\n    mu_param_name: str,\n    n_jobs: int = 1,\n    joblib_verbosity: int = 0,\n    joblib_backend: str = \"loky\",\n    batch_size: int = 100,\n):\n    \"\"\"Reconstruct a mu layer from the adata and a given LFC field.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n\n    lfc_param_name : str\n        The name of the log fold changes parameter in the adata.\n\n    mu_param_name : str\n        The name of the mu parameter in the adata.\n\n    n_jobs : int\n        Number of jobs to run in parallel.\n\n    joblib_verbosity : int\n        Verbosity level of joblib.\n\n    joblib_backend : str\n        Joblib backend to use.\n\n    batch_size : int\n        Batch size for parallelization.\n    \"\"\"\n    can_set_mu_layer(\n        local_adata, lfc_param_name, mu_param_name=mu_param_name, raise_error=True\n    )\n    if mu_param_name in local_adata.layers.keys():\n        return\n    gene_names = local_adata.var_names[local_adata.varm[\"non_zero\"]]\n    beta = local_adata.varm[lfc_param_name].loc[gene_names].to_numpy()\n    design_matrix = local_adata.obsm[\"design_matrix\"].values\n    size_factors = local_adata.obsm[\"size_factors\"]\n\n    with parallel_backend(joblib_backend):\n        res = Parallel(n_jobs=n_jobs, verbose=joblib_verbosity)(\n            delayed(make_mu_batch)(\n                beta[i : i + batch_size],\n                design_matrix,\n                size_factors,\n            )\n            for i in range(0, len(beta), batch_size)\n        )\n\n    if len(res) == 0:\n        mu = np.zeros((local_adata.shape[0], 0))\n    else:\n        mu = np.concatenate(list(res), axis=1)\n\n    mu_layer = np.full(local_adata.shape, np.NaN)\n\n    mu_layer[:, local_adata.var_names.get_indexer(gene_names)] = mu\n\n    local_adata.layers[mu_param_name] = mu_layer\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.normed_counts","title":"<code>normed_counts</code>","text":"<p>Module to construct the normed_counts layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.normed_counts.can_get_normed_counts","title":"<code>can_get_normed_counts(adata, raise_error=False)</code>","text":"<p>Check if the normed_counts layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the normed_counts layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the normed_counts layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the normed_counts layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/normed_counts.py</code> <pre><code>def can_get_normed_counts(adata: ad.AnnData, raise_error: bool = False) -&gt; bool:\n    \"\"\"Check if the normed_counts layer can be reconstructed.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local AnnData object.\n\n    raise_error : bool, optional\n        If True, raise an error if the normed_counts layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the normed_counts layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the normed_counts layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"normed_counts\" in adata.layers.keys():\n        return True\n    has_X = adata.X is not None\n    has_size_factors = \"size_factors\" in adata.obsm.keys()\n    if not has_X or not has_size_factors:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain the X field \"\n                \"and the size_factors obsm to compute the normed_counts layer.\"\n                \" Here are the keys present in the adata: \"\n                f\" obsm : {adata.obsm.keys()}\"\n            )\n        return False\n    return True\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.normed_counts.set_normed_counts","title":"<code>set_normed_counts(adata)</code>","text":"<p>Reconstruct the normed_counts layer.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_layers/normed_counts.py</code> <pre><code>def set_normed_counts(adata: ad.AnnData):\n    \"\"\"Reconstruct the normed_counts layer.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local AnnData object.\n    \"\"\"\n    can_get_normed_counts(adata, raise_error=True)\n    if \"normed_counts\" in adata.layers.keys():\n        return\n    adata.layers[\"normed_counts\"] = adata.X / adata.obsm[\"size_factors\"][:, None]\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.sqerror","title":"<code>sqerror</code>","text":"<p>Module to construct the sqerror layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.sqerror.can_get_sqerror_layer","title":"<code>can_get_sqerror_layer(adata, raise_error=False)</code>","text":"<p>Check if the squared error layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the squared error layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the squared error layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the squared error layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/sqerror.py</code> <pre><code>def can_get_sqerror_layer(adata: ad.AnnData, raise_error: bool = False) -&gt; bool:\n    \"\"\"Check if the squared error layer can be reconstructed.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The local AnnData object.\n\n    raise_error : bool, optional\n        If True, raise an error if the squared error layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the squared error layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the squared error layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"sqerror\" in adata.layers.keys():\n        return True\n    try:\n        has_normed_counts = can_get_normed_counts(adata, raise_error=raise_error)\n    except ValueError as normed_counts_error:\n        raise ValueError(\n            f\"Error while checking if normed_counts can be\"\n            f\" reconstructed: {normed_counts_error}\"\n        ) from normed_counts_error\n\n    has_cell_means = \"cell_means\" in adata.varm.keys()\n    has_cell_obs = \"cells\" in adata.obs.keys()\n    if not has_normed_counts or not has_cell_means or not has_cell_obs:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain the normed_counts layer, the cells obs, \"\n                \"and the cell_means varm to compute the squared error layer.\"\n                \" Here are the keys present in the adata: \"\n                f\"obs : {adata.obs.keys()}, varm : {adata.varm.keys()}\"\n            )\n        return False\n    return True\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.sqerror.set_sqerror_layer","title":"<code>set_sqerror_layer(local_adata)</code>","text":"<p>Compute the squared error between the normalized counts and the trimmed mean.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>Local AnnData. It is expected to have the following fields: - layers[\"normed_counts\"]: the normalized counts. - varm[\"cell_means\"]: the trimmed mean. - obs[\"cells\"]: the cells.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_layers/sqerror.py</code> <pre><code>def set_sqerror_layer(local_adata: ad.AnnData):\n    \"\"\"Compute the squared error between the normalized counts and the trimmed mean.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        Local AnnData. It is expected to have the following fields:\n        - layers[\"normed_counts\"]: the normalized counts.\n        - varm[\"cell_means\"]: the trimmed mean.\n        - obs[\"cells\"]: the cells.\n    \"\"\"\n    can_get_sqerror_layer(local_adata, raise_error=True)\n    if \"sqerror\" in local_adata.layers.keys():\n        return\n    cell_means = local_adata.varm[\"cell_means\"]\n    set_normed_counts(local_adata)\n    if isinstance(cell_means, pd.DataFrame):\n        cells = local_adata.obs[\"cells\"]\n        # restrict to the cells that are in the cell means columns\n        cells = cells[cells.isin(cell_means.columns)]\n        qmat = cell_means[cells].T\n        qmat.index = cells.index\n\n        # initialize wiht nans\n        layer = np.full_like(local_adata.layers[\"normed_counts\"], np.nan)\n        indices = local_adata.obs_names.get_indexer(qmat.index)\n        layer[indices, :] = (\n            local_adata[qmat.index, :].layers[\"normed_counts\"] - qmat\n        ) ** 2\n    else:\n        layer = (local_adata.layers[\"normed_counts\"] - cell_means[None, :]) ** 2\n    local_adata.layers[\"sqerror\"] = layer\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.y_hat","title":"<code>y_hat</code>","text":"<p>Module containing the necessary functions to reconstruct the y_hat layer.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.y_hat.can_get_y_hat","title":"<code>can_get_y_hat(local_adata, raise_error=False)</code>","text":"<p>Check if the y_hat layer can be reconstructed.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the y_hat layer cannot be reconstructed.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the y_hat layer can be reconstructed, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the y_hat layer cannot be reconstructed and raise_error is True.</p> Source code in <code>fedpydeseq2/core/utils/layers/build_layers/y_hat.py</code> <pre><code>def can_get_y_hat(local_adata: ad.AnnData, raise_error: bool = False) -&gt; bool:\n    \"\"\"Check if the y_hat layer can be reconstructed.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n\n    raise_error : bool, optional\n        If True, raise an error if the y_hat layer cannot be reconstructed.\n\n    Returns\n    -------\n    bool\n        True if the y_hat layer can be reconstructed, False otherwise.\n\n    Raises\n    ------\n    ValueError\n        If the y_hat layer cannot be reconstructed and raise_error is True.\n    \"\"\"\n    if \"_y_hat\" in local_adata.layers.keys():\n        return True\n    has_design_matrix = \"design_matrix\" in local_adata.obsm.keys()\n    has_beta_rough_dispersions = \"_beta_rough_dispersions\" in local_adata.varm.keys()\n    if not has_design_matrix or not has_beta_rough_dispersions:\n        if raise_error:\n            raise ValueError(\n                \"Local adata must contain the design matrix obsm \"\n                \"and the _beta_rough_dispersions varm to compute the y_hat layer.\"\n                \" Here are the keys present in the local adata: \"\n                f\"obsm : {local_adata.obsm.keys()} and varm : {local_adata.varm.keys()}\"\n            )\n        return False\n    return True\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_layers.y_hat.set_y_hat","title":"<code>set_y_hat(local_adata)</code>","text":"<p>Reconstruct the y_hat layer.</p> <p>Parameters:</p> Name Type Description Default <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_layers/y_hat.py</code> <pre><code>def set_y_hat(local_adata: ad.AnnData):\n    \"\"\"Reconstruct the y_hat layer.\n\n    Parameters\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData object.\n    \"\"\"\n    can_get_y_hat(local_adata, raise_error=True)\n    if \"_y_hat\" in local_adata.layers.keys():\n        return\n    y_hat = (\n        local_adata.obsm[\"design_matrix\"].to_numpy()\n        @ local_adata.varm[\"_beta_rough_dispersions\"].T\n    )\n    local_adata.layers[\"_y_hat\"] = y_hat\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_refit_adata","title":"<code>build_refit_adata</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_refit_adata.set_basic_refit_adata","title":"<code>set_basic_refit_adata(self)</code>","text":"<p>Set the basic refit adata from the local adata.</p> <p>This function checks that the local adata is loaded and the replaced genes are computed and stored in the varm field. It then sets the refit adata from the local adata.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The object containing the local adata and the refit adata.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_refit_adata.py</code> <pre><code>def set_basic_refit_adata(self: Any):\n    \"\"\"Set the basic refit adata from the local adata.\n\n    This function checks that the local adata is loaded and the replaced\n    genes are computed and stored in the varm field. It then sets the refit\n    adata from the local adata.\n\n    Parameters\n    ----------\n    self : Any\n        The object containing the local adata and the refit adata.\n    \"\"\"\n    assert (\n        self.local_adata is not None\n    ), \"Local adata must be loaded before setting the refit adata.\"\n    assert (\n        \"replaced\" in self.local_adata.varm.keys()\n    ), \"Replaced genes must be computed before setting the refit adata.\"\n\n    genes_to_replace = pd.Series(\n        self.local_adata.varm[\"replaced\"], index=self.local_adata.var_names\n    )\n    if self.refit_adata is None:\n        self.refit_adata = self.local_adata[:, genes_to_replace].copy()\n        # Clear the varm field of the refit adata\n        self.refit_adata.varm = None\n    elif \"refitted\" not in self.local_adata.varm.keys():\n        self.refit_adata.X = self.local_adata[:, genes_to_replace].X.copy()\n        self.refit_adata.obsm = self.local_adata.obsm\n    else:\n        genes_to_refit = pd.Series(\n            self.local_adata.varm[\"refitted\"], index=self.local_adata.var_names\n        )\n        self.refit_adata.X = self.local_adata[:, genes_to_refit].X.copy()\n        self.refit_adata.obsm = self.local_adata.obsm\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.build_refit_adata.set_imputed_counts_refit_adata","title":"<code>set_imputed_counts_refit_adata(self)</code>","text":"<p>Set the imputed counts in the refit adata.</p> <p>This function checks that the refit adata, the local adata, the replaced genes, the trimmed mean normed counts, the size factors, the cooks G cutoff, and the replaceable genes are computed and stored in the appropriate fields. It then sets the imputed counts in the refit adata.</p> <p>Note that this function must be run on an object which already contains a refit_adata, whose counts, obsm and uns have been set with the <code>set_basic_refit_adata</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The object containing the refit adata, the local adata, the replaced genes, the trimmed mean normed counts, the size factors, the cooks G cutoff, and the replaceable genes.</p> required Source code in <code>fedpydeseq2/core/utils/layers/build_refit_adata.py</code> <pre><code>def set_imputed_counts_refit_adata(self: Any):\n    \"\"\"Set the imputed counts in the refit adata.\n\n    This function checks that the refit adata, the local adata, the replaced\n    genes, the trimmed mean normed counts, the size factors, the cooks G cutoff,\n    and the replaceable genes are computed and stored in the appropriate fields.\n    It then sets the imputed counts in the refit adata.\n\n    Note that this function must be run on an object which already contains\n    a refit_adata, whose counts, obsm and uns have been set with the\n    `set_basic_refit_adata` function.\n\n    Parameters\n    ----------\n    self : Any\n        The object containing the refit adata, the local adata, the replaced\n        genes, the trimmed mean normed counts, the size factors, the cooks G\n        cutoff, and the replaceable genes.\n    \"\"\"\n    assert (\n        self.refit_adata is not None\n    ), \"Refit adata must be loaded before setting the imputed counts.\"\n    assert (\n        self.local_adata is not None\n    ), \"Local adata must be loaded before setting the imputed counts.\"\n    assert (\n        \"replaced\" in self.local_adata.varm.keys()\n    ), \"Replaced genes must be computed before setting the imputed counts.\"\n    assert (\n        \"_trimmed_mean_normed_counts\" in self.refit_adata.varm.keys()\n    ), \"Trimmed mean normed counts must be computed before setting the imputed counts.\"\n    assert (\n        \"size_factors\" in self.refit_adata.obsm.keys()\n    ), \"Size factors must be computed before setting the imputed counts.\"\n    assert (\n        \"_where_cooks_g_cutoff\" in self.local_adata.uns.keys()\n    ), \"Cooks G cutoff must be computed before setting the imputed counts.\"\n    assert (\n        \"replaceable\" in self.refit_adata.obsm.keys()\n    ), \"Replaceable genes must be computed before setting the imputed counts.\"\n\n    trimmed_mean_normed_counts = self.refit_adata.varm[\"_trimmed_mean_normed_counts\"]\n\n    replacement_counts = pd.DataFrame(\n        self.refit_adata.obsm[\"size_factors\"][:, None] * trimmed_mean_normed_counts,\n        columns=self.refit_adata.var_names,\n        index=self.refit_adata.obs_names,\n    ).astype(int)\n\n    idx = np.zeros(self.local_adata.shape, dtype=bool)\n    idx[self.local_adata.uns[\"_where_cooks_g_cutoff\"]] = True\n\n    # Restrict to the genes to replace\n    if \"refitted\" not in self.local_adata.varm.keys():\n        idx = idx[:, self.local_adata.varm[\"replaced\"]]\n    else:\n        idx = idx[:, self.local_adata.varm[\"refitted\"]]\n\n    # Replace the counts\n    self.refit_adata.X[self.refit_adata.obsm[\"replaceable\"][:, None] &amp; idx] = (\n        replacement_counts.values[self.refit_adata.obsm[\"replaceable\"][:, None] &amp; idx]\n    )\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.cooks_layer","title":"<code>cooks_layer</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.cooks_layer.can_skip_local_cooks_preparation","title":"<code>can_skip_local_cooks_preparation(self)</code>","text":"<p>Check if the Cook's distance is in the layers to save.</p> <p>This function checks if the Cook's distance is in the layers to save.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The object.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the Cook's distance is in the layers to save.</p> Source code in <code>fedpydeseq2/core/utils/layers/cooks_layer.py</code> <pre><code>def can_skip_local_cooks_preparation(self: Any) -&gt; bool:\n    \"\"\"Check if the Cook's distance is in the layers to save.\n\n    This function checks if the Cook's distance is in the layers to save.\n\n    Parameters\n    ----------\n    self : Any\n        The object.\n\n    Returns\n    -------\n    bool:\n        Whether the Cook's distance is in the layers to save.\n    \"\"\"\n    only_from_disk = (\n        not hasattr(self, \"save_layers_to_disk\") or self.save_layers_to_disk\n    )\n    if only_from_disk and \"cooks\" in self.local_adata.layers.keys():\n        return True\n    if hasattr(self, \"layers_to_save_on_disk\"):\n        layers_to_save_on_disk = self.layers_to_save_on_disk\n        if (\n            layers_to_save_on_disk is not None\n            and \"local_adata\" in layers_to_save_on_disk\n            and layers_to_save_on_disk[\"local_adata\"] is not None\n            and \"cooks\" in layers_to_save_on_disk[\"local_adata\"]\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.cooks_layer.make_hat_matrix_summands_batch","title":"<code>make_hat_matrix_summands_batch(design_matrix, size_factors, beta, dispersions, min_mu)</code>","text":"<p>Make the local hat matrix.</p> <p>This is quite similar to the make_irls_summands_batch function, but it does not require the counts, and returns only the H matrix.</p> <p>This is used in the final step of the IRLS algorithm to compute the local hat matrix.</p> <p>Parameters:</p> Name Type Description Default <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <code>beta</code> <code>ndarray</code> <p>The log fold change matrix, of shape (batch_size, n_params).</p> required <code>dispersions</code> <code>ndarray</code> <p>The dispersions, of shape (batch_size).</p> required <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability.</p> required <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>The H matrix, of shape (batch_size, n_params, n_params).</p> Source code in <code>fedpydeseq2/core/utils/layers/cooks_layer.py</code> <pre><code>def make_hat_matrix_summands_batch(\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n    beta: np.ndarray,\n    dispersions: np.ndarray,\n    min_mu: float,\n) -&gt; np.ndarray:\n    \"\"\"Make the local hat matrix.\n\n    This is quite similar to the make_irls_summands_batch function, but it does not\n    require the counts, and returns only the H matrix.\n\n    This is used in the final step of the IRLS algorithm to compute the local hat\n    matrix.\n\n    Parameters\n    ----------\n    design_matrix : np.ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : np.ndarray\n        The size factors, of shape (n_obs).\n    beta : np.ndarray\n        The log fold change matrix, of shape (batch_size, n_params).\n    dispersions : np.ndarray\n        The dispersions, of shape (batch_size).\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n\n\n    Returns\n    -------\n    H : np.ndarray\n        The H matrix, of shape (batch_size, n_params, n_params).\n    \"\"\"\n    mu = size_factors[:, None] * np.exp(design_matrix @ beta.T)\n\n    mu = np.maximum(mu, min_mu)\n\n    W = mu / (1.0 + mu * dispersions[None, :])\n\n    H = (design_matrix.T[:, :, None] * W).transpose(2, 0, 1) @ design_matrix[None, :, :]\n\n    return H\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.cooks_layer.prepare_cooks_agg","title":"<code>prepare_cooks_agg(method)</code>","text":"<p>Decorate the aggregation step to compute the Cook's distance.</p> <p>This decorator is supposed to be placed on the aggregation step just before a local step which needs the \"cooks\" layer. The decorator will check if the shared state contains the necessary keys for the Cook's distance computation. If this is not the case, then the Cook's distance must have been saved in the layers_to_save. It will compute the Cook's dispersion, the hat matrix inverse, and then call the method.</p> <p>It will add the following keys to the shared state: - cooks_dispersions - global_hat_matrix_inv</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The aggregation method to decorate. It must have the following signature: method(self, shared_states: Optional[list], **method_parameters).</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The decorated method.</p> Source code in <code>fedpydeseq2/core/utils/layers/cooks_layer.py</code> <pre><code>def prepare_cooks_agg(method: Callable):\n    \"\"\"Decorate the aggregation step to compute the Cook's distance.\n\n    This decorator is supposed to be placed on the aggregation step just before\n    a local step which needs the \"cooks\" layer. The decorator will check if the\n    shared state contains the necessary keys for the Cook's distance computation.\n    If this is not the case, then the Cook's distance must have been saved in the\n    layers_to_save.\n    It will compute the Cook's dispersion, the hat matrix inverse, and then call\n    the method.\n\n    It will add the following keys to the shared state:\n    - cooks_dispersions\n    - global_hat_matrix_inv\n\n    Parameters\n    ----------\n    method : Callable\n        The aggregation method to decorate.\n        It must have the following signature:\n        method(self, shared_states: Optional[list], **method_parameters).\n\n    Returns\n    -------\n    Callable:\n        The decorated method.\n    \"\"\"\n\n    @wraps(method)\n    def method_inner(\n        self,\n        shared_states: list | None,\n        **method_parameters,\n    ):\n        # Check that the shared state contains the necessary keys\n        # for the Cook's distance computation\n        # If this is not the case, then the cooks distance must have\n        # been saved in the layers_to_save\n\n        try:\n            assert isinstance(shared_states, list)\n            assert \"n_samples\" in shared_states[0].keys()\n            assert \"varEst\" in shared_states[0].keys()\n            assert \"mean_normed_counts\" in shared_states[0].keys()\n            assert \"local_hat_matrix\" in shared_states[0].keys()\n        except AssertionError as assertion_error:\n            only_from_disk = (\n                not hasattr(self, \"save_layers_to_disk\") or self.save_layers_to_disk\n            )\n            if only_from_disk:\n                return method(self, shared_states, **method_parameters)\n            elif isinstance(shared_states, list) and shared_states[0][\"_skip_cooks\"]:\n                return method(self, shared_states, **method_parameters)\n            raise ValueError(\n                \"The shared state does not contain the necessary keys for\"\n                \"the Cook's distance computation.\"\n            ) from assertion_error\n\n        assert isinstance(shared_states, list)\n\n        # ---- Step 1: Compute Cooks dispersion ---- #\n\n        n_sample_tot = sum(\n            [shared_state[\"n_samples\"] for shared_state in shared_states]\n        )\n        varEst = shared_states[0][\"varEst\"]\n        mean_normed_counts = (\n            np.array(\n                [\n                    (shared_state[\"mean_normed_counts\"] * shared_state[\"n_samples\"])\n                    for shared_state in shared_states\n                ]\n            ).sum(axis=0)\n            / n_sample_tot\n        )\n        mask_zero = mean_normed_counts == 0\n        mask_varEst_zero = varEst == 0\n        alpha = varEst - mean_normed_counts\n        alpha[~mask_zero] = alpha[~mask_zero] / mean_normed_counts[~mask_zero] ** 2\n        alpha[mask_varEst_zero &amp; mask_zero] = np.nan\n        alpha[mask_varEst_zero &amp; (~mask_zero)] = (\n            np.inf * alpha[mask_varEst_zero &amp; (~mask_zero)]\n        )\n\n        # cannot use the typical min_disp = 1e-8 here or else all counts in the same\n        # group as the outlier count will get an extreme Cook's distance\n        minDisp = 0.04\n        alpha = cast(pd.Series, np.maximum(alpha, minDisp))\n\n        # --- Step 2: Compute the hat matrix inverse --- #\n\n        global_hat_matrix = sum([state[\"local_hat_matrix\"] for state in shared_states])\n        n_jobs, joblib_verbosity, joblib_backend, batch_size = get_joblib_parameters(\n            self\n        )\n        ridge_factor = np.diag(np.repeat(1e-6, global_hat_matrix.shape[1]))\n        with parallel_backend(joblib_backend):\n            res = Parallel(n_jobs=n_jobs, verbose=joblib_verbosity)(\n                delayed(np.linalg.inv)(hat_matrices + ridge_factor)\n                for hat_matrices in np.split(\n                    global_hat_matrix,\n                    range(\n                        batch_size,\n                        len(global_hat_matrix),\n                        batch_size,\n                    ),\n                )\n            )\n\n        global_hat_matrix_inv = np.concatenate(res)\n\n        # ---- Step 3: Run the method ---- #\n\n        shared_state = method(self, shared_states, **method_parameters)\n\n        # ---- Step 4: Save the Cook's dispersion and the hat matrix inverse ---- #\n\n        shared_state[\"cooks_dispersions\"] = alpha\n        shared_state[\"global_hat_matrix_inv\"] = global_hat_matrix_inv\n\n        return shared_state\n\n    return method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.cooks_layer.prepare_cooks_local","title":"<code>prepare_cooks_local(method)</code>","text":"<p>Decorate the local method just preceding a local method needing cooks.</p> <p>This method is only applied if the Cooks layer is not present or must not be saved between steps.</p> <p>This step is used to compute the local hat matrix and the mean normed counts.</p> <p>Before the method is called, the varEst must be accessed from the shared state, or from the local adata if it is not present in the shared state.</p> <p>The local hat matrix and the mean normed counts are computed, and the following keys are added to the shared state: - local_hat_matrix - mean_normed_counts - n_samples - varEst</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The remote_data method to decorate.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The decorated method.</p> Source code in <code>fedpydeseq2/core/utils/layers/cooks_layer.py</code> <pre><code>def prepare_cooks_local(method: Callable):\n    \"\"\"Decorate the local method just preceding a local method needing cooks.\n\n    This method is only applied if the Cooks layer is not present or must not be\n    saved between steps.\n\n    This step is used to compute the local hat matrix and the mean normed counts.\n\n    Before the method is called, the varEst must be accessed from the shared state,\n    or from the local adata if it is not present in the shared state.\n\n    The local hat matrix and the mean normed counts are computed, and the following\n    keys are added to the shared state:\n    - local_hat_matrix\n    - mean_normed_counts\n    - n_samples\n    - varEst\n\n    Parameters\n    ----------\n    method : Callable\n        The remote_data method to decorate.\n\n    Returns\n    -------\n    Callable:\n        The decorated method.\n    \"\"\"\n\n    @wraps(method)\n    def method_inner(\n        self,\n        data_from_opener: ad.AnnData,\n        shared_state: Any = None,\n        **method_parameters,\n    ):\n        # ---- Step 0: If can skip, we skip ---- #\n        if can_skip_local_cooks_preparation(self):\n            shared_state = method(\n                self, data_from_opener, shared_state, **method_parameters\n            )\n            shared_state[\"_skip_cooks\"] = True\n            return shared_state\n\n        # ---- Step 1: Access varEst ---- #\n\n        if \"varEst\" in self.local_adata.varm.keys():\n            varEst = self.local_adata.varm[\"varEst\"]\n        else:\n            assert \"varEst\" in shared_state\n            varEst = shared_state[\"varEst\"]\n            self.local_adata.varm[\"varEst\"] = varEst\n\n        # ---- Step 2: Run the method ---- #\n        shared_state = method(self, data_from_opener, shared_state, **method_parameters)\n\n        # ---- Step 3: Compute the local hat matrix ---- #\n\n        n_jobs, joblib_verbosity, joblib_backend, batch_size = get_joblib_parameters(\n            self\n        )\n        # Compute hat matrix\n        (\n            gene_names,\n            design_matrix,\n            size_factors,\n            counts,\n            dispersions,\n            beta,\n        ) = get_lfc_utils_from_gene_mask_adata(\n            self.local_adata,\n            None,\n            \"dispersions\",\n            lfc_param_name=\"LFC\",\n        )\n\n        with parallel_backend(joblib_backend):\n            res = Parallel(n_jobs=n_jobs, verbose=joblib_verbosity)(\n                delayed(make_hat_matrix_summands_batch)(\n                    design_matrix,\n                    size_factors,\n                    beta[i : i + batch_size],\n                    dispersions[i : i + batch_size],\n                    self.min_mu,\n                )\n                for i in range(0, len(beta), batch_size)\n            )\n\n        if len(res) == 0:\n            H = np.zeros((0, beta.shape[1], beta.shape[1]))\n        else:\n            H = np.concatenate(res)\n\n        shared_state[\"local_hat_matrix\"] = H\n\n        # ---- Step 4: Compute the mean normed counts ---- #\n\n        mean_normed_counts = self.local_adata.layers[\"normed_counts\"].mean(axis=0)\n\n        shared_state[\"mean_normed_counts\"] = mean_normed_counts\n        shared_state[\"n_samples\"] = self.local_adata.n_obs\n        shared_state[\"varEst\"] = varEst\n        shared_state[\"_skip_cooks\"] = False\n\n        return shared_state\n\n    return method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.joblib_utils","title":"<code>joblib_utils</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.joblib_utils.get_joblib_parameters","title":"<code>get_joblib_parameters(x)</code>","text":"<p>Get the joblib parameters from an object, and return them as a tuple.</p> <p>If the object has no joblib parameters, default values are returned.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Object from which to extract the joblib parameters.</p> required <p>Returns:</p> Name Type Description <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel.</p> <code>joblib_verbosity</code> <code>int</code> <p>Verbosity level of joblib.</p> <code>joblib_backend</code> <code>str</code> <p>Joblib backend.</p> <code>batch_size</code> <code>int</code> <p>Batch size for the IRLS algorithm.</p> Source code in <code>fedpydeseq2/core/utils/layers/joblib_utils.py</code> <pre><code>def get_joblib_parameters(x: Any) -&gt; tuple[int, int, str, int]:\n    \"\"\"Get the joblib parameters from an object, and return them as a tuple.\n\n    If the object has no joblib parameters, default values are returned.\n\n    Parameters\n    ----------\n    x: Any\n        Object from which to extract the joblib parameters.\n\n    Returns\n    -------\n    n_jobs: int\n        Number of jobs to run in parallel.\n    joblib_verbosity: int\n        Verbosity level of joblib.\n    joblib_backend: str\n        Joblib backend.\n    batch_size: int\n        Batch size for the IRLS algorithm.\n    \"\"\"\n    n_jobs = x.num_jobs if hasattr(x, \"num_jobs\") else 1\n\n    joblib_verbosity = x.joblib_verbosity if hasattr(x, \"joblib_verbosity\") else 0\n    joblib_backend = x.joblib_backend if hasattr(x, \"joblib_backend\") else \"loky\"\n    batch_size = x.irls_batch_size if hasattr(x, \"irls_batch_size\") else 100\n    return n_jobs, joblib_verbosity, joblib_backend, batch_size\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.reconstruct_adatas_decorator","title":"<code>reconstruct_adatas_decorator</code>","text":"<p>Module containing a decorator to handle simple layers.</p> <p>This wrapper is used to load and save simple layers from the adata object. These simple layers are defined in SIMPLE_LAYERS.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.reconstruct_adatas_decorator.check_and_load_layers","title":"<code>check_and_load_layers(self, adata_name, layers_to_load, shared_state, only_from_disk)</code>","text":"<p>Check and load layers for a given adata_name.</p> <p>This function checks the availability of the layers to load and loads them, for the adata_name adata.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The object containing the adata.</p> required <code>adata_name</code> <code>str</code> <p>The name of the adata to load the layers into.</p> required <code>layers_to_load</code> <code>dict[str, Optional[list[str]]]</code> <p>The layers to load for each adata. It must have adata_name as a key.</p> required <code>shared_state</code> <code>Optional[dict]</code> <p>The shared state.</p> required <code>only_from_disk</code> <code>bool</code> <p>Whether to load only the layers from disk.</p> required Source code in <code>fedpydeseq2/core/utils/layers/reconstruct_adatas_decorator.py</code> <pre><code>def check_and_load_layers(\n    self: Any,\n    adata_name: str,\n    layers_to_load: dict[str, list[str] | None],\n    shared_state: dict | None,\n    only_from_disk: bool,\n):\n    \"\"\"Check and load layers for a given adata_name.\n\n    This function checks the availability of the layers to load\n    and loads them, for the adata_name adata.\n\n    Parameters\n    ----------\n    self : Any\n        The object containing the adata.\n    adata_name : str\n        The name of the adata to load the layers into.\n    layers_to_load : dict[str, Optional[list[str]]]\n        The layers to load for each adata. It must have adata_name\n        as a key.\n    shared_state : Optional[dict]\n        The shared state.\n    only_from_disk : bool\n        Whether to load only the layers from disk.\n    \"\"\"\n    adata = getattr(self, adata_name)\n    layers_to_load_adata = layers_to_load[adata_name]\n    available_layers_adata = get_available_layers(\n        adata,\n        shared_state,\n        refit=adata_name == \"refit_adata\",\n        all_layers_from_disk=only_from_disk,\n    )\n    if layers_to_load_adata is None:\n        layers_to_load_adata = available_layers_adata\n    else:\n        assert np.all(\n            [layer in available_layers_adata for layer in layers_to_load_adata]\n        )\n    if adata is None:\n        return\n    assert layers_to_load_adata is not None\n    n_jobs, joblib_verbosity, joblib_backend, batch_size = get_joblib_parameters(self)\n    load_layers(\n        adata=adata,\n        shared_state=shared_state,\n        layers_to_load=layers_to_load_adata,\n        n_jobs=n_jobs,\n        joblib_verbosity=joblib_verbosity,\n        joblib_backend=joblib_backend,\n        batch_size=batch_size,\n    )\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.reconstruct_adatas_decorator.reconstruct_adatas","title":"<code>reconstruct_adatas(method)</code>","text":"<p>Decorate a method to load layers and remove them before saving the state.</p> <p>This decorator loads the layers from the data_from_opener and the adata object before calling the method. It then removes the layers from the adata object after the method is called.</p> <p>The object self CAN have the following attributes:</p> <ul> <li> <p>save_layers_to_disk: if this argument exists or is True, we save all the layers on disk, without removing them at the end of each local step. If it is False, we remove all layers that must be removed at the end of each local step. This argument is prevalent above all others described below.</p> </li> <li> <p>layers_to_save_on_disk: if this argument exists, contains the layers that must be saved on disk at EVERY local step. It can be either None (in which case the default behaviour is to save no layers) or a dictionary with a refit_adata and local_adata key. The associated values contain either None (no layers) or a list of layers to save at each step.</p> </li> </ul> <p>This decorator adds two parameters to each method decorated with it: - layers_to_load - layers_to_save_on_disk</p> <p>If the layers_to_load is None, the default is to load all available layers. Else, we only load the layers specified in the layers_to_load argument.</p> <p>The layers_to_save_on_disk argument is ADDED to the layers_to_save_on_disk attribute of self for the duration of the method and then removed. That way, the inner method can access the names of the layers_to_save_on_disk which will effectively be saved at the end of the step.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method to decorate. This method is expected to have the following signature: method(self, data_from_opener: ad.AnnData, shared_state: Any,  **method_parameters).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated method, which loads the simple layers before calling the method and removes the simple layers after the method is called.</p> Source code in <code>fedpydeseq2/core/utils/layers/reconstruct_adatas_decorator.py</code> <pre><code>def reconstruct_adatas(method: Callable):\n    \"\"\"Decorate a method to load layers and remove them before saving the state.\n\n    This decorator loads the layers from the data_from_opener and the adata\n    object before calling the method. It then removes the layers from the adata\n    object after the method is called.\n\n    The object self CAN have the following attributes:\n\n    - save_layers_to_disk: if this argument exists or is True, we save all the layers\n    on disk, without removing them at the end of each local step. If it is False,\n    we remove all layers that must be removed at the end of each local step.\n    This argument is prevalent above all others described below.\n\n    - layers_to_save_on_disk: if this argument exists, contains the layers that\n    must be saved on disk at EVERY local step. It can be either None (in which\n    case the default behaviour is to save no layers) or a dictionary with a refit_adata\n    and local_adata key. The associated values contain either None (no layers) or\n    a list of layers to save at each step.\n\n    This decorator adds two parameters to each method decorated with it:\n    - layers_to_load\n    - layers_to_save_on_disk\n\n    If the layers_to_load is None, the default is to load all available layers.\n    Else, we only load the layers specified in the layers_to_load argument.\n\n    The layers_to_save_on_disk argument is ADDED to the layers_to_save_on_disk attribute\n    of self for the duration of the method and then removed. That way, the inner\n    method can access the names of the layers_to_save_on_disk which will effectively\n    be saved at the end of the step.\n\n    Parameters\n    ----------\n    method : Callable\n        The method to decorate. This method is expected to have the following signature:\n        method(self, data_from_opener: ad.AnnData, shared_state: Any,\n         **method_parameters).\n\n    Returns\n    -------\n    Callable\n        The decorated method, which loads the simple layers before calling the method\n        and removes the simple layers after the method is called.\n    \"\"\"\n\n    @wraps(method)\n    def method_inner(\n        self,\n        data_from_opener: ad.AnnData,\n        shared_state: Any = None,\n        layers_to_load: LayersToLoadSaveType = None,\n        layers_to_save_on_disk: LayersToLoadSaveType = None,\n        **method_parameters,\n    ):\n        if layers_to_load is None:\n            layers_to_load = {\"local_adata\": None, \"refit_adata\": None}\n        if hasattr(self, \"layers_to_save_on_disk\"):\n            if self.layers_to_save_on_disk is None:\n                global_layers_to_save_on_disk = None\n            else:\n                global_layers_to_save_on_disk = self.layers_to_save_on_disk.copy()\n\n            if global_layers_to_save_on_disk is None:\n                self.layers_to_save_on_disk = {\"local_adata\": [], \"refit_adata\": []}\n        else:\n            self.layers_to_save_on_disk = {\"local_adata\": [], \"refit_adata\": []}\n\n        if layers_to_save_on_disk is None:\n            layers_to_save_on_disk = {\"local_adata\": [], \"refit_adata\": []}\n\n        # Set the layers_to_save_on_disk attribute to the union of the layers specified\n        # in the argument and those in the attribute, to be accessed by the method.\n        assert isinstance(self.layers_to_save_on_disk, dict)\n        for adata_name in [\"local_adata\", \"refit_adata\"]:\n            if self.layers_to_save_on_disk[adata_name] is None:\n                self.layers_to_save_on_disk[adata_name] = []\n            if layers_to_save_on_disk[adata_name] is None:\n                layers_to_save_on_disk[adata_name] = []\n            self.layers_to_save_on_disk[adata_name] = list(\n                set(\n                    layers_to_save_on_disk[adata_name]\n                    + self.layers_to_save_on_disk[adata_name]\n                )\n            )\n\n        # Check that the layers_to_load and layers_to_save are valid\n        assert set(layers_to_load.keys()) == {\"local_adata\", \"refit_adata\"}\n        assert set(self.layers_to_save_on_disk.keys()) == {\"local_adata\", \"refit_adata\"}\n\n        # Load the counts of the adata\n        if self.local_adata is not None:\n            if self.local_adata.X is None:\n                self.local_adata.X = data_from_opener.X\n\n        # Load the available layers\n        only_from_disk = (\n            not hasattr(self, \"save_layers_to_disk\") or self.save_layers_to_disk\n        )\n\n        # Start by loading the local adata\n        check_and_load_layers(\n            self, \"local_adata\", layers_to_load, shared_state, only_from_disk\n        )\n\n        # Create the refit adata\n        reconstruct_refit_adata_without_layers(self)\n\n        # Load the layers of the refit adata\n        check_and_load_layers(\n            self, \"refit_adata\", layers_to_load, shared_state, only_from_disk\n        )\n\n        # Apply the method\n        shared_state = method(self, data_from_opener, shared_state, **method_parameters)\n\n        # Remove all layers which must not be saved on disk\n        for adata_name in [\"local_adata\", \"refit_adata\"]:\n            adata = getattr(self, adata_name)\n            if adata is None:\n                continue\n            if only_from_disk:\n                layers_to_save_on_disk_adata: list | None = list(adata.layers.keys())\n            else:\n                layers_to_save_on_disk_adata = self.layers_to_save_on_disk[adata_name]\n                assert layers_to_save_on_disk_adata is not None\n                for layer in layers_to_save_on_disk_adata:\n                    if layer not in adata.layers.keys():\n                        print(\"Warning: layer not in adata: \", layer)\n            assert layers_to_save_on_disk_adata is not None\n            remove_layers(\n                adata=adata,\n                layers_to_save_on_disk=layers_to_save_on_disk_adata,\n                refit=adata_name == \"refit_adata\",\n            )\n\n        # Reset the layers_to_save_on_disk attribute\n        try:\n            self.layers_to_save_on_disk = global_layers_to_save_on_disk\n        except NameError:\n            del self.layers_to_save_on_disk\n\n        return shared_state\n\n    return method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.reconstruct_adatas_decorator.reconstruct_refit_adata_without_layers","title":"<code>reconstruct_refit_adata_without_layers(self)</code>","text":"<p>Reconstruct the refit adata without the layers.</p> <p>This function reconstructs the refit adata without the layers. It is used to avoid the counts and the obsm being loaded uselessly in the refit_adata.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The object containing the adata.</p> required Source code in <code>fedpydeseq2/core/utils/layers/reconstruct_adatas_decorator.py</code> <pre><code>def reconstruct_refit_adata_without_layers(self: Any):\n    \"\"\"Reconstruct the refit adata without the layers.\n\n    This function reconstructs the refit adata without the layers.\n    It is used to avoid the counts and the obsm being loaded uselessly in the\n    refit_adata.\n\n    Parameters\n    ----------\n    self : Any\n        The object containing the adata.\n    \"\"\"\n    if self.refit_adata is None:\n        return\n    if self.local_adata is not None and \"replaced\" in self.local_adata.varm.keys():\n        set_basic_refit_adata(self)\n    if self.local_adata is not None and \"refitted\" in self.local_adata.varm.keys():\n        set_imputed_counts_refit_adata(self)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.utils","title":"<code>utils</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.utils.get_available_layers","title":"<code>get_available_layers(adata, shared_state, refit=False, all_layers_from_disk=False)</code>","text":"<p>Get the available layers in the adata.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>Optional[AnnData]</code> <p>The local adata.</p> required <code>shared_state</code> <code>dict</code> <p>The shared state containing the Cook's dispersion values.</p> required <code>refit</code> <code>bool</code> <p>Whether to refit the layers.</p> <code>False</code> <code>all_layers_from_disk</code> <code>bool</code> <p>Whether to get all layers from disk.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of available layers.</p> Source code in <code>fedpydeseq2/core/utils/layers/utils.py</code> <pre><code>def get_available_layers(\n    adata: ad.AnnData | None,\n    shared_state: dict | None,\n    refit: bool = False,\n    all_layers_from_disk: bool = False,\n) -&gt; list[str]:\n    \"\"\"Get the available layers in the adata.\n\n    Parameters\n    ----------\n    adata : Optional[ad.AnnData]\n        The local adata.\n\n    shared_state : dict\n        The shared state containing the Cook's dispersion values.\n\n    refit : bool\n        Whether to refit the layers.\n\n    all_layers_from_disk : bool\n        Whether to get all layers from disk.\n\n    Returns\n    -------\n    list[str]\n        List of available layers.\n    \"\"\"\n    if adata is None:\n        return []\n    if all_layers_from_disk:\n        return list(adata.layers.keys())\n    available_layers = []\n    if can_get_normed_counts(adata, raise_error=False):\n        available_layers.append(\"normed_counts\")\n    if can_get_y_hat(adata, raise_error=False):\n        available_layers.append(\"_y_hat\")\n    if can_get_mu_hat(adata, raise_error=False):\n        available_layers.append(\"_mu_hat\")\n    if can_get_fit_lin_mu_hat(adata, raise_error=False):\n        available_layers.append(\"_fit_lin_mu_hat\")\n    if can_get_sqerror_layer(adata, raise_error=False):\n        available_layers.append(\"sqerror\")\n    if not refit and can_set_cooks_layer(\n        adata, shared_state=shared_state, raise_error=False\n    ):\n        available_layers.append(\"cooks\")\n    if not refit and can_set_hat_diagonals_layer(\n        adata, shared_state=shared_state, raise_error=False\n    ):\n        available_layers.append(\"_hat_diagonals\")\n    if can_set_mu_layer(\n        adata, lfc_param_name=\"LFC\", mu_param_name=\"_mu_LFC\", raise_error=False\n    ):\n        available_layers.append(\"_mu_LFC\")\n    if can_set_mu_layer(\n        adata,\n        lfc_param_name=\"_mu_hat_LFC\",\n        mu_param_name=\"_irls_mu_hat\",\n        raise_error=False,\n    ):\n        available_layers.append(\"_irls_mu_hat\")\n\n    return available_layers\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.utils.load_layers","title":"<code>load_layers(adata, shared_state, layers_to_load, n_jobs=1, joblib_verbosity=0, joblib_backend='loky', batch_size=100)</code>","text":"<p>Load the simple layers from the data_from_opener and the adata object.</p> <p>This function loads the layers in the layers_to_load attribute in the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object to load the layers into.</p> required <code>shared_state</code> <code>dict</code> <p>The shared state containing the Cook's dispersion values.</p> required <code>layers_to_load</code> <code>list[str]</code> <p>The list of layers to load.</p> required <code>n_jobs</code> <code>int</code> <p>The number of jobs to use for parallel processing.</p> <code>1</code> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity level of joblib.</p> <code>0</code> <code>joblib_backend</code> <code>str</code> <p>The joblib backend to use.</p> <code>'loky'</code> <code>batch_size</code> <code>int</code> <p>The batch size for parallel processing.</p> <code>100</code> Source code in <code>fedpydeseq2/core/utils/layers/utils.py</code> <pre><code>def load_layers(\n    adata: ad.AnnData,\n    shared_state: dict | None,\n    layers_to_load: list[str],\n    n_jobs: int = 1,\n    joblib_verbosity: int = 0,\n    joblib_backend: str = \"loky\",\n    batch_size: int = 100,\n):\n    \"\"\"Load the simple layers from the data_from_opener and the adata object.\n\n    This function loads the layers in the layers_to_load attribute in the\n    adata object.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The AnnData object to load the layers into.\n\n    shared_state : dict, optional\n        The shared state containing the Cook's dispersion values.\n\n    layers_to_load : list[str]\n        The list of layers to load.\n\n    n_jobs : int\n        The number of jobs to use for parallel processing.\n\n    joblib_verbosity : int\n        The verbosity level of joblib.\n\n    joblib_backend : str\n        The joblib backend to use.\n\n    batch_size : int\n        The batch size for parallel processing.\n    \"\"\"\n    # Assert that all layers are either complex or simple\n    assert np.all(\n        layer in AVAILABLE_LAYERS for layer in layers_to_load\n    ), f\"All layers in layers_to_load must be in {AVAILABLE_LAYERS}\"\n\n    if \"normed_counts\" in layers_to_load:\n        set_normed_counts(adata=adata)\n    if \"_mu_LFC\" in layers_to_load:\n        set_mu_layer(\n            local_adata=adata,\n            lfc_param_name=\"LFC\",\n            mu_param_name=\"_mu_LFC\",\n            n_jobs=n_jobs,\n            joblib_verbosity=joblib_verbosity,\n            joblib_backend=joblib_backend,\n            batch_size=batch_size,\n        )\n    if \"_irls_mu_hat\" in layers_to_load:\n        set_mu_layer(\n            local_adata=adata,\n            lfc_param_name=\"_mu_hat_LFC\",\n            mu_param_name=\"_irls_mu_hat\",\n            n_jobs=n_jobs,\n            joblib_verbosity=joblib_verbosity,\n            joblib_backend=joblib_backend,\n            batch_size=batch_size,\n        )\n    if \"sqerror\" in layers_to_load:\n        set_sqerror_layer(adata)\n    if \"_y_hat\" in layers_to_load:\n        set_y_hat(adata)\n    if \"_fit_lin_mu_hat\" in layers_to_load:\n        set_fit_lin_mu_hat(adata)\n    if \"_mu_hat\" in layers_to_load:\n        set_mu_hat_layer(adata)\n    if \"_hat_diagonals\" in layers_to_load:\n        set_hat_diagonals_layer(adata=adata, shared_state=shared_state)\n    if \"cooks\" in layers_to_load:\n        set_cooks_layer(adata=adata, shared_state=shared_state)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.layers.utils.remove_layers","title":"<code>remove_layers(adata, layers_to_save_on_disk, refit=False)</code>","text":"<p>Remove the simple layers from the adata object.</p> <p>This function removes the simple layers from the adata object. The layers_to_save parameter can be used to specify which layers to save in the local state. If layers_to_save is None, no layers are saved.</p> <p>This function also adds all present layers to the _available_layers field in the adata object. This field is used to keep track of the layers that are present in the adata object.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>The AnnData object to remove the layers from.</p> required <code>refit</code> <code>bool</code> <p>Whether the adata object is the refit_adata object.</p> <code>False</code> <code>layers_to_save_on_disk</code> <code>list[str]</code> <p>The list of layers to save. If None, no layers are saved.</p> required Source code in <code>fedpydeseq2/core/utils/layers/utils.py</code> <pre><code>def remove_layers(\n    adata: ad.AnnData,\n    layers_to_save_on_disk: list[str],\n    refit: bool = False,\n):\n    \"\"\"Remove the simple layers from the adata object.\n\n    This function removes the simple layers from the adata object. The layers_to_save\n    parameter can be used to specify which layers to save in the local state.\n    If layers_to_save is None, no layers are saved.\n\n    This function also adds all present layers to the _available_layers field in the\n    adata object. This field is used to keep track of the layers that are present in\n    the adata object.\n\n    Parameters\n    ----------\n    adata : ad.AnnData\n        The AnnData object to remove the layers from.\n\n    refit : bool\n        Whether the adata object is the refit_adata object.\n\n    layers_to_save_on_disk : list[str]\n        The list of layers to save. If None, no layers are saved.\n    \"\"\"\n    adata.X = None\n    if refit:\n        adata.obsm = None\n\n    layer_names = list(adata.layers.keys()).copy()\n    for layer_name in layer_names:\n        if layer_name in layers_to_save_on_disk:\n            continue\n        del adata.layers[layer_name]\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging","title":"<code>logging</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators","title":"<code>logging_decorators</code>","text":"<p>Module containing decorators to log the input and outputs of a method.</p> <p>All logging is controlled through a logging configuration file. This configuration file can be either set by the log_config_path attribute of the class, or by the default_config.ini file in the same directory as this module.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.end_iteration","title":"<code>end_iteration()</code>","text":"<p>Add the  balise to the logging file.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def end_iteration():\n    \"\"\"Add the &lt;/iteration&gt; balise to the logging file.\"\"\"\n    # Add &lt;/iteration&gt; balise\n    text_to_add = \"&lt;/iteration&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.end_loop","title":"<code>end_loop()</code>","text":"<p>Add the  balise to the logging file.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def end_loop():\n    \"\"\"Add the &lt;/iterations&gt; balise to the logging file.\"\"\"\n    # Add &lt;/iterations&gt; balise\n    text_to_add = \"&lt;/iterations&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.get_method_logger","title":"<code>get_method_logger(method)</code>","text":"<p>Get the method logger from a configuration file.</p> <p>If the class instance has a log_config_path attribute, the logger is configured with the file at this path.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The class instance</p> required <code>method</code> <code>Callable</code> <p>The class method.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>The logger instance.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def get_method_logger(method: Callable) -&gt; logging.Logger:\n    \"\"\"Get the method logger from a configuration file.\n\n    If the class instance has a log_config_path attribute,\n    the logger is configured with the file at this path.\n\n    Parameters\n    ----------\n    self: Any\n        The class instance\n    method: Callable\n        The class method.\n\n    Returns\n    -------\n    logging.Logger\n        The logger instance.\n    \"\"\"\n    logger_config_path = get_logger_configuration()\n    if logger_config_path is None:\n        # Take basic configuration\n        logging.basicConfig()\n        logger = logging.getLogger(method.__name__)\n        return logger\n    try:\n        logging.config.fileConfig(logger_config_path, disable_existing_loggers=False)\n        logger = logging.getLogger(method.__name__)\n    except Exception as e:  # noqa: BLE001\n        logging.basicConfig()\n        logger = logging.getLogger(method.__name__)\n        logger.warning(\n            f\"Error while trying to configure the logger with the file at \"\n            f\"{logger_config_path}. Using basic configuration. Error : {e}\"\n        )\n    return logger\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.get_shared_state_balises","title":"<code>get_shared_state_balises(shared_state)</code>","text":"<p>Get the shared state balises.</p> <p>Parameters:</p> Name Type Description Default <code>shared_state</code> <code>Any</code> <p>The shared state containing the inputs to be logged. Expected to be a dictionary.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The shared state balises.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def get_shared_state_balises(shared_state: Any) -&gt; str:\n    \"\"\"Get the shared state balises.\n\n    Parameters\n    ----------\n    shared_state : Any\n        The shared state containing the inputs to be logged.\n        Expected to be a dictionary.\n\n    Returns\n    -------\n    str\n        The shared state balises.\n    \"\"\"\n    text_to_add = \"\"\n    # For each key in the shared state, add a input basise withe three sub\n    # balises : key, type and shape if relevant\n    if isinstance(shared_state, dict):\n        for key, value in shared_state.items():\n            text_to_add += \"&lt;item&gt;\\n\"\n            text_to_add += f\"&lt;key&gt;{key}&lt;/key&gt;\\n\"\n            text_to_add += f\"&lt;type&gt;{type(value)}&lt;/type&gt;\\n\"\n            if hasattr(value, \"shape\"):\n                text_to_add += f\"&lt;shape&gt;{value.shape}&lt;/shape&gt;\\n\"\n            text_to_add += \"&lt;/item&gt;\\n\"\n        return text_to_add\n    return \"\"\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.log_organisation_method","title":"<code>log_organisation_method(method)</code>","text":"<p>Decorate a method to log when it is called and when it ends.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method to decorate. This method is expected to have the following signature: method(self, args, *kwargs).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated method, which logs when it is called and when it ends.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def log_organisation_method(method: Callable):\n    \"\"\"Decorate a method to log when it is called and when it ends.\n\n    Parameters\n    ----------\n    method : Callable\n        The method to decorate. This method is expected to have the following signature:\n        method(self, *args, **kwargs).\n\n    Returns\n    -------\n    Callable\n        The decorated method, which logs when it is called and when it ends.\n    \"\"\"\n\n    @wraps(method)\n    def method_inner(\n        self,\n        *args,\n        **kwargs,\n    ):\n        write_info_before_organisation_method(method)\n        output = method(self, *args, **kwargs)\n        write_info_after_organisation_method()\n\n        return output\n\n    return method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.log_remote","title":"<code>log_remote(method)</code>","text":"<p>Decorate a remote method to log the input and outputs.</p> <p>This decorator logs the shared state keys with the info level.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method to decorate. This method is expected to have the following signature: method(self, shared_states: Optional[list], **method_parameters).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated method, which logs the shared state keys with the info level.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def log_remote(method: Callable):\n    \"\"\"Decorate a remote method to log the input and outputs.\n\n    This decorator logs the shared state keys with the info level.\n\n    Parameters\n    ----------\n    method : Callable\n        The method to decorate. This method is expected to have the following signature:\n        method(self, shared_states: Optional[list], **method_parameters).\n\n    Returns\n    -------\n    Callable\n        The decorated method, which logs the shared state keys with the info level.\n    \"\"\"\n\n    @wraps(method)\n    def remote_method_inner(\n        self,\n        shared_states: list | None,\n        **method_parameters,\n    ):\n        log_flag = log_shared_state_adata_flag()\n\n        if log_flag:\n            logger = get_method_logger(method)\n            if shared_states is not None:\n                shared_state = shared_states[0]\n                if shared_state is not None:\n                    logger.info(\n                        f\"First input shared state keys : {list(shared_state.keys())}\"\n                    )\n                else:\n                    logger.info(\"First input shared state is None.\")\n            else:\n                logger.info(\"No input shared states.\")\n        write_info_before_function(\n            method,\n            shared_states[0] if isinstance(shared_states, list) else None,  # type: ignore\n            \"remote\",\n        )\n\n        shared_state = method(self, shared_states, **method_parameters)\n\n        write_info_after_function(shared_state, \"remote\")\n\n        if log_flag:\n            if shared_state is not None:\n                logger.info(f\"Output shared state keys : {list(shared_state.keys())}\")\n            else:\n                logger.info(\"No output shared state.\")\n\n        return shared_state\n\n    return remote_method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.log_remote_data","title":"<code>log_remote_data(method)</code>","text":"<p>Decorate a remote_data to log the input and outputs.</p> <p>This decorator logs the shared state keys with the info level, and the different layers of the local_adata and refit_adata with the debug level.</p> <p>This is done before and after the method call.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method to decorate. This method is expected to have the following signature: method(self, data_from_opener: ad.AnnData, shared_state: Any = None, **method_parameters).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated method, which logs the shared state keys with the info level and the different layers of the local_adata and refit_adata with the debug level.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def log_remote_data(method: Callable):\n    \"\"\"Decorate a remote_data to log the input and outputs.\n\n    This decorator logs the shared state keys with the info level,\n    and the different layers of the local_adata and refit_adata with the debug level.\n\n    This is done before and after the method call.\n\n    Parameters\n    ----------\n    method : Callable\n        The method to decorate. This method is expected to have the following signature:\n        method(self, data_from_opener: ad.AnnData,\n        shared_state: Any = None, **method_parameters).\n\n    Returns\n    -------\n    Callable\n        The decorated method, which logs the shared state keys with the info level\n        and the different layers of the local_adata and refit_adata with the debug\n        level.\n    \"\"\"\n\n    @wraps(method)\n    def remote_method_inner(\n        self,\n        data_from_opener: ad.AnnData,\n        shared_state: Any = None,\n        **method_parameters,\n    ):\n        log_shared_state_adatas(\n            self, method, shared_state, \"---- Before running the method ----\"\n        )\n        write_info_before_function(method, shared_state, \"remote_data\")\n\n        shared_state = method(self, data_from_opener, shared_state, **method_parameters)\n\n        write_info_after_function(shared_state, \"remote_data\")\n        log_shared_state_adatas(self, method, shared_state, \"---- After method ----\")\n        return shared_state\n\n    return remote_method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.log_save_local_state","title":"<code>log_save_local_state(method)</code>","text":"<p>Decorate a method to log the size of the saved local state.</p> <p>This function is destined to decorate the save_local_state method of a class.</p> <p>It logs the size of the local state saved in the local state path, in MB. This is logged as an info message.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method to decorate. This method is expected to have the following signature: method(self, path: pathlib.Path).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated method, which logs the size of the local state saved.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def log_save_local_state(method: Callable):\n    \"\"\"Decorate a method to log the size of the saved local state.\n\n    This function is destined to decorate the save_local_state method of a class.\n\n    It logs the size of the local state saved in the local state path, in MB.\n    This is logged as an info message.\n\n    Parameters\n    ----------\n    method : Callable\n        The method to decorate. This method is expected to have the following signature:\n        method(self, path: pathlib.Path).\n\n    Returns\n    -------\n    Callable\n        The decorated method, which logs the size of the local state saved.\n    \"\"\"\n\n    @wraps(method)\n    def remote_method_inner(\n        self,\n        path: pathlib.Path,\n    ):\n        output = method(self, path)\n\n        if log_shared_state_size_flag():\n            logger = get_method_logger(method)\n            logger.info(\n                f\"Size of saved local state: \"\n                f\"{os.path.getsize(path) / 1024 / 1024}\"\n                \" MB\"\n            )\n\n        return output\n\n    return remote_method_inner\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.log_shared_state_adatas","title":"<code>log_shared_state_adatas(self, method, shared_state, message)</code>","text":"<p>Log the information of the local step.</p> <p>Precisely, log the shared state keys (info), and the different layers of the local_adata and refit_adata (debug).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The class instance</p> required <code>method</code> <code>Callable</code> <p>The class method.</p> required <code>shared_state</code> <code>Optional[dict]</code> <p>The shared state dictionary, whose keys we log with the info level.</p> required <code>message</code> <code>str or None</code> <p>A message to log before the shared state keys.</p> required Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def log_shared_state_adatas(\n    self: Any, method: Callable, shared_state: dict | None, message: str | None\n):\n    \"\"\"Log the information of the local step.\n\n    Precisely, log the shared state keys (info),\n    and the different layers of the local_adata and refit_adata (debug).\n\n    Parameters\n    ----------\n    self : Any\n        The class instance\n    method : Callable\n        The class method.\n    shared_state : Optional[dict]\n        The shared state dictionary, whose keys we log with the info level.\n    message : str or None\n        A message to log before the shared state keys.\n    \"\"\"\n    log_flag = log_shared_state_adata_flag()\n    if not log_flag:\n        return\n    logger = get_method_logger(method)\n    if message is not None:\n        logger.info(message)\n\n    if shared_state is not None:\n        logger.info(f\"Shared state keys : {list(shared_state.keys())}\")\n    else:\n        logger.info(\"No shared state\")\n\n    for adata_name in [\"local_adata\", \"refit_adata\"]:\n        if hasattr(self, adata_name) and getattr(self, adata_name) is not None:\n            adata = getattr(self, adata_name)\n            logger.debug(f\"{adata_name} layers : {list(adata.layers.keys())}\")\n            if \"_available_layers\" in self.local_adata.uns:\n                available_layers = self.local_adata.uns[\"_available_layers\"]\n                logger.debug(f\"{adata_name} available layers : {available_layers}\")\n            logger.debug(f\"{adata_name} uns keys : {list(adata.uns.keys())}\")\n            logger.debug(f\"{adata_name} varm keys : {list(adata.varm.keys())}\")\n            logger.debug(f\"{adata_name} obsm keys : {list(adata.obsm.keys())}\")\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.start_iteration","title":"<code>start_iteration(iteration_number)</code>","text":"<p>Add the  balise to the logging file. <p>Parameters:</p> Name Type Description Default <code>iteration_number</code> <code>int</code> <p>The number of the iteration.</p> required Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def start_iteration(iteration_number: int):\n    \"\"\"Add the &lt;iteration&gt; balise to the logging file.\n\n    Parameters\n    ----------\n    iteration_number : int\n        The number of the iteration.\n    \"\"\"\n    # Add &lt;iteration&gt; balise\n    text_to_add = \"&lt;iteration&gt;\\n\"\n    # Add iteration number balise\n    text_to_add += f\"&lt;number&gt;{iteration_number}&lt;/number&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.start_loop","title":"<code>start_loop()</code>","text":"<p>Add the  balise to the logging file. Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def start_loop():\n    \"\"\"Add the &lt;iterations&gt; balise to the logging file.\"\"\"\n    # Add &lt;iterations&gt; balise\n    text_to_add = \"&lt;iterations&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.write_info_after_function","title":"<code>write_info_after_function(shared_state, function_type)</code>","text":"<p>Append the information of the local step to a file.</p> <p>Parameters:</p> Name Type Description Default <code>shared_state</code> <code>Any</code> <p>The shared state containing the inputs to be logged. Expected to be a dictionary.</p> required <code>function_type</code> <code>str</code> <p>The type of the function (local, remote or remote_data).</p> required Notes <p>This function appends the following information to the specified file: - The name of the method enclosed in <code>&lt;name&gt;</code> tags. - The outputs from the shared state enclosed in <code>&lt;outputs&gt;</code> tags.   Each output includes:     - <code>&lt;key&gt;</code>: The key of the output.     - <code>&lt;type&gt;</code>: The type of the output.     - <code>&lt;shape&gt;</code>: The shape of the output, if applicable.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def write_info_after_function(shared_state: Any, function_type: str):\n    \"\"\"Append the information of the local step to a file.\n\n    Parameters\n    ----------\n    shared_state : Any\n        The shared state containing the inputs to be logged.\n        Expected to be a dictionary.\n    function_type : str\n        The type of the function (local, remote or remote_data).\n\n    Notes\n    -----\n    This function appends the following information to the specified file:\n    - The name of the method enclosed in `&lt;name&gt;` tags.\n    - The outputs from the shared state enclosed in `&lt;outputs&gt;` tags.\n      Each output includes:\n        - `&lt;key&gt;`: The key of the output.\n        - `&lt;type&gt;`: The type of the output.\n        - `&lt;shape&gt;`: The shape of the output, if applicable.\n    \"\"\"\n    # Add outputs opening balise\n    text_to_add = \"&lt;output&gt;\\n\"\n    # For each key in the shared state, add a output balise withe three sub\n    # balises : key, type and shape if relevant\n    text_to_add += get_shared_state_balises(shared_state)\n    text_to_add += \"&lt;/output&gt;\\n\"\n    text_to_add += f\"&lt;/{function_type}&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.write_info_after_organisation_method","title":"<code>write_info_after_organisation_method()</code>","text":"<p>Mark the end of the organisation method in the logging file.</p> Notes <p>This function appends the following information to the specified file: - The name of the method enclosed in <code>&lt;name&gt;</code> tags.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def write_info_after_organisation_method():\n    \"\"\"Mark the end of the organisation method in the logging file.\n\n    Notes\n    -----\n    This function appends the following information to the specified file:\n    - The name of the method enclosed in `&lt;name&gt;` tags.\n    \"\"\"\n    text_to_add = \"&lt;/bloc&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.write_info_before_function","title":"<code>write_info_before_function(method, shared_state, function_type)</code>","text":"<p>Append the information of the local step to a file.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method whose name will be logged.</p> required <code>shared_state</code> <code>Any</code> <p>The shared state containing the inputs to be logged. Expected to be a dictionary.</p> required <code>function_type</code> <code>str</code> <p>The type of the function (local, remote or remote_data).</p> required Notes <p>This function appends the following information to the specified file: - The name of the method enclosed in <code>&lt;name&gt;</code> tags. - The inputs from the shared state enclosed in <code>&lt;inputs&gt;</code> tags. Each input includes:     - <code>&lt;key&gt;</code>: The key of the input.     - <code>&lt;type&gt;</code>: The type of the input.     - <code>&lt;shape&gt;</code>: The shape of the input, if applicable.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def write_info_before_function(method: Callable, shared_state: Any, function_type: str):\n    \"\"\"Append the information of the local step to a file.\n\n    Parameters\n    ----------\n    method : Callable\n        The method whose name will be logged.\n    shared_state : Any\n        The shared state containing the inputs to be logged.\n        Expected to be a dictionary.\n    function_type : str\n        The type of the function (local, remote or remote_data).\n\n    Notes\n    -----\n    This function appends the following information to the specified file:\n    - The name of the method enclosed in `&lt;name&gt;` tags.\n    - The inputs from the shared state enclosed in `&lt;inputs&gt;` tags. Each input includes:\n        - `&lt;key&gt;`: The key of the input.\n        - `&lt;type&gt;`: The type of the input.\n        - `&lt;shape&gt;`: The shape of the input, if applicable.\n    \"\"\"\n    text_to_add = f\"&lt;{function_type}&gt;\\n\"\n    # Add name balise\n    text_to_add += f\"&lt;name&gt;{method.__name__}&lt;/name&gt;\\n\"\n    # Add inputs opening balise\n    text_to_add += \"&lt;input&gt;\\n\"\n    # For each key in the shared state, add a input basise withe three sub\n    # balises : key, type and shape if relevant\n    text_to_add += get_shared_state_balises(shared_state)\n\n    text_to_add += \"&lt;/input&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.logging_decorators.write_info_before_organisation_method","title":"<code>write_info_before_organisation_method(method)</code>","text":"<p>Add the organisation moethod name to the logging file.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>The method whose name will be logged.</p> required Notes <p>This function appends the following information to the specified file: - The name of the method enclosed in <code>&lt;name&gt;</code> tags.</p> Source code in <code>fedpydeseq2/core/utils/logging/logging_decorators.py</code> <pre><code>def write_info_before_organisation_method(method: Callable):\n    \"\"\"Add the organisation moethod name to the logging file.\n\n    Parameters\n    ----------\n    method : Callable\n        The method whose name will be logged.\n\n    Notes\n    -----\n    This function appends the following information to the specified file:\n    - The name of the method enclosed in `&lt;name&gt;` tags.\n    \"\"\"\n    text_to_add = \"&lt;bloc&gt;\\n\"\n    # Add name balise\n    text_to_add += f\"&lt;name&gt;{method.__name__}&lt;/name&gt;\\n\"\n    # Append the text to the file\n    file_path = get_workflow_file()\n    if file_path is not None and file_path.exists():\n        with open(file_path, \"a\") as file:\n            file.write(text_to_add)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils","title":"<code>utils</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.get_logger_configuration","title":"<code>get_logger_configuration()</code>","text":"<p>Return the logger configuration ini path from the log configuration file.</p> <p>Returns:</p> Type Description <code>str or None</code> <p>The logger configuration ini path, or None if not available.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def get_logger_configuration() -&gt; str | None:\n    \"\"\"Return the logger configuration ini path from the log configuration file.\n\n    Returns\n    -------\n    str or None\n        The logger configuration ini path, or None if not available.\n    \"\"\"\n    config = read_log_config_path()\n    if config is None:\n        return None\n    return config.get(\"logger_configuration_ini_path\")\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.get_workflow_configuration","title":"<code>get_workflow_configuration()</code>","text":"<p>Return the workflow dictionary generated from the log configuration file.</p> <p>Returns:</p> Type Description <code>dict or None</code> <p>The generate workflow dictionary, or None if not available.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def get_workflow_configuration() -&gt; dict[str, Any] | None:\n    \"\"\"Return the workflow dictionary generated from the log configuration file.\n\n    Returns\n    -------\n    dict or None\n        The generate workflow dictionary, or None if not available.\n    \"\"\"\n    config = read_log_config_path()\n    if config is None:\n        return None\n    return config.get(\"generate_workflow\")\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.get_workflow_file","title":"<code>get_workflow_file()</code>","text":"<p>Return the workflow file path if the configuration is set to True.</p> <p>Returns:</p> Type Description <code>Path or None</code> <p>The workflow file path, or None if not available.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def get_workflow_file() -&gt; Path | None:\n    \"\"\"Return the workflow file path if the configuration is set to True.\n\n    Returns\n    -------\n    Path or None\n        The workflow file path, or None if not available.\n    \"\"\"\n    workflow_config = get_workflow_configuration()\n    if workflow_config is not None:\n        create_workflow = workflow_config.get(\"create_workflow\")\n        if create_workflow:\n            workflow_file = workflow_config.get(\"workflow_file_path\")\n            assert workflow_file is not None, \"Workflow file path is not provided.\"\n            workflow_file_path = Path(workflow_file)\n            assert workflow_file_path.exists(), \"Workflow file does not exist.\"\n            return workflow_file_path\n    return None\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.log_shared_state_adata_flag","title":"<code>log_shared_state_adata_flag()</code>","text":"<p>Return the log_adata_content flag from the log configuration file.</p> <p>Returns:</p> Type Description <code>bool or None</code> <p>The log_adata_content flag, or None if not available.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def log_shared_state_adata_flag() -&gt; bool | None:\n    \"\"\"Return the log_adata_content flag from the log configuration file.\n\n    Returns\n    -------\n    bool or None\n        The log_adata_content flag, or None if not available.\n    \"\"\"\n    config = read_log_config_path()\n    if config is None:\n        return None\n    return config.get(\"log_shared_state_adata_content\")\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.log_shared_state_size_flag","title":"<code>log_shared_state_size_flag()</code>","text":"<p>Return the log_shared_state_size flag from the log configuration file.</p> <p>Returns:</p> Type Description <code>bool or None</code> <p>The log_shared_state_size flag, or None if not available.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def log_shared_state_size_flag() -&gt; bool | None:\n    \"\"\"Return the log_shared_state_size flag from the log configuration file.\n\n    Returns\n    -------\n    bool or None\n        The log_shared_state_size flag, or None if not available.\n    \"\"\"\n    config = read_log_config_path()\n    if config is None:\n        return None\n    return config.get(\"log_shared_state_size\")\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.read_log_config_path","title":"<code>read_log_config_path()</code>","text":"<p>Read the log_config_path.json file and return its content as a dictionary.</p> <p>Returns:</p> Type Description <code>dict or None</code> <p>The content of the log_config_path.json file as a dictionary, or None if the file does not exist or the path is null.</p> Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def read_log_config_path() -&gt; dict[str, Any] | None:\n    \"\"\"Read the log_config_path.json file and return its content as a dictionary.\n\n    Returns\n    -------\n    dict or None\n        The content of the log_config_path.json file as a dictionary,\n        or None if the file does not exist or the path is null.\n    \"\"\"\n    current_dir = Path(__file__).parent\n    config_file_path = current_dir / \"log_config_path.json\"\n\n    if not config_file_path.exists():\n        return None\n\n    with config_file_path.open(\"r\") as config_file:\n        config_content = json.load(config_file)\n\n    log_config_path = config_content.get(\"log_config_path\")\n    if log_config_path is None:\n        return None\n\n    log_config_path = Path(log_config_path)\n    if not log_config_path.exists():\n        raise FileNotFoundError(\n            f\"The log configuration file at {log_config_path} does not exist.\"\n        )\n\n    with log_config_path.open(\"r\") as log_config_file:\n        log_config = json.load(log_config_file)\n        return log_config\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.set_log_config_path","title":"<code>set_log_config_path(log_config_path)</code>","text":"<p>Create a log_config_path.json in the same directory as this Python file.</p> <p>The JSON file contains one key: 'log_config_path' which is set to null if <code>log_config_path</code> is None, or the string representation of the path if a path is provided. If the file already exists, a warning is raised using loguru, but the file is overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>log_config_path</code> <code>str or Path or None</code> <p>The path to be set in the JSON configuration file. If None, the value will be null.</p> required Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def set_log_config_path(log_config_path: str | Path | None) -&gt; None:\n    \"\"\"Create a log_config_path.json in the same directory as this Python file.\n\n    The JSON file contains one key: 'log_config_path' which is set to null if\n    `log_config_path` is None, or the string representation of the path if a\n    path is provided.\n    If the file already exists, a warning is raised using loguru, but the file\n    is overwritten.\n\n    Parameters\n    ----------\n    log_config_path : str or Path or None\n        The path to be set in the JSON configuration file. If None, the value\n        will be null.\n    \"\"\"\n    # Determine the directory of the current file\n    current_dir = Path(__file__).parent\n\n    # Define the path to the log_config_path.json file\n    config_file_path = current_dir / \"log_config_path.json\"\n\n    # Check if the file already exists and log a warning if it does\n    if config_file_path.exists():\n        logger.warning(f\"{config_file_path} already exists and will be overwritten.\")\n\n    # Prepare the content to be written to the JSON file\n    config_content = {\n        \"log_config_path\": str(log_config_path) if log_config_path is not None else None\n    }\n\n    # Write the content to the JSON file\n    with config_file_path.open(\"w\") as config_file:\n        json.dump(config_content, config_file, indent=4)\n\n    # Set up the workflow file\n    if log_config_path is not None:\n        setup_workflow_file(log_config_path)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.logging.utils.setup_workflow_file","title":"<code>setup_workflow_file(log_config_path)</code>","text":"<p>Create the workflow file if the configuration is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>log_config_path</code> <code>str or Path</code> <p>The path to the log configuration file.</p> required Source code in <code>fedpydeseq2/core/utils/logging/utils.py</code> <pre><code>def setup_workflow_file(log_config_path: str | Path) -&gt; None:\n    \"\"\"Create the workflow file if the configuration is set to True.\n\n    Parameters\n    ----------\n    log_config_path : str or Path\n        The path to the log configuration file.\n    \"\"\"\n    log_config_path = Path(log_config_path)\n    if not log_config_path.exists():\n        return\n    with log_config_path.open(\"r\") as log_config_file:\n        log_config = json.load(log_config_file)\n\n    workflow_config = log_config.get(\"generate_workflow\")\n    if workflow_config is None:\n        return\n    # Get the create workflow flag\n    create_workflow = workflow_config.get(\"create_workflow\")\n    if create_workflow is None or not create_workflow:\n        return\n    # Get the workflow file path\n    workflow_file_path = workflow_config.get(\"workflow_file_path\")\n    # If it is not provided, return\n    if workflow_file_path is None:\n        return\n    # Create parent directories and the file\n    workflow_file_path = Path(workflow_file_path)\n    workflow_file_path.parent.mkdir(parents=True, exist_ok=True)\n    workflow_file_path.touch(exist_ok=True)\n\n    clean_workflow = workflow_config.get(\"clean_workflow_files\")\n    if clean_workflow is None or not clean_workflow:\n        return\n    # Clean the workflow file\n    with workflow_file_path.open(\"w\") as workflow_file:\n        workflow_file.write(\"\")\n    return\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle","title":"<code>mle</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.batch_mle_grad","title":"<code>batch_mle_grad(counts, design, mu, alpha)</code>","text":"<p>Estimate the local gradients wrt dispersions on a batch of genes.</p> <p>Returns both the gradient of the negative likelihood, and two matrices used to compute the gradient of the Cox-Reid adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Raw counts for a set of genes (n_samples x n_genes).</p> required <code>design</code> <code>ndarray</code> <p>Design matrix (n_samples x n_params).</p> required <code>mu</code> <code>ndarray</code> <p>Mean estimation for the NB model (n_samples x n_genes).</p> required <code>alpha</code> <code>float</code> <p>Initial dispersion estimate (nn_genes).</p> required <p>Returns:</p> Name Type Description <code>grad</code> <code>ndarray</code> <p>Gradient of the negative log likelihood of the observations counts following :math:<code>NB(\\\\mu, \\\\alpha)</code> (n_genes).</p> <code>M1</code> <code>ndarray</code> <p>First summand for the gradient of the CR adjustment (n_genes x n_params x n_params).</p> <code>M2</code> <code>ndarray</code> <p>Second summand for the gradient of the CR adjustment (n_genes x n_params x n_params).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def batch_mle_grad(\n    counts: np.ndarray, design: np.ndarray, mu: np.ndarray, alpha: np.ndarray\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    r\"\"\"Estimate the local gradients wrt dispersions on a batch of genes.\n\n    Returns both the gradient of the negative likelihood, and two matrices used to\n    compute the gradient of the Cox-Reid adjustment.\n\n\n    Parameters\n    ----------\n    counts : ndarray\n        Raw counts for a set of genes (n_samples x n_genes).\n\n    design : ndarray\n        Design matrix (n_samples x n_params).\n\n    mu : ndarray\n        Mean estimation for the NB model (n_samples x n_genes).\n\n    alpha : float\n        Initial dispersion estimate (nn_genes).\n\n    Returns\n    -------\n    grad : ndarray\n        Gradient of the negative log likelihood of the observations counts following\n        :math:`NB(\\\\mu, \\\\alpha)` (n_genes).\n\n    M1 : ndarray\n        First summand for the gradient of the CR adjustment\n        (n_genes x n_params x n_params).\n\n    M2 : ndarray\n        Second summand for the gradient of the CR adjustment\n        (n_genes x n_params x n_params).\n    \"\"\"\n    grad = alpha * vec_nb_nll_grad(\n        counts,\n        mu,\n        alpha,\n    )  # Need to multiply by alpha to get the gradient wrt log_alpha\n\n    W = mu / (1 + mu * alpha[None, :])\n\n    dW = -(W**2)\n    M1 = (design.T[:, :, None] * W).transpose(2, 0, 1) @ design[None, :, :]\n    M2 = (design.T[:, :, None] * dW).transpose(2, 0, 1) @ design[None, :, :]\n\n    return grad, M1, M2\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.batch_mle_update","title":"<code>batch_mle_update(log_alpha, global_CR_summand_1, global_CR_summand_2, global_ll_grad, lr, alpha_hat=None, prior_disp_var=None, prior_reg=False)</code>","text":"<p>Perform a global dispersions update on a batch of genes.</p> <p>Parameters:</p> Name Type Description Default <code>log_alpha</code> <code>ndarray</code> <p>Current global log dispersions (n_genes).</p> required <code>global_CR_summand_1</code> <code>ndarray</code> <p>Global summand 1 for the CR adjustment (n_genes x n_params x n_params).</p> required <code>global_CR_summand_2</code> <code>ndarray</code> <p>Global summand 2 for the CR adjustment (n_genes x n_params x n_params).</p> required <code>global_ll_grad</code> <code>ndarray</code> <p>Global gradient of the negative log likelihood (n_genes).</p> required <code>lr</code> <code>float</code> <p>Learning rate.</p> required <code>alpha_hat</code> <code>ndarray</code> <p>Reference dispersions (for MAP estimation, n_genes).</p> <code>None</code> <code>prior_disp_var</code> <code>float</code> <p>Prior dispersion variance.</p> <code>None</code> <code>prior_reg</code> <code>bool</code> <p>Whether to use prior regularization for MAP estimation (default: <code>False</code>).</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Updated global log dispersions (n_genes).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def batch_mle_update(\n    log_alpha: np.ndarray,\n    global_CR_summand_1: np.ndarray,\n    global_CR_summand_2: np.ndarray,\n    global_ll_grad: np.ndarray,\n    lr: float,\n    alpha_hat: np.ndarray | None = None,\n    prior_disp_var: float | None = None,\n    prior_reg: bool = False,\n):\n    \"\"\"Perform a global dispersions update on a batch of genes.\n\n    Parameters\n    ----------\n    log_alpha : ndarray\n        Current global log dispersions (n_genes).\n\n    global_CR_summand_1 : ndarray\n        Global summand 1 for the CR adjustment (n_genes x n_params x n_params).\n\n    global_CR_summand_2 : ndarray\n        Global summand 2 for the CR adjustment (n_genes x n_params x n_params).\n\n    global_ll_grad : ndarray\n        Global gradient of the negative log likelihood (n_genes).\n\n    lr : float\n        Learning rate.\n\n    alpha_hat : ndarray\n        Reference dispersions (for MAP estimation, n_genes).\n\n    prior_disp_var : float\n        Prior dispersion variance.\n\n    prior_reg : bool\n        Whether to use prior regularization for MAP estimation (default: ``False``).\n\n    Returns\n    -------\n    ndarray\n        Updated global log dispersions (n_genes).\n    \"\"\"\n    # Add prior regularization, if required\n    if prior_reg:\n        global_ll_grad += (log_alpha - np.log(alpha_hat)) / prior_disp_var\n\n    # Compute CR reg grad (not separable, cannot be computed locally)\n    global_CR_grad = np.array(\n        0.5\n        * (np.linalg.inv(global_CR_summand_1) * global_CR_summand_2).sum(1).sum(1)\n        * np.exp(log_alpha)\n    )\n\n    # Update dispersion\n    global_log_alpha = log_alpha - lr * (global_ll_grad + global_CR_grad)\n\n    return global_log_alpha\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.global_grid_cr_loss","title":"<code>global_grid_cr_loss(nll, cr_grid)</code>","text":"<p>Compute the global negative log likelihood on a grid.</p> <p>Sums previously computed local negative log likelihoods and Cox-Reid adjustments.</p> <p>Parameters:</p> Name Type Description Default <code>nll</code> <code>ndarray</code> <p>Negative log likelihoods of size (n_genes x grid_length).</p> required <code>cr_grid</code> <code>ndarray</code> <p>Summands for the Cox-Reid adjustment (n_genes x grid_length x n_params x n_params).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Adjusted negative log likelihood (n_genes x grid_length).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def global_grid_cr_loss(\n    nll: np.ndarray,\n    cr_grid: np.ndarray,\n) -&gt; np.ndarray:\n    \"\"\"Compute the global negative log likelihood on a grid.\n\n    Sums previously computed local negative log likelihoods and Cox-Reid adjustments.\n\n    Parameters\n    ----------\n    nll : ndarray\n        Negative log likelihoods of size (n_genes x grid_length).\n\n    cr_grid : ndarray\n        Summands for the Cox-Reid adjustment\n        (n_genes x grid_length x n_params x n_params).\n\n    Returns\n    -------\n    ndarray\n        Adjusted negative log likelihood (n_genes x grid_length).\n    \"\"\"\n    if np.any(np.isnan(cr_grid)):\n        n_genes, grid_length, n_params, _ = cr_grid.shape\n        cr_grid = cr_grid.reshape(-1, n_params, n_params)\n        mask_nan = np.any(np.isnan(cr_grid), axis=(1, 2))\n        slogdet = np.zeros(n_genes * grid_length, dtype=cr_grid.dtype)\n        slogdet[mask_nan] = np.nan\n        if np.any(~mask_nan):\n            slogdet[~mask_nan] = np.linalg.slogdet(cr_grid[~mask_nan])[1]\n        return nll + 0.5 * slogdet.reshape(n_genes, grid_length)\n    else:\n        return nll + 0.5 * np.linalg.slogdet(cr_grid)[1]\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.local_grid_summands","title":"<code>local_grid_summands(counts, design, mu, alpha_grid)</code>","text":"<p>Compute local summands of the adjusted negative log likelihood on a grid.</p> <p>Includes the Cox-Reid regularization.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Raw counts for a set of genes (n_samples x n_genes).</p> required <code>design</code> <code>ndarray</code> <p>Design matrix (n_samples x n_params).</p> required <code>mu</code> <code>ndarray</code> <p>Mean estimation for the NB model (n_samples x n_genes).</p> required <code>alpha_grid</code> <code>ndarray</code> <p>Dispersion estimates (n_genes x grid_length).</p> required <p>Returns:</p> Name Type Description <code>nll</code> <code>ndarray</code> <p>Negative log likelihoods of size (n_genes x grid_length).</p> <code>cr_matrix</code> <code>ndarray</code> <p>Summands for the Cox-Reid adjustment (n_genes x grid_length x n_params x n_params).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def local_grid_summands(\n    counts: np.ndarray,\n    design: np.ndarray,\n    mu: np.ndarray,\n    alpha_grid: np.ndarray,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute local summands of the adjusted negative log likelihood on a grid.\n\n    Includes the Cox-Reid regularization.\n\n    Parameters\n    ----------\n    counts : ndarray\n        Raw counts for a set of genes (n_samples x n_genes).\n\n    design : ndarray\n        Design matrix (n_samples x n_params).\n\n    mu : ndarray\n        Mean estimation for the NB model (n_samples x n_genes).\n\n    alpha_grid : ndarray\n        Dispersion estimates (n_genes x grid_length).\n\n    Returns\n    -------\n    nll : ndarray\n        Negative log likelihoods of size (n_genes x grid_length).\n\n    cr_matrix : ndarray\n        Summands for the Cox-Reid adjustment\n        (n_genes x grid_length x n_params x n_params).\n    \"\"\"\n    # W is of size (n_samples x n_genes x grid_length)\n    W = mu[:, :, None] / (1 + mu[:, :, None] * alpha_grid)\n    # cr_matrix is of size (n_genes x grid_length x n_params x n_params)\n    cr_matrix = (design.T[:, :, None, None] * W).transpose(2, 3, 0, 1) @ design[\n        None, None, :, :\n    ]\n    # cr_matrix is of size (n_genes x grid_length)\n    nll = grid_nb_nll(counts, mu, alpha_grid)\n\n    return nll, cr_matrix\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.single_mle_grad","title":"<code>single_mle_grad(counts, design, mu, alpha)</code>","text":"<p>Estimate the local gradients of a negative binomial GLM wrt dispersions.</p> <p>Returns both the gradient of the negative likelihood, and two matrices used to compute the gradient of the Cox-Reid adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Raw counts for a given gene (n_samples).</p> required <code>design</code> <code>ndarray</code> <p>Design matrix (n_samples x n_params).</p> required <code>mu</code> <code>ndarray</code> <p>Mean estimation for the NB model (n_samples).</p> required <code>alpha</code> <code>float</code> <p>Initial dispersion estimate (1).</p> required <p>Returns:</p> Name Type Description <code>grad</code> <code>ndarray</code> <p>Gradient of the negative log likelihood of the observations counts following :math:<code>NB(\\\\mu, \\\\alpha)</code> (1).</p> <code>M1</code> <code>ndarray</code> <p>First summand for the gradient of the CR adjustment (n_params x n_params).</p> <code>M2</code> <code>ndarray</code> <p>Second summand for the gradient of the CR adjustment (n_params x n_params).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def single_mle_grad(\n    counts: np.ndarray, design: np.ndarray, mu: np.ndarray, alpha: float\n) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    r\"\"\"Estimate the local gradients of a negative binomial GLM wrt dispersions.\n\n    Returns both the gradient of the negative likelihood, and two matrices used to\n    compute the gradient of the Cox-Reid adjustment.\n\n\n    Parameters\n    ----------\n    counts : ndarray\n        Raw counts for a given gene (n_samples).\n\n    design : ndarray\n        Design matrix (n_samples x n_params).\n\n    mu : ndarray\n        Mean estimation for the NB model (n_samples).\n\n    alpha : float\n        Initial dispersion estimate (1).\n\n    Returns\n    -------\n    grad : ndarray\n        Gradient of the negative log likelihood of the observations counts following\n        :math:`NB(\\\\mu, \\\\alpha)` (1).\n\n    M1 : ndarray\n        First summand for the gradient of the CR adjustment (n_params x n_params).\n\n    M2 : ndarray\n        Second summand for the gradient of the CR adjustment (n_params x n_params).\n    \"\"\"\n    grad = alpha * dnb_nll(counts, mu, alpha)\n    W = mu / (1 + mu * alpha)\n    dW = -(W**2)\n    M1 = (design.T * W) @ design\n    M2 = (design.T * dW) @ design\n\n    return grad, M1, M2\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.mle.vec_loss","title":"<code>vec_loss(counts, design, mu, alpha, cr_reg=True, prior_reg=False, alpha_hat=None, prior_disp_var=None)</code>","text":"<p>Compute the adjusted negative log likelihood of a batch of genes.</p> <p>Includes Cox-Reid regularization and (optionally) prior regularization.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Raw counts for a set of genes (n_samples x n_genes).</p> required <code>design</code> <code>ndarray</code> <p>Design matrix (n_samples x n_params).</p> required <code>mu</code> <code>ndarray</code> <p>Mean estimation for the NB model (n_samples x n_genes).</p> required <code>alpha</code> <code>ndarray</code> <p>Dispersion estimates (n_genes).</p> required <code>cr_reg</code> <code>bool</code> <p>Whether to include Cox-Reid regularization (default: True).</p> <code>True</code> <code>prior_reg</code> <code>bool</code> <p>Whether to include prior regularization (default: False).</p> <code>False</code> <code>alpha_hat</code> <code>ndarray</code> <p>Reference dispersions (for MAP estimation, n_genes).</p> <code>None</code> <code>prior_disp_var</code> <code>float</code> <p>Prior dispersion variance.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Adjusted negative log likelihood (n_genes).</p> Source code in <code>fedpydeseq2/core/utils/mle.py</code> <pre><code>def vec_loss(\n    counts: np.ndarray,\n    design: np.ndarray,\n    mu: np.ndarray,\n    alpha: np.ndarray,\n    cr_reg: bool = True,\n    prior_reg: bool = False,\n    alpha_hat: np.ndarray | None = None,\n    prior_disp_var: float | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Compute the adjusted negative log likelihood of a batch of genes.\n\n    Includes Cox-Reid regularization and (optionally) prior regularization.\n\n    Parameters\n    ----------\n    counts : ndarray\n        Raw counts for a set of genes (n_samples x n_genes).\n\n    design : ndarray\n        Design matrix (n_samples x n_params).\n\n    mu : ndarray\n        Mean estimation for the NB model (n_samples x n_genes).\n\n    alpha : ndarray\n        Dispersion estimates (n_genes).\n\n    cr_reg : bool\n        Whether to include Cox-Reid regularization (default: True).\n\n    prior_reg : bool\n        Whether to include prior regularization (default: False).\n\n    alpha_hat : ndarray, optional\n        Reference dispersions (for MAP estimation, n_genes).\n\n    prior_disp_var : float, optional\n        Prior dispersion variance.\n\n    Returns\n    -------\n    ndarray\n        Adjusted negative log likelihood (n_genes).\n    \"\"\"\n    # closure to be minimized\n    reg = 0\n    if cr_reg:\n        W = mu / (1 + mu * alpha)\n        reg += (\n            0.5\n            * np.linalg.slogdet((design.T[:, :, None] * W).transpose(2, 0, 1) @ design)[\n                1\n            ]\n        )\n    if prior_reg:\n        if prior_disp_var is None:\n            raise ValueError(\"Sigma_prior is required for prior regularization\")\n        reg += (np.log(alpha) - np.log(alpha_hat)) ** 2 / (2 * prior_disp_var)\n    return nb_nll(counts, mu, alpha) + reg\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.negative_binomial","title":"<code>negative_binomial</code>","text":"<p>Gradients and loss functions for the negative binomial distribution.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.negative_binomial.grid_nb_nll","title":"<code>grid_nb_nll(counts, mu, alpha_grid, mask_nan=None)</code>","text":"<p>Neg log-likelihood of a negative binomial, batched wrt genes on a grid.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Observations, n_samples x n_genes.</p> required <code>mu</code> <code>ndarray</code> <p>Mean estimation for the NB model (n_samples x n_genes).</p> required <code>alpha_grid</code> <code>ndarray</code> <p>Dispersions (n_genes x grid_length).</p> required <code>mask_nan</code> <code>ndarray</code> <p>Mask for the values of the grid where mu should have taken values &gt;&gt; 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Negative log likelihoods of size (n_genes x grid_length).</p> Source code in <code>fedpydeseq2/core/utils/negative_binomial.py</code> <pre><code>def grid_nb_nll(\n    counts: np.ndarray,\n    mu: np.ndarray,\n    alpha_grid: np.ndarray,\n    mask_nan: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    r\"\"\"Neg log-likelihood of a negative binomial, batched wrt genes on a grid.\n\n    Parameters\n    ----------\n    counts : ndarray\n        Observations, n_samples x n_genes.\n\n    mu : ndarray\n        Mean estimation for the NB model (n_samples x n_genes).\n\n    alpha_grid : ndarray\n        Dispersions (n_genes x grid_length).\n\n    mask_nan : ndarray\n        Mask for the values of the grid where mu should have taken values &gt;&gt; 1.\n\n    Returns\n    -------\n    ndarray\n        Negative log likelihoods of size (n_genes x grid_length).\n    \"\"\"\n    n = len(counts)\n    alpha_neg1 = 1 / alpha_grid\n    ndim_alpha = alpha_grid.ndim\n    extra_dims_counts = tuple(range(2, 2 + ndim_alpha - 1))\n    expanded_counts = np.expand_dims(counts, axis=extra_dims_counts)\n    # In order to avoid infinities, we replace all big values in the mu with 1 and\n    # modify the final quantity with their true value for the inputs were mu should have\n    # taken values &gt;&gt; 1\n    if mask_nan is not None:\n        mu[mask_nan] = 1.0\n    expanded_mu = np.expand_dims(mu, axis=extra_dims_counts)\n    logbinom = (\n        gammaln(expanded_counts + alpha_neg1[None, :])\n        - gammaln(expanded_counts + 1)\n        - gammaln(alpha_neg1[None, :])\n    )\n\n    nll = n * alpha_neg1 * np.log(alpha_grid) + (\n        -logbinom\n        + (expanded_counts + alpha_neg1) * np.log(alpha_neg1 + expanded_mu)\n        - expanded_counts * np.log(expanded_mu)\n    ).sum(0)\n    if mask_nan is not None:\n        nll[mask_nan.sum(0) &gt; 0] = np.nan\n    return nll\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.negative_binomial.mu_grid_nb_nll","title":"<code>mu_grid_nb_nll(counts, mu_grid, alpha)</code>","text":"<p>Compute the neg log-likelihood of a negative binomial.</p> <p>This function is batched wrt genes on a mu grid.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Observations, (n_obs, batch_size).</p> required <code>mu_grid</code> <code>ndarray</code> <p>Means of the distribution :math:<code>\\\\mu</code>, (n_mu, batch_size, n_obs).</p> required <code>alpha</code> <code>ndarray</code> <p>Dispersions of the distribution :math:<code>\\\\alpha</code>, s.t. the variance is :math:<code>\\\\mu + \\\\alpha \\\\mu^2</code>, of size (batch_size,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Negative log likelihoods of the observations counts following :math:<code>NB(\\\\mu, \\\\alpha)</code>, of size (n_mu, batch_size).</p> Notes <p>[1] https://en.wikipedia.org/wiki/Negative_binomial_distribution</p> Source code in <code>fedpydeseq2/core/utils/negative_binomial.py</code> <pre><code>def mu_grid_nb_nll(\n    counts: np.ndarray, mu_grid: np.ndarray, alpha: np.ndarray\n) -&gt; np.ndarray:\n    r\"\"\"Compute the neg log-likelihood of a negative binomial.\n\n    This function is *batched* wrt genes on a mu grid.\n\n    Parameters\n    ----------\n    counts : ndarray\n        Observations, (n_obs, batch_size).\n\n    mu_grid : ndarray\n        Means of the distribution :math:`\\\\mu`, (n_mu, batch_size, n_obs).\n\n    alpha : ndarray\n        Dispersions of the distribution :math:`\\\\alpha`,\n        s.t. the variance is :math:`\\\\mu + \\\\alpha \\\\mu^2`,\n        of size (batch_size,).\n\n    Returns\n    -------\n    ndarray\n        Negative log likelihoods of the observations counts\n        following :math:`NB(\\\\mu, \\\\alpha)`, of size (n_mu, batch_size).\n\n    Notes\n    -----\n    [1] https://en.wikipedia.org/wiki/Negative_binomial_distribution\n    \"\"\"\n    n = len(counts)\n    alpha_neg1 = 1 / alpha  # shape (batch_size,)\n    logbinom = np.expand_dims(\n        (\n            gammaln(counts.T + alpha_neg1[:, None])\n            - gammaln(counts.T + 1)\n            - gammaln(alpha_neg1[:, None])\n        ),\n        axis=0,\n    )  # Of size (1, batch_size, n_obs)\n    first_term = np.expand_dims(\n        n * alpha_neg1 * np.log(alpha), axis=0\n    )  # Of size (1, batch_size)\n    second_term = np.expand_dims(\n        counts.T + np.expand_dims(alpha_neg1, axis=1), axis=0\n    ) * np.log(\n        np.expand_dims(alpha_neg1, axis=(0, 2)) + mu_grid\n    )  # Of size (n_mu, batch_size, n_obs)\n    third_term = -np.expand_dims(counts.T, axis=0) * np.log(\n        mu_grid\n    )  # Of size (n_mu, batch_size, n_obs)\n    return first_term + (-logbinom + second_term + third_term).sum(axis=2)\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.negative_binomial.vec_nb_nll_grad","title":"<code>vec_nb_nll_grad(counts, mu, alpha)</code>","text":"<p>Return the gradient of the negative log-likelihood of a negative binomial.</p> <p>Vectorized version (wrt genes).</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Observations, n_samples x n_genes.</p> required <code>mu</code> <code>ndarray</code> <p>Mean of the distribution.</p> required <code>alpha</code> <code>Series</code> <p>Dispersion of the distribution, s.t. the variance is :math:<code>\\\\mu + \\\\alpha_grid * \\\\mu^2</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Gradient of the negative log likelihood of the observations counts following :math:<code>NB(\\\\mu, \\\\alpha_grid)</code>.</p> Source code in <code>fedpydeseq2/core/utils/negative_binomial.py</code> <pre><code>def vec_nb_nll_grad(\n    counts: np.ndarray, mu: np.ndarray, alpha: np.ndarray\n) -&gt; np.ndarray:\n    r\"\"\"Return the gradient of the negative log-likelihood of a negative binomial.\n\n    Vectorized version (wrt genes).\n\n    Parameters\n    ----------\n    counts : ndarray\n        Observations, n_samples x n_genes.\n\n    mu : ndarray\n        Mean of the distribution.\n\n    alpha : pd.Series\n        Dispersion of the distribution, s.t. the variance is\n        :math:`\\\\mu + \\\\alpha_grid * \\\\mu^2`.\n\n    Returns\n    -------\n    ndarray\n        Gradient of the negative log likelihood of the observations counts following\n        :math:`NB(\\\\mu, \\\\alpha_grid)`.\n    \"\"\"\n    alpha_neg1 = 1 / alpha\n    ll_part = alpha_neg1**2 * (\n        polygamma(0, alpha_neg1[None, :])\n        - polygamma(0, counts + alpha_neg1[None, :])\n        + np.log(1 + mu * alpha[None, :])\n        + (counts - mu) / (mu + alpha_neg1[None, :])\n    ).sum(0)\n\n    return -ll_part\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.pass_on_results","title":"<code>pass_on_results</code>","text":"<p>Module to implement the passing of the first shared state.</p>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.pass_on_results--todo-remove-after-all-savings-have-been-factored-out-if-not-needed-anymore","title":"TODO remove after all savings have been factored out, if not needed anymore.","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.pass_on_results.AggPassOnResults","title":"<code>AggPassOnResults</code>","text":"<p>Mixin to pass on the first shared state.</p> Source code in <code>fedpydeseq2/core/utils/pass_on_results.py</code> <pre><code>class AggPassOnResults:\n    \"\"\"Mixin to pass on the first shared state.\"\"\"\n\n    results: dict | None\n\n    @remote\n    @log_remote\n    def pass_on_results(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Pass on the shared state.\n\n        This method simply returns the first shared state.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of shared states.\n\n        Returns\n        -------\n        dict : The first shared state.\n        \"\"\"\n        results = shared_states[0]\n        # This is an ugly way to save the results for the simulation mode.\n        # In simulation mode, we will look at the results attribute of the class\n        # to get the results.\n        # In the real mode, we will download the last shared state.\n        self.results = results\n        return results\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.pass_on_results.AggPassOnResults.pass_on_results","title":"<code>pass_on_results(shared_states)</code>","text":"<p>Pass on the shared state.</p> <p>This method simply returns the first shared state.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of shared states.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>The first shared state.</code> Source code in <code>fedpydeseq2/core/utils/pass_on_results.py</code> <pre><code>@remote\n@log_remote\ndef pass_on_results(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Pass on the shared state.\n\n    This method simply returns the first shared state.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of shared states.\n\n    Returns\n    -------\n    dict : The first shared state.\n    \"\"\"\n    results = shared_states[0]\n    # This is an ugly way to save the results for the simulation mode.\n    # In simulation mode, we will look at the results attribute of the class\n    # to get the results.\n    # In the real mode, we will download the last shared state.\n    self.results = results\n    return results\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.pipe_steps","title":"<code>pipe_steps</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.pipe_steps.aggregation_step","title":"<code>aggregation_step(aggregation_method, train_data_nodes, aggregation_node, input_shared_states, round_idx, description='', clean_models=True, method_params=None)</code>","text":"<p>Perform an aggregation step of the federated learning strategy.</p> <p>Used as a wrapper to execute an aggregation method on the data of each organization.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_method</code> <code>Callable</code> <p>Method to be executed on the shared states.</p> required <code>train_data_nodes</code> <code>list</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <code>AggregationNode</code> <p>Aggregation node.</p> required <code>input_shared_states</code> <code>list</code> <p>List of shared states to be aggregated.</p> required <code>round_idx</code> <code>int</code> <p>Round index.</p> required <code>description</code> <code>str</code> <p>Description of the algorithm.</p> <code>''</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the computation.</p> <code>True</code> <code>method_params</code> <code>dict</code> <p>Optional keyword arguments to be passed to the aggregation method.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SharedStateRef</code> <p>A shared state containing the results of the aggregation.</p> <code>round_idx</code> <code>int</code> <p>Round index incremented by 1</p> Source code in <code>fedpydeseq2/core/utils/pipe_steps.py</code> <pre><code>def aggregation_step(\n    aggregation_method: Callable,\n    train_data_nodes: list[TrainDataNode],\n    aggregation_node: AggregationNode,\n    input_shared_states: list[SharedStateRef],\n    round_idx: int,\n    description: str = \"\",\n    clean_models: bool = True,\n    method_params: dict | None = None,\n) -&gt; tuple[SharedStateRef, int]:\n    \"\"\"Perform an aggregation step of the federated learning strategy.\n\n    Used as a wrapper to execute an aggregation method on the data of each organization.\n\n    Parameters\n    ----------\n    aggregation_method : Callable\n        Method to be executed on the shared states.\n    train_data_nodes : list\n        List of TrainDataNode.\n    aggregation_node : AggregationNode\n        Aggregation node.\n    input_shared_states : list\n        List of shared states to be aggregated.\n    round_idx : int\n        Round index.\n    description:  str\n        Description of the algorithm.\n    clean_models : bool\n        Whether to clean the models after the computation.\n    method_params : dict, optional\n        Optional keyword arguments to be passed to the aggregation method.\n\n    Returns\n    -------\n    SharedStateRef\n        A shared state containing the results of the aggregation.\n    round_idx : int\n        Round index incremented by 1\n    \"\"\"\n    method_params = method_params or {}\n    share_state = aggregation_node.update_states(\n        aggregation_method(\n            shared_states=input_shared_states,\n            _algo_name=description,\n            **method_params,\n        ),\n        round_idx=round_idx,\n        authorized_ids={\n            train_data_node.organization_id for train_data_node in train_data_nodes\n        },\n        clean_models=clean_models,\n    )\n    round_idx += 1\n    return share_state, round_idx\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.pipe_steps.local_step","title":"<code>local_step(local_method, train_data_nodes, output_local_states, round_idx, input_local_states=None, input_shared_state=None, aggregation_id=None, description='', clean_models=True, method_params=None)</code>","text":"<p>Local step of the federated learning strategy.</p> <p>Used as a wrapper to execute a local method on the data of each organization.</p> <p>Parameters:</p> Name Type Description Default <code>local_method</code> <code>Callable</code> <p>Method to be executed on the local data.</p> required <code>train_data_nodes</code> <code>TrainDataNode</code> <p>List of TrainDataNode.</p> required <code>output_local_states</code> <code>dict</code> <p>Dictionary of local states to be updated.</p> required <code>round_idx</code> <code>int</code> <p>Round index.</p> required <code>input_local_states</code> <code>dict</code> <p>Dictionary of local states to be used as input.</p> <code>None</code> <code>input_shared_state</code> <code>SharedStateRef</code> <p>Shared state to be used as input.</p> <code>None</code> <code>aggregation_id</code> <code>str</code> <p>Aggregation node id.</p> <code>None</code> <code>description</code> <code>str</code> <p>Description of the algorithm.</p> <code>''</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the computation.</p> <code>True</code> <code>method_params</code> <code>dict</code> <p>Optional keyword arguments to be passed to the local method.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output_local_states</code> <code>dict</code> <p>Local states containing the results of the local method, to keep within the training nodes.</p> <code>output_shared_states</code> <code>list</code> <p>Shared states containing the results of the local method,  to be sent to the aggregation node.</p> <code>round_idx</code> <code>int</code> <p>Round index incremented by 1</p> Source code in <code>fedpydeseq2/core/utils/pipe_steps.py</code> <pre><code>def local_step(\n    local_method: Callable,\n    train_data_nodes: list[TrainDataNode],\n    output_local_states: dict[str, LocalStateRef],\n    round_idx: int,\n    input_local_states: dict[str, LocalStateRef] | None = None,\n    input_shared_state: SharedStateRef | None = None,\n    aggregation_id: str | None = None,\n    description: str = \"\",\n    clean_models: bool = True,\n    method_params: dict | None = None,\n) -&gt; tuple[dict[str, LocalStateRef], list[SharedStateRef], int]:\n    \"\"\"Local step of the federated learning strategy.\n\n    Used as a wrapper to execute a local method on the data of each organization.\n\n    Parameters\n    ----------\n    local_method : Callable\n        Method to be executed on the local data.\n    train_data_nodes : TrainDataNode\n        List of TrainDataNode.\n    output_local_states : dict\n        Dictionary of local states to be updated.\n    round_idx : int\n        Round index.\n    input_local_states : dict, optional\n        Dictionary of local states to be used as input.\n    input_shared_state : SharedStateRef, optional\n        Shared state to be used as input.\n    aggregation_id : str, optional\n        Aggregation node id.\n    description : str\n        Description of the algorithm.\n    clean_models : bool\n        Whether to clean the models after the computation.\n    method_params : dict, optional\n        Optional keyword arguments to be passed to the local method.\n\n    Returns\n    -------\n    output_local_states : dict\n        Local states containing the results of the local method,\n        to keep within the training nodes.\n    output_shared_states : list\n        Shared states containing the results of the local method,\n         to be sent to the aggregation node.\n    round_idx : int\n        Round index incremented by 1\n    \"\"\"\n    output_shared_states = []\n    method_params = method_params or {}\n\n    for node in train_data_nodes:\n        next_local_state, next_shared_state = node.update_states(\n            local_method(\n                node.data_sample_keys,\n                shared_state=input_shared_state,\n                _algo_name=description,\n                **method_params,\n            ),\n            local_state=(\n                input_local_states[node.organization_id] if input_local_states else None\n            ),\n            round_idx=round_idx,\n            authorized_ids={node.organization_id},\n            aggregation_id=aggregation_id,\n            clean_models=clean_models,\n        )\n\n        output_local_states[node.organization_id] = next_local_state\n        output_shared_states.append(next_shared_state)\n\n    round_idx += 1\n    return output_local_states, output_shared_states, round_idx\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.stat_utils","title":"<code>stat_utils</code>","text":""},{"location":"api/core/utils/#fedpydeseq2.core.utils.stat_utils.build_contrast","title":"<code>build_contrast(design_factors, design_columns, continuous_factors=None, contrast=None)</code>","text":"<p>Check the validity of the contrast (if provided).</p> <p>If not, build a default contrast, corresponding to the last column of the design matrix. A contrast should be a list of three strings, in the following format: <code>['variable_of_interest', 'tested_level', 'reference_level']</code>. Names must correspond to the metadata data passed to the FedCenters. E.g., <code>['condition', 'B', 'A']</code> will measure the LFC of 'condition B' compared to 'condition A'. For continuous variables, the last two strings will be left empty, e.g. ``['measurement', '', '']. If None, the last variable from the design matrix is chosen as the variable of interest, and the reference level is picked alphabetically.</p> <p>Parameters:</p> Name Type Description Default <code>design_factors</code> <code>list</code> <p>The design factors.</p> required <code>design_columns</code> <code>list</code> <p>The names of the columns of the design matrices in the centers.</p> required <code>continuous_factors</code> <code>list</code> <p>The continuous factors in the design, if any. (default: <code>None</code>).</p> <code>None</code> <code>contrast</code> <code>list</code> <p>A list of three strings, in the following format: <code>['variable_of_interest', 'tested_level', 'reference_level']</code>. (default: <code>None</code>).</p> <code>None</code> Source code in <code>fedpydeseq2/core/utils/stat_utils.py</code> <pre><code>def build_contrast(\n    design_factors,\n    design_columns,\n    continuous_factors=None,\n    contrast: list[str] | None = None,\n) -&gt; list[str]:\n    \"\"\"Check the validity of the contrast (if provided).\n\n    If not, build a default\n    contrast, corresponding to the last column of the design matrix.\n    A contrast should be a list of three strings, in the following format:\n    ``['variable_of_interest', 'tested_level', 'reference_level']``.\n    Names must correspond to the metadata data passed to the FedCenters.\n    E.g., ``['condition', 'B', 'A']`` will measure the LFC of 'condition B'\n    compared to 'condition A'.\n    For continuous variables, the last two strings will be left empty, e.g.\n    ``['measurement', '', ''].\n    If None, the last variable from the design matrix\n    is chosen as the variable of interest, and the reference level is picked\n    alphabetically.\n\n    Parameters\n    ----------\n    design_factors : list\n        The design factors.\n    design_columns : list\n        The names of the columns of the design matrices in the centers.\n    continuous_factors : list, optional\n        The continuous factors in the design, if any. (default: ``None``).\n    contrast : list, optional\n        A list of three strings, in the following format:\n        ``['variable_of_interest', 'tested_level', 'reference_level']``.\n        (default: ``None``).\n    \"\"\"\n    if contrast is not None:  # Test contrast if provided\n        if len(contrast) != 3:\n            raise ValueError(\"The contrast should contain three strings.\")\n        if contrast[0] not in design_factors:\n            raise KeyError(\n                f\"The contrast variable ('{contrast[0]}') should be one \"\n                f\"of the design factors.\"\n            )\n        # TODO: Ideally, we should check that the levels are valid. This might leak\n        # data from the centers, though.\n\n    else:  # Build contrast if None\n        factor = design_factors[-1]\n        # Check whether this factor is categorical or continuous.\n        if continuous_factors is not None and factor in continuous_factors:\n            # The factor is continuous\n            contrast = [factor, \"\", \"\"]\n        else:\n            # The factor is categorical\n            factor_col = next(col for col in design_columns if col.startswith(factor))\n            split_col = factor_col.split(\"_\")\n            contrast = [split_col[0], split_col[1], split_col[-1]]\n\n    return contrast\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.stat_utils.build_contrast_vector","title":"<code>build_contrast_vector(contrast, LFC_columns)</code>","text":"<p>Build a vector corresponding to the desired contrast.</p> <p>Allows to test any pair of levels without refitting LFCs.</p> <p>Parameters:</p> Name Type Description Default <code>contrast</code> <code>list</code> <p>A list of three strings, in the following format: <code>['variable_of_interest', 'tested_level', 'reference_level']</code>.</p> required <code>LFC_columns</code> <code>list</code> <p>The names of the columns of the LFC matrices in the centers.</p> required <p>Returns:</p> Name Type Description <code>contrast_vector</code> <code>ndarray</code> <p>The contrast vector, containing multipliers to apply to the LFCs.</p> <code>contrast_idx</code> <code>(int, optional)</code> <p>The index of the tested contrast in the LFC matrix.</p> Source code in <code>fedpydeseq2/core/utils/stat_utils.py</code> <pre><code>def build_contrast_vector(contrast, LFC_columns) -&gt; tuple[np.ndarray, int | None]:\n    \"\"\"Build a vector corresponding to the desired contrast.\n\n    Allows to test any pair of levels without refitting LFCs.\n\n    Parameters\n    ----------\n    contrast : list\n        A list of three strings, in the following format:\n        ``['variable_of_interest', 'tested_level', 'reference_level']``.\n    LFC_columns : list\n        The names of the columns of the LFC matrices in the centers.\n\n    Returns\n    -------\n    contrast_vector : ndarray\n        The contrast vector, containing multipliers to apply to the LFCs.\n    contrast_idx : int, optional\n        The index of the tested contrast in the LFC matrix.\n    \"\"\"\n    factor = contrast[0]\n    alternative = contrast[1]\n    ref = contrast[2]\n    if ref == alternative == \"\":\n        # \"factor\" is a continuous variable\n        contrast_level = factor\n    else:\n        contrast_level = f\"{factor}_{alternative}_vs_{ref}\"\n\n    contrast_vector = np.zeros(len(LFC_columns))\n    if contrast_level in LFC_columns:\n        contrast_idx = LFC_columns.get_loc(contrast_level)\n        contrast_vector[contrast_idx] = 1\n    elif f\"{factor}_{ref}_vs_{alternative}\" in LFC_columns:\n        # Reference and alternative are inverted\n        contrast_idx = LFC_columns.get_loc(f\"{factor}_{ref}_vs_{alternative}\")\n        contrast_vector[contrast_idx] = -1\n    else:\n        # Need to change reference\n        # Get any column corresponding to the desired factor and extract old ref\n        old_ref = next(col for col in LFC_columns if col.startswith(factor)).split(\n            \"_vs_\"\n        )[-1]\n        new_alternative_idx = LFC_columns.get_loc(\n            f\"{factor}_{alternative}_vs_{old_ref}\"\n        )\n        new_ref_idx = LFC_columns.get_loc(f\"{factor}_{ref}_vs_{old_ref}\")\n        contrast_vector[new_alternative_idx] = 1\n        contrast_vector[new_ref_idx] = -1\n        # In that case there is no contrast index\n        contrast_idx = None\n\n    return contrast_vector, contrast_idx\n</code></pre>"},{"location":"api/core/utils/#fedpydeseq2.core.utils.stat_utils.wald_test","title":"<code>wald_test(M, lfc, ridge_factor, contrast_vector, lfc_null, alt_hypothesis)</code>","text":"<p>Run Wald test for a single gene.</p> <p>Computes Wald statistics, standard error and p-values from dispersion and LFC estimates.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>ndarray</code> <p>Central parameter in the covariance matrix estimator.</p> required <code>lfc</code> <code>ndarray</code> <p>Log-fold change estimate (in natural log scale).</p> required <code>ridge_factor</code> <code>ndarray</code> <p>Regularization factors.</p> required <code>contrast_vector</code> <code>ndarray</code> <p>Vector encoding the contrast that is being tested.</p> required <code>lfc_null</code> <code>float</code> <p>The log fold change (in natural log scale) under the null hypothesis.</p> required <code>alt_hypothesis</code> <code>str</code> <p>The alternative hypothesis for computing wald p-values.</p> required <p>Returns:</p> Name Type Description <code>wald_p_value</code> <code>float</code> <p>Estimated p-value.</p> <code>wald_statistic</code> <code>float</code> <p>Wald statistic.</p> <code>wald_se</code> <code>float</code> <p>Standard error of the Wald statistic.</p> Source code in <code>fedpydeseq2/core/utils/stat_utils.py</code> <pre><code>def wald_test(\n    M: np.ndarray,\n    lfc: np.ndarray,\n    ridge_factor: np.ndarray | None,\n    contrast_vector: np.ndarray,\n    lfc_null: float,\n    alt_hypothesis: Literal[\"greaterAbs\", \"lessAbs\", \"greater\", \"less\"] | None,\n) -&gt; tuple[float, float, float]:\n    \"\"\"Run Wald test for a single gene.\n\n    Computes Wald statistics, standard error and p-values from\n    dispersion and LFC estimates.\n\n    Parameters\n    ----------\n    M : ndarray\n        Central parameter in the covariance matrix estimator.\n\n    lfc : ndarray\n        Log-fold change estimate (in natural log scale).\n\n    ridge_factor : ndarray, optional\n        Regularization factors.\n\n    contrast_vector : ndarray\n        Vector encoding the contrast that is being tested.\n\n    lfc_null : float\n        The log fold change (in natural log scale) under the null hypothesis.\n\n    alt_hypothesis : str, optional\n        The alternative hypothesis for computing wald p-values.\n\n    Returns\n    -------\n    wald_p_value : float\n        Estimated p-value.\n\n    wald_statistic : float\n        Wald statistic.\n\n    wald_se : float\n        Standard error of the Wald statistic.\n    \"\"\"\n    # Build covariance matrix estimator\n\n    if ridge_factor is None:\n        ridge_factor = np.diag(np.repeat(1e-6, M.shape[0]))\n    H = np.linalg.inv(M + ridge_factor)\n    Hc = H @ contrast_vector\n    # Evaluate standard error and Wald statistic\n    wald_se: float = np.sqrt(Hc.T @ M @ Hc)\n\n    def greater(lfc_null):\n        stat = contrast_vector @ np.fmax((lfc - lfc_null) / wald_se, 0)\n        pval = norm.sf(stat)\n        return stat, pval\n\n    def less(lfc_null):\n        stat = contrast_vector @ np.fmin((lfc - lfc_null) / wald_se, 0)\n        pval = norm.sf(np.abs(stat))\n        return stat, pval\n\n    def greater_abs(lfc_null):\n        stat = contrast_vector @ (\n            np.sign(lfc) * np.fmax((np.abs(lfc) - lfc_null) / wald_se, 0)\n        )\n        pval = 2 * norm.sf(np.abs(stat))  # Only case where the test is two-tailed\n        return stat, pval\n\n    def less_abs(lfc_null):\n        stat_above, pval_above = greater(-abs(lfc_null))\n        stat_below, pval_below = less(abs(lfc_null))\n        return min(stat_above, stat_below, key=abs), max(pval_above, pval_below)\n\n    wald_statistic: float\n    wald_p_value: float\n    if alt_hypothesis:\n        wald_statistic, wald_p_value = {\n            \"greaterAbs\": greater_abs(lfc_null),\n            \"lessAbs\": less_abs(lfc_null),\n            \"greater\": greater(lfc_null),\n            \"less\": less(lfc_null),\n        }[alt_hypothesis]\n    else:\n        wald_statistic = contrast_vector @ (lfc - lfc_null) / wald_se\n        wald_p_value = 2 * norm.sf(np.abs(wald_statistic))\n\n    return wald_p_value, wald_statistic, wald_se\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_core/","title":"Core of the fed-pydeseq2 package: <code>deseq2_core</code>","text":""},{"location":"api/core/deseq2_core/deseq2_core/#workflow-graph","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>run_deseq_pipe</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/deseq2_core/#api","title":"API","text":"<p>The core of the fed-pydeseq2 package is encapsulated in the following Mixin class.</p>"},{"location":"api/core/deseq2_core/deseq2_core/#fedpydeseq2.core.deseq2_core.deseq2_full_pipe.DESeq2FullPipe","title":"<code>DESeq2FullPipe</code>","text":"<p>               Bases: <code>BuildDesignMatrix</code>, <code>ComputeSizeFactors</code>, <code>DESeq2LFCDispersions</code>, <code>ComputeCookDistances</code>, <code>ReplaceCooksOutliers</code>, <code>ReplaceRefittedValues</code>, <code>DESeq2Stats</code>, <code>SavePipelineResults</code></p> <p>A Mixin class to run the full DESeq2 pipeline.</p> <p>Methods:</p> Name Description <code>run_deseq_pipe</code> <p>The method to run the full DESeq2 pipeline.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_full_pipe.py</code> <pre><code>class DESeq2FullPipe(\n    BuildDesignMatrix,\n    ComputeSizeFactors,\n    DESeq2LFCDispersions,\n    ComputeCookDistances,\n    ReplaceCooksOutliers,\n    ReplaceRefittedValues,\n    DESeq2Stats,\n    SavePipelineResults,\n):\n    \"\"\"A Mixin class to run the full DESeq2 pipeline.\n\n    Methods\n    -------\n    run_deseq_pipe\n        The method to run the full DESeq2 pipeline.\n    \"\"\"\n\n    @log_organisation_method\n    def run_deseq_pipe(\n        self,\n        train_data_nodes: list[TrainDataNode],\n        aggregation_node: AggregationNode,\n        local_states: dict[str, LocalStateRef],\n        round_idx: int = 0,\n        clean_models: bool = True,\n        clean_last_model: bool = False,\n    ):\n        \"\"\"Run the DESeq2 pipeline.\n\n        Parameters\n        ----------\n        train_data_nodes : list[TrainDataNode]\n            List of the train nodes.\n        aggregation_node : AggregationNode\n            Aggregation node.\n        local_states : dict[str, LocalStateRef]\n            Local states.\n        round_idx : int\n            Round index.\n        clean_models : bool\n            Whether to clean the models after the computation. (default: ``True``).\n            Note that as intermediate steps are very memory consuming, it is recommended\n            to clean the models after each step.\n        clean_last_model : bool\n            Whether to clean the last model. (default: ``False``).\n        \"\"\"\n        #### Build design matrices ####\n\n        logger.info(\"Building design matrices...\")\n\n        local_states, log_mean_shared_states, round_idx = self.build_design_matrix(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished building design matrices.\")\n\n        #### Compute size factors ####\n        # Note: in refit mode, this doesn't recompute size factors,\n        # just the log features\n\n        logger.info(\"Computing size factors...\")\n\n        (\n            local_states,\n            gram_features_shared_states,\n            round_idx,\n        ) = self.compute_size_factors(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            shared_states=log_mean_shared_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished computing size factors.\")\n\n        #### Compute LFC and dispersions ####\n\n        logger.info(\"Running LFC and dispersions.\")\n\n        local_states, round_idx = self.run_deseq2_lfc_dispersions(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            gram_features_shared_states=gram_features_shared_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished running LFC and dispersions.\")\n\n        logger.info(\"Computing Cook distances...\")\n\n        (\n            local_states,\n            cooks_shared_state,\n            round_idx,\n        ) = self.compute_cook_distance(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished computing Cook distances.\")\n\n        #### Refit cooks if necessary ####\n        if self.refit_cooks:\n            logger.info(\"Refitting Cook outliers...\")\n            (\n                local_states,\n                gram_features_shared_states,\n                round_idx,\n            ) = self.replace_outliers(\n                train_data_nodes,\n                aggregation_node,\n                local_states,\n                cooks_shared_state,\n                round_idx,\n                clean_models=clean_models,\n            )\n\n            local_states, round_idx = self.run_deseq2_lfc_dispersions(\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                local_states=local_states,\n                gram_features_shared_states=gram_features_shared_states,\n                round_idx=round_idx,\n                clean_models=clean_models,\n                refit_mode=True,\n            )\n            # Replace values in the main ``local_adata`` object\n            local_states, round_idx = self.replace_refitted_values(\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                local_states=local_states,\n                round_idx=round_idx,\n                clean_models=clean_models,\n            )\n\n            logger.info(\"Finished refitting Cook outliers.\")\n\n        #### Compute DESeq2 statistics ####\n\n        logger.info(\"Running DESeq2 statistics.\")\n\n        local_states, round_idx = self.run_deseq2_stats(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished running DESeq2 statistics.\")\n\n        # Build the results that will be downloaded at the end of the pipeline.\n\n        logger.info(\"Saving pipeline results.\")\n        self.save_pipeline_results(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished saving pipeline results.\")\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_core/#fedpydeseq2.core.deseq2_core.deseq2_full_pipe.DESeq2FullPipe.run_deseq_pipe","title":"<code>run_deseq_pipe(train_data_nodes, aggregation_node, local_states, round_idx=0, clean_models=True, clean_last_model=False)</code>","text":"<p>Run the DESeq2 pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <code>list[TrainDataNode]</code> <p>List of the train nodes.</p> required <code>aggregation_node</code> <code>AggregationNode</code> <p>Aggregation node.</p> required <code>local_states</code> <code>dict[str, LocalStateRef]</code> <p>Local states.</p> required <code>round_idx</code> <code>int</code> <p>Round index.</p> <code>0</code> <code>clean_models</code> <code>bool</code> <p>Whether to clean the models after the computation. (default: <code>True</code>). Note that as intermediate steps are very memory consuming, it is recommended to clean the models after each step.</p> <code>True</code> <code>clean_last_model</code> <code>bool</code> <p>Whether to clean the last model. (default: <code>False</code>).</p> <code>False</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_full_pipe.py</code> <pre><code>@log_organisation_method\ndef run_deseq_pipe(\n    self,\n    train_data_nodes: list[TrainDataNode],\n    aggregation_node: AggregationNode,\n    local_states: dict[str, LocalStateRef],\n    round_idx: int = 0,\n    clean_models: bool = True,\n    clean_last_model: bool = False,\n):\n    \"\"\"Run the DESeq2 pipeline.\n\n    Parameters\n    ----------\n    train_data_nodes : list[TrainDataNode]\n        List of the train nodes.\n    aggregation_node : AggregationNode\n        Aggregation node.\n    local_states : dict[str, LocalStateRef]\n        Local states.\n    round_idx : int\n        Round index.\n    clean_models : bool\n        Whether to clean the models after the computation. (default: ``True``).\n        Note that as intermediate steps are very memory consuming, it is recommended\n        to clean the models after each step.\n    clean_last_model : bool\n        Whether to clean the last model. (default: ``False``).\n    \"\"\"\n    #### Build design matrices ####\n\n    logger.info(\"Building design matrices...\")\n\n    local_states, log_mean_shared_states, round_idx = self.build_design_matrix(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished building design matrices.\")\n\n    #### Compute size factors ####\n    # Note: in refit mode, this doesn't recompute size factors,\n    # just the log features\n\n    logger.info(\"Computing size factors...\")\n\n    (\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n    ) = self.compute_size_factors(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_states=log_mean_shared_states,\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished computing size factors.\")\n\n    #### Compute LFC and dispersions ####\n\n    logger.info(\"Running LFC and dispersions.\")\n\n    local_states, round_idx = self.run_deseq2_lfc_dispersions(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        local_states=local_states,\n        gram_features_shared_states=gram_features_shared_states,\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished running LFC and dispersions.\")\n\n    logger.info(\"Computing Cook distances...\")\n\n    (\n        local_states,\n        cooks_shared_state,\n        round_idx,\n    ) = self.compute_cook_distance(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished computing Cook distances.\")\n\n    #### Refit cooks if necessary ####\n    if self.refit_cooks:\n        logger.info(\"Refitting Cook outliers...\")\n        (\n            local_states,\n            gram_features_shared_states,\n            round_idx,\n        ) = self.replace_outliers(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            cooks_shared_state,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, round_idx = self.run_deseq2_lfc_dispersions(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            gram_features_shared_states=gram_features_shared_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=True,\n        )\n        # Replace values in the main ``local_adata`` object\n        local_states, round_idx = self.replace_refitted_values(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished refitting Cook outliers.\")\n\n    #### Compute DESeq2 statistics ####\n\n    logger.info(\"Running DESeq2 statistics.\")\n\n    local_states, round_idx = self.run_deseq2_stats(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished running DESeq2 statistics.\")\n\n    # Build the results that will be downloaded at the end of the pipeline.\n\n    logger.info(\"Saving pipeline results.\")\n    self.save_pipeline_results(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished saving pipeline results.\")\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_core/#steps-of-fedpydeseq2-and-corresponding-mixin-classes","title":"Steps of fedpydeseq2 and corresponding Mixin classes.","text":""},{"location":"api/core/deseq2_core/deseq2_core/#building-the-design-matrix-build_design_matrix","title":"Building the design matrix: build_design_matrix","text":""},{"location":"api/core/deseq2_core/deseq2_core/#computing-the-size-factors-compute_size_factors","title":"Computing the size factors: compute_size_factors","text":""},{"location":"api/core/deseq2_core/deseq2_core/#computing-log-fold-changes-and-dispersions-deseq2_lfc_dispersions","title":"Computing log fold changes and dispersions: deseq2_lfc_dispersions","text":""},{"location":"api/core/deseq2_core/deseq2_core/#computing-the-cooks-distance-compute_cook_distance","title":"Computing the cooks distance: compute_cook_distance","text":""},{"location":"api/core/deseq2_core/deseq2_core/#replacing-outliers-replace_outliers","title":"Replacing outliers: replace_outliers","text":""},{"location":"api/core/deseq2_core/deseq2_core/#replace-refitted-values-replace_refitted_values","title":"Replace refitted values: replace_refitted_values","text":""},{"location":"api/core/deseq2_core/deseq2_core/#computing-statistics-deseq2_stats","title":"Computing statistics: deseq2_stats","text":""},{"location":"api/core/deseq2_core/deseq2_core/#saving-the-results-save_pipeline_results","title":"Saving the results: save_pipeline_results","text":""},{"location":"api/core/deseq2_core/deseq2_core/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 local_levels dict A dictionary whose keys are the names of the categorical factors, and whose values are the list of values taken by this factor in a given center. For example \\(\\texttt{\\{stage: [Advanced, Non-advanced], gender: [female]\\}}\\). Each center Server 2 merged_levels dict A dictionary whose keys are the names of the categorical factors and whose values are arrays containing the list of values taken by this factor across all centers. For example \\(\\texttt{\\{stage: [Advanced , Non-advanced], gender: [female, male]\\}}\\). Server Center 3 design_columns Index \\((p,)\\) The name of the columns of the local design matrix, before aggregation. They are of the form intercept, factor for continuous factors, and factor_level_vs_factor_ref_level otherwise. For example, \\(\\texttt{[intercept, stage\\_Advanced\\_vs\\_Non-advanced, gender\\_male\\_vs\\_female]}\\). Each center Server 4 merged_columns Index \\((p,)\\) The union of the design columns across all centers. Local design matrices will then be updated to include all columns. Server Center 4 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server Center 5 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 5 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 6 global_log_mean nparray \\((G,)\\) The mean of the log of the counts across all samples in all centers \\(\\overline{\\log(Y)}_g =\\sum_{k=1}^{K}\\tfrac{n_k}{n}\\overline{\\log(Y)}^{(k)}_{g}\\). Server Center 7 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server 7 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 8 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 8 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 8 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 9 disp_function_type str The type of the dispersion function used to model the dispersions. It can be either \"parametric\", if the iterative scheme to fit the trend curve has converged, the LBFGS-B method used to fit the parameters has converged, and the coefficienst of the trend curve are non-negative; or \"mean\" otherwise. Server Center 9 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server Center 9 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server Center 9 trend_coeffs nparray \\((2,)\\) The coefficients of the trend curve fitted to the dispersions. We model the dispersions \\(\\alpha_g\\) of the gene \\(g\\) as a sample from an exponential distribution whose mean \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is a function of \\(\\overline{Z}_g\\) parametrized by two coefficients \\(\\alpha_0\\) and \\(\\text{a}_1\\); \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g) = \\alpha_0 + \\tfrac{\\text{a}_1}{\\overline{Z}_g}\\). \\(\\alpha_{\\texttt{trend}}\\) is called the trend curve. The coefficients are \\((\\alpha_0, \\text{a}_1)\\), and are obtained by starting with the set of non zero genes  and by iteratively i) minimizing the negative log likelihood of the exponential distribution on the set of genes with LBFGS-B and ii) removing the genes where the ratio between the dispersion \\(\\alpha_g\\) and the trend curve \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is above \\(15\\) or below \\(10^{-4}\\) (we consider these genes as outliers to this model), until the set of genes is stable (see \\citep{love2014deseq2} for more details). Server Center 9 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server Center 9 mean_disp NoneType None if the dispersion function type is \"parametric\", and the trimmed mean of the genewise dispersions whose value is above \\(10\\times \\texttt{min\\_disp}\\), with trimming proportion \\(0.001\\) otherwise. Server Center 12 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center 14 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). If use_lvl is \\(\\texttt{True}\\), for each gene \\(g\\), it is the maximum across all admissible levels of the trimmed mean of the squared error for the gene and level in question, with trim ratio \\(r_{\\texttt{trim}}\\), scaled (multiplied) by the scale factor \\(1.51\\). If use_level is \\(\\texttt{False}\\), the trimmed mean of the squared error scaled by \\(1.51\\) with trim ratio \\(r_{\\texttt{trim}}\\). Server Center 15 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 15 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 15 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 15 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is passed on from the previous shared state. Each center Server 15 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 16 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 16 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 17 replaceable_samples bool \\(()\\) A boolean indicating if there are any replaceable samples in the center. A sample \\(i\\) of center \\(k\\) is said to be replaceable if there are at least \\(\\texttt{min\\_replicates}\\) samples across all centers which share the same design factor levels as \\(i\\). \\(\\texttt{min\\_replicates}\\) is a user defined parameter, set to \\(7\\) by default. Each center Server 17 local_genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in the center. For a given gene \\(g\\) and given sample \\(i\\), the Cook's distance is computed as \\(\\tfrac{h^{(k)}_{ig}}{p (1-h^{(k)}_{ig})^2}~(R^2)^{(k)}_{ig}\\) where \\((R^2)^{(k)}_{ig}\\) is the squared Pearson residual of the negative binomial GLM with log fold changes \\(\\beta_g\\) and dispersions \\(\\alpha^{\\texttt{cooks}}_g\\), computed as \\((Y^{(k)}_{ig} - \\mu^{(k)}_{ig})^2/(V^{\\texttt{NB}})^{(k)}_{ig}\\), and \\(h^{(k)}_{ig}\\) is the \\(i\\)-th diagonal element of \\(X^{(k)} H^{-1}_{g} (X^{(k)})^{\\top}\\), where \\(H^{-1}_{g}\\) is the inverse of the global hat matrix. The cutoff value is set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Here, \\(\\mu^{(k)}_{ig} = \\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) and \\((V^{\\texttt{NB}})^{(k)}_{ig} = \\mu^{(k)}_{ig} (1 + \\mu^{(k)}_{ig} \\alpha^{\\texttt{cooks}}_g)\\). Each center Server 18 genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server Center 20 trimmed_mean_normed_counts nparray \\((G_{\\texttt{r}},)\\) For each gene to replace \\(g\\), the trimmed mean of the normed counts across all samples, denoted with \\(\\overline{Z}^{\\texttt{trim}}_{g}\\), with trim ratio set to \\(0.2\\). Server Center 21 loc_new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if the new count matrix of the center is all zeroes (across samples) for this gene (the new count matrix is computed by imputing the Cook's outliers with the trimmed mean of the normed counts times the size factor). Each center Server 22 new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if all counts across all centers are zero for this gene. Server Center 23 local_features nparray \\((p, G_{\\texttt{act}})\\) \\(\\Phi^{(k)}  := (X^{(k)})^{\\top} Z^{(k)}\\), where \\(Z^{(k)}\\) is the normalized counts in the center \\(k\\) on the set of genes to replace, where the value of Cook's outliers have been replaced using a trimmed mean. Each center Server 24 genewise_dispersions nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 24 lower_log_bounds nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 24 upper_log_bounds nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 29 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 29 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 29 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Server Center 29 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. This statistics depends on the \\(\\texttt{lfc\\_null}\\) parameter which sets the null hypothesis on the log fold change (set to \\(0\\) by default), and the \\(\\texttt{alt\\_hypothesis}\\) parameter, which defines the alternative hypothesis on the log fold change (set to \\(\\texttt{None}\\) by default, can be \\(\\texttt{greater}, ~\\texttt{greaterAbs}, ~\\texttt{less},~ \\texttt{lessAbs}\\)). If the alternative hypothesis is \\(\\texttt{None}\\), then the Wald statistic is computed as the centered normalized log fold change. Server Center 29 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server Center 30 p_values nparray \\((G,)\\) If Cook's filtering is enabled (which is the case by default with the $\\texttt{cooks_filter} $ parameter), then the p-values of genes which have \\(\\leq 2\\) samples above the gene count of the sample maximizing the Cook's distance across all centers are set to \\(\\texttt{nan}\\). Otherwise, passed on without modification. Server Center 30 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Passed on without modification. Server Center 30 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Passed on without modification. Server Center 32 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Each center Server 32 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Each center Server 32 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Each center Server 32 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Each center Server 32 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Each center Server 32 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Each center Server 32 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Each center Server 32 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Each center Server 32 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 32 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Each center Server 32 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 32 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 32 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Each center Server 32 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 32 gene_names Index \\((G,)\\) The gene names. Each center Server 32 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Each center Server 33 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server All 33 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Server All 33 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server All 33 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server All 33 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Server All 33 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Server All 33 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Server All 33 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 33 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server All 33 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server All 33 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Server All 33 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 33 gene_names Index \\((G,)\\) The gene names. Server All 33 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server All 33 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Server All 33 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server All"},{"location":"api/core/deseq2_core/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 local_levels dict A dictionary whose keys are the names of the categorical factors, and whose values are the list of values taken by this factor in a given center. For example \\(\\texttt{\\{stage: [Advanced, Non-advanced], gender: [female]\\}}\\). Each center Server 2 merged_levels dict A dictionary whose keys are the names of the categorical factors and whose values are arrays containing the list of values taken by this factor across all centers. For example \\(\\texttt{\\{stage: [Advanced , Non-advanced], gender: [female, male]\\}}\\). Server Center 3 design_columns Index \\((p,)\\) The name of the columns of the local design matrix, before aggregation. They are of the form intercept, factor for continuous factors, and factor_level_vs_factor_ref_level otherwise. For example, \\(\\texttt{[intercept, stage\\_Advanced\\_vs\\_Non-advanced, gender\\_male\\_vs\\_female]}\\). Each center Server 4 merged_columns Index \\((p,)\\) The union of the design columns across all centers. Local design matrices will then be updated to include all columns. Server Center 4 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server Center 5 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 5 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 6 global_log_mean nparray \\((G,)\\) The mean of the log of the counts across all samples in all centers \\(\\overline{\\log(Y)}_g =\\sum_{k=1}^{K}\\tfrac{n_k}{n}\\overline{\\log(Y)}^{(k)}_{g}\\). Server Center 7 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server 7 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 8 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 8 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 8 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 9 disp_function_type str The type of the dispersion function used to model the dispersions. It can be either \"parametric\", if the iterative scheme to fit the trend curve has converged, the LBFGS-B method used to fit the parameters has converged, and the coefficienst of the trend curve are non-negative; or \"mean\" otherwise. Server Center 9 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server Center 9 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server Center 9 trend_coeffs nparray \\((2,)\\) The coefficients of the trend curve fitted to the dispersions. We model the dispersions \\(\\alpha_g\\) of the gene \\(g\\) as a sample from an exponential distribution whose mean \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is a function of \\(\\overline{Z}_g\\) parametrized by two coefficients \\(\\alpha_0\\) and \\(\\text{a}_1\\); \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g) = \\alpha_0 + \\tfrac{\\text{a}_1}{\\overline{Z}_g}\\). \\(\\alpha_{\\texttt{trend}}\\) is called the trend curve. The coefficients are \\((\\alpha_0, \\text{a}_1)\\), and are obtained by starting with the set of non zero genes  and by iteratively i) minimizing the negative log likelihood of the exponential distribution on the set of genes with LBFGS-B and ii) removing the genes where the ratio between the dispersion \\(\\alpha_g\\) and the trend curve \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is above \\(15\\) or below \\(10^{-4}\\) (we consider these genes as outliers to this model), until the set of genes is stable (see \\citep{love2014deseq2} for more details). Server Center 9 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server Center 9 mean_disp NoneType None if the dispersion function type is \"parametric\", and the trimmed mean of the genewise dispersions whose value is above \\(10\\times \\texttt{min\\_disp}\\), with trimming proportion \\(0.001\\) otherwise. Server Center 12 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center 14 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). If use_lvl is \\(\\texttt{True}\\), for each gene \\(g\\), it is the maximum across all admissible levels of the trimmed mean of the squared error for the gene and level in question, with trim ratio \\(r_{\\texttt{trim}}\\), scaled (multiplied) by the scale factor \\(1.51\\). If use_level is \\(\\texttt{False}\\), the trimmed mean of the squared error scaled by \\(1.51\\) with trim ratio \\(r_{\\texttt{trim}}\\). Server Center 15 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 15 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 15 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 15 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is passed on from the previous shared state. Each center Server 15 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 16 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 16 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 17 replaceable_samples bool \\(()\\) A boolean indicating if there are any replaceable samples in the center. A sample \\(i\\) of center \\(k\\) is said to be replaceable if there are at least \\(\\texttt{min\\_replicates}\\) samples across all centers which share the same design factor levels as \\(i\\). \\(\\texttt{min\\_replicates}\\) is a user defined parameter, set to \\(7\\) by default. Each center Server 17 local_genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in the center. For a given gene \\(g\\) and given sample \\(i\\), the Cook's distance is computed as \\(\\tfrac{h^{(k)}_{ig}}{p (1-h^{(k)}_{ig})^2}~(R^2)^{(k)}_{ig}\\) where \\((R^2)^{(k)}_{ig}\\) is the squared Pearson residual of the negative binomial GLM with log fold changes \\(\\beta_g\\) and dispersions \\(\\alpha^{\\texttt{cooks}}_g\\), computed as \\((Y^{(k)}_{ig} - \\mu^{(k)}_{ig})^2/(V^{\\texttt{NB}})^{(k)}_{ig}\\), and \\(h^{(k)}_{ig}\\) is the \\(i\\)-th diagonal element of \\(X^{(k)} H^{-1}_{g} (X^{(k)})^{\\top}\\), where \\(H^{-1}_{g}\\) is the inverse of the global hat matrix. The cutoff value is set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Here, \\(\\mu^{(k)}_{ig} = \\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) and \\((V^{\\texttt{NB}})^{(k)}_{ig} = \\mu^{(k)}_{ig} (1 + \\mu^{(k)}_{ig} \\alpha^{\\texttt{cooks}}_g)\\). Each center Server 18 genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server Center 20 trimmed_mean_normed_counts nparray \\((G_{\\texttt{r}},)\\) For each gene to replace \\(g\\), the trimmed mean of the normed counts across all samples, denoted with \\(\\overline{Z}^{\\texttt{trim}}_{g}\\), with trim ratio set to \\(0.2\\). Server Center 21 loc_new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if the new count matrix of the center is all zeroes (across samples) for this gene (the new count matrix is computed by imputing the Cook's outliers with the trimmed mean of the normed counts times the size factor). Each center Server 22 new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if all counts across all centers are zero for this gene. Server Center 23 local_features nparray \\((p, G_{\\texttt{act}})\\) \\(\\Phi^{(k)}  := (X^{(k)})^{\\top} Z^{(k)}\\), where \\(Z^{(k)}\\) is the normalized counts in the center \\(k\\) on the set of genes to replace, where the value of Cook's outliers have been replaced using a trimmed mean. Each center Server 24 genewise_dispersions nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 24 lower_log_bounds nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 24 upper_log_bounds nparray \\((G_{\\texttt{r}},)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 29 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 29 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 29 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Server Center 29 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. This statistics depends on the \\(\\texttt{lfc\\_null}\\) parameter which sets the null hypothesis on the log fold change (set to \\(0\\) by default), and the \\(\\texttt{alt\\_hypothesis}\\) parameter, which defines the alternative hypothesis on the log fold change (set to \\(\\texttt{None}\\) by default, can be \\(\\texttt{greater}, ~\\texttt{greaterAbs}, ~\\texttt{less},~ \\texttt{lessAbs}\\)). If the alternative hypothesis is \\(\\texttt{None}\\), then the Wald statistic is computed as the centered normalized log fold change. Server Center 29 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server Center 30 p_values nparray \\((G,)\\) If Cook's filtering is enabled (which is the case by default with the $\\texttt{cooks_filter} $ parameter), then the p-values of genes which have \\(\\leq 2\\) samples above the gene count of the sample maximizing the Cook's distance across all centers are set to \\(\\texttt{nan}\\). Otherwise, passed on without modification. Server Center 30 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Passed on without modification. Server Center 30 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Passed on without modification. Server Center 32 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Each center Server 32 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Each center Server 32 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Each center Server 32 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Each center Server 32 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Each center Server 32 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Each center Server 32 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Each center Server 32 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Each center Server 32 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 32 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Each center Server 32 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 32 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 32 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Each center Server 32 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 32 gene_names Index \\((G,)\\) The gene names. Each center Server 32 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Each center Server 33 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server All 33 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Server All 33 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server All 33 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server All 33 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Server All 33 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Server All 33 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Server All 33 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 33 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server All 33 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server All 33 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Server All 33 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 33 gene_names Index \\((G,)\\) The gene names. Server All 33 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server All 33 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Server All 33 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server All"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>build_design_matrix</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#api","title":"API","text":"<p>Module to regroup the steps to build the design matrix for DESeq2.</p>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.build_design_matrix","title":"<code>build_design_matrix</code>","text":""},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.build_design_matrix.BuildDesignMatrix","title":"<code>BuildDesignMatrix</code>","text":"<p>               Bases: <code>AggMergeDesignColumnsBuildContrast</code>, <code>AggMergeDesignLevels</code>, <code>LocGetLocalFactors</code>, <code>LocSetLocalDesign</code>, <code>LocOderDesignComputeLogMean</code></p> <p>Mixin class to implement the computation of the design matrix.</p> <p>Methods:</p> Name Description <code>build_design_matrix</code> <p>The method to build the design matrix, that must be used in the main pipeline.</p> <code>check_design_matrix</code> <p>The method to check the design matrix, that must be used in the main pipeline while we are testing.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/build_design_matrix.py</code> <pre><code>class BuildDesignMatrix(\n    AggMergeDesignColumnsBuildContrast,\n    AggMergeDesignLevels,\n    LocGetLocalFactors,\n    LocSetLocalDesign,\n    LocOderDesignComputeLogMean,\n):\n    \"\"\"Mixin class to implement the computation of the design matrix.\n\n    Methods\n    -------\n    build_design_matrix\n        The method to build the design matrix, that must be used in the main\n        pipeline.\n\n    check_design_matrix\n        The method to check the design matrix, that must be used in the main\n        pipeline while we are testing.\n    \"\"\"\n\n    @log_organisation_method\n    def build_design_matrix(\n        self, train_data_nodes, aggregation_node, local_states, round_idx, clean_models\n    ):\n        \"\"\"Build the design matrix.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx:\n            The current round\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_states: dict\n            Shared states containing the necessary local information to start\n            the next step of the pipeline, which is computing the size factors.\n            They contain a \"log_means\" key and a \"n_samples\" key.\n\n        round_idx: int\n            The updated round\n        \"\"\"\n        # ---- For each design factor, get the list of each center's levels ---- #\n        if len(local_states) == 0:\n            # In that case, there is no reference dds, and this is the first step of\n            # The pipeline\n            input_local_states = None\n        else:\n            # In this case, there was already a step before, and we need to propagate\n            # the local states\n            input_local_states = local_states\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.get_local_factors,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=input_local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Computing local design factor levels\",\n            clean_models=clean_models,\n        )\n\n        # ---- For each design factor, merge the list of unique levels ---- #\n\n        design_levels_aggregated_state, round_idx = aggregation_step(\n            aggregation_method=self.merge_design_levels,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Merging design levels\",\n            clean_models=clean_models,\n        )\n\n        # ---- Initialize design matrices in each center ---- #\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.set_local_design,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=design_levels_aggregated_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Setting local design matrices\",\n            clean_models=clean_models,\n        )\n\n        # ---- Merge design columns ---- #\n\n        design_columns_aggregated_state, round_idx = aggregation_step(\n            aggregation_method=self.merge_design_columns_and_build_contrast,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Merge local design matrix columns\",\n            clean_models=clean_models,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.order_design_cols_compute_local_log_mean,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            round_idx=round_idx,\n            input_shared_state=design_columns_aggregated_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Computing local log means\",\n            clean_models=clean_models,\n        )\n\n        return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.build_design_matrix.BuildDesignMatrix.build_design_matrix","title":"<code>build_design_matrix(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Build the design matrix.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>The current round</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>shared_states</code> <code>dict</code> <p>Shared states containing the necessary local information to start the next step of the pipeline, which is computing the size factors. They contain a \"log_means\" key and a \"n_samples\" key.</p> <code>round_idx</code> <code>int</code> <p>The updated round</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/build_design_matrix.py</code> <pre><code>@log_organisation_method\ndef build_design_matrix(\n    self, train_data_nodes, aggregation_node, local_states, round_idx, clean_models\n):\n    \"\"\"Build the design matrix.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx:\n        The current round\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_states: dict\n        Shared states containing the necessary local information to start\n        the next step of the pipeline, which is computing the size factors.\n        They contain a \"log_means\" key and a \"n_samples\" key.\n\n    round_idx: int\n        The updated round\n    \"\"\"\n    # ---- For each design factor, get the list of each center's levels ---- #\n    if len(local_states) == 0:\n        # In that case, there is no reference dds, and this is the first step of\n        # The pipeline\n        input_local_states = None\n    else:\n        # In this case, there was already a step before, and we need to propagate\n        # the local states\n        input_local_states = local_states\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.get_local_factors,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=input_local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Computing local design factor levels\",\n        clean_models=clean_models,\n    )\n\n    # ---- For each design factor, merge the list of unique levels ---- #\n\n    design_levels_aggregated_state, round_idx = aggregation_step(\n        aggregation_method=self.merge_design_levels,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Merging design levels\",\n        clean_models=clean_models,\n    )\n\n    # ---- Initialize design matrices in each center ---- #\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.set_local_design,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=design_levels_aggregated_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Setting local design matrices\",\n        clean_models=clean_models,\n    )\n\n    # ---- Merge design columns ---- #\n\n    design_columns_aggregated_state, round_idx = aggregation_step(\n        aggregation_method=self.merge_design_columns_and_build_contrast,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Merge local design matrix columns\",\n        clean_models=clean_models,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.order_design_cols_compute_local_log_mean,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        round_idx=round_idx,\n        input_shared_state=design_columns_aggregated_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Computing local log means\",\n        clean_models=clean_models,\n    )\n\n    return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps","title":"<code>substeps</code>","text":"<p>Module containing the substeps for the computation of design matrices.</p> <p>This module contains all these substeps as mixin classes.</p>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.AggMergeDesignColumnsBuildContrast","title":"<code>AggMergeDesignColumnsBuildContrast</code>","text":"<p>Mixin to merge the columns of the design matrices and build contrast.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>class AggMergeDesignColumnsBuildContrast:\n    \"\"\"Mixin to merge the columns of the design matrices and build contrast.\"\"\"\n\n    design_factors: list[str]\n    continuous_factors: list[str] | None\n    contrast: list[str] | None\n\n    @remote\n    @log_remote\n    def merge_design_columns_and_build_contrast(self, shared_states):\n        \"\"\"Merge the columns of the design matrices and build constrasts.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (dictionaries of design columns) from training nodes.\n\n        Returns\n        -------\n        dict\n            Shared state containing:\n            - merged_columns: the names of the columns that the local design matrices\n              should have.\n            - contrast: the contrast (in a list of strings form) to be used for the\n              DESeq2 model.\n        \"\"\"\n        merged_columns = pd.Index([])\n\n        for state in shared_states:\n            merged_columns = merged_columns.union(state[\"design_columns\"])\n\n        # We now also have everything to compute the contrasts\n        contrast = build_contrast(\n            self.design_factors,\n            merged_columns,\n            self.continuous_factors,\n            self.contrast,\n        )\n\n        return {\"merged_columns\": merged_columns, \"contrast\": contrast}\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.AggMergeDesignColumnsBuildContrast.merge_design_columns_and_build_contrast","title":"<code>merge_design_columns_and_build_contrast(shared_states)</code>","text":"<p>Merge the columns of the design matrices and build constrasts.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (dictionaries of design columns) from training nodes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Shared state containing: - merged_columns: the names of the columns that the local design matrices   should have. - contrast: the contrast (in a list of strings form) to be used for the   DESeq2 model.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>@remote\n@log_remote\ndef merge_design_columns_and_build_contrast(self, shared_states):\n    \"\"\"Merge the columns of the design matrices and build constrasts.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (dictionaries of design columns) from training nodes.\n\n    Returns\n    -------\n    dict\n        Shared state containing:\n        - merged_columns: the names of the columns that the local design matrices\n          should have.\n        - contrast: the contrast (in a list of strings form) to be used for the\n          DESeq2 model.\n    \"\"\"\n    merged_columns = pd.Index([])\n\n    for state in shared_states:\n        merged_columns = merged_columns.union(state[\"design_columns\"])\n\n    # We now also have everything to compute the contrasts\n    contrast = build_contrast(\n        self.design_factors,\n        merged_columns,\n        self.continuous_factors,\n        self.contrast,\n    )\n\n    return {\"merged_columns\": merged_columns, \"contrast\": contrast}\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.AggMergeDesignLevels","title":"<code>AggMergeDesignLevels</code>","text":"<p>Mixin to merge the levels of the design factors.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>class AggMergeDesignLevels:\n    \"\"\"Mixin to merge the levels of the design factors.\"\"\"\n\n    categorical_factors: list[str]\n\n    @remote\n    @log_remote\n    def merge_design_levels(self, shared_states):\n        \"\"\"Merge the levels of the design factors.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (dictionaries of local_levels) from training nodes.\n\n        Returns\n        -------\n        dict\n            Dictionary of unique levels for each factor.\n        \"\"\"\n        # merge levels\n        merged_levels = {factor: set() for factor in self.categorical_factors}\n        for factor in self.categorical_factors:\n            for state in shared_states:\n                merged_levels[factor] = set(state[\"local_levels\"][factor]).union(\n                    merged_levels[factor]\n                )\n\n        return {\n            \"merged_levels\": {\n                factor: np.array(list(levels))\n                for factor, levels in merged_levels.items()\n            }\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.AggMergeDesignLevels.merge_design_levels","title":"<code>merge_design_levels(shared_states)</code>","text":"<p>Merge the levels of the design factors.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (dictionaries of local_levels) from training nodes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of unique levels for each factor.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>@remote\n@log_remote\ndef merge_design_levels(self, shared_states):\n    \"\"\"Merge the levels of the design factors.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (dictionaries of local_levels) from training nodes.\n\n    Returns\n    -------\n    dict\n        Dictionary of unique levels for each factor.\n    \"\"\"\n    # merge levels\n    merged_levels = {factor: set() for factor in self.categorical_factors}\n    for factor in self.categorical_factors:\n        for state in shared_states:\n            merged_levels[factor] = set(state[\"local_levels\"][factor]).union(\n                merged_levels[factor]\n            )\n\n    return {\n        \"merged_levels\": {\n            factor: np.array(list(levels))\n            for factor, levels in merged_levels.items()\n        }\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocGetLocalFactors","title":"<code>LocGetLocalFactors</code>","text":"<p>Mixin to get the list of unique levels for each categorical design factor.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>class LocGetLocalFactors:\n    \"\"\"Mixin to get the list of unique levels for each categorical design factor.\"\"\"\n\n    categorical_factors: list[str]\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_local_factors(\n        self, data_from_opener, shared_state=None\n    ):  # pylint: disable=unused-argument\n        \"\"\"Get the list of unique levels for each categorical design factor.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Copied in local anndata objects.\n\n        shared_state : None, optional\n            Not used.\n\n        Returns\n        -------\n        dict\n            A dictionary of unique local levels for each factor.\n        \"\"\"\n        self.local_adata = data_from_opener.copy()\n        return {\n            \"local_levels\": {\n                factor: self.local_adata.obs[factor].unique()\n                for factor in self.categorical_factors\n            }\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocGetLocalFactors.get_local_factors","title":"<code>get_local_factors(data_from_opener, shared_state=None)</code>","text":"<p>Get the list of unique levels for each categorical design factor.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Copied in local anndata objects.</p> required <code>shared_state</code> <code>None</code> <p>Not used.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of unique local levels for each factor.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_local_factors(\n    self, data_from_opener, shared_state=None\n):  # pylint: disable=unused-argument\n    \"\"\"Get the list of unique levels for each categorical design factor.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Copied in local anndata objects.\n\n    shared_state : None, optional\n        Not used.\n\n    Returns\n    -------\n    dict\n        A dictionary of unique local levels for each factor.\n    \"\"\"\n    self.local_adata = data_from_opener.copy()\n    return {\n        \"local_levels\": {\n            factor: self.local_adata.obs[factor].unique()\n            for factor in self.categorical_factors\n        }\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocOderDesignComputeLogMean","title":"<code>LocOderDesignComputeLogMean</code>","text":"<p>Mixin to order design cols and compute the local log mean.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>The local AnnData.</p> <p>Methods:</p> Name Description <code>order_design_cols_compute_local_log_mean</code> <p>Order design columns and compute the local log mean.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>class LocOderDesignComputeLogMean:\n    \"\"\"Mixin to order design cols and compute the local log mean.\n\n    Attributes\n    ----------\n    local_adata : ad.AnnData\n        The local AnnData.\n\n    Methods\n    -------\n    order_design_cols_compute_local_log_mean\n        Order design columns and compute the local log mean.\n    \"\"\"\n\n    local_adata: ad.AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def order_design_cols_compute_local_log_mean(\n        self, data_from_opener, shared_state=None\n    ):\n        \"\"\"Order design columns and compute the local log mean.\n\n        This function also sets the contrast in the local AnnData,\n        and saves the number of parameters in the uns field.\n\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state with:\n            - \"merged_columns\" a set containing the names of columns that the design\n                matrix should have.\n            - \"contrast\" the contrast to be used for the DESeq2 model.\n\n        Returns\n        -------\n        dict\n            Local mean of logs and number of samples.\n        \"\"\"\n        #### ----Step 1: Order design columns---- ####\n\n        self.local_adata.uns[\"contrast\"] = shared_state[\"contrast\"]\n\n        for col in shared_state[\"merged_columns\"]:\n            if col not in self.local_adata.obsm[\"design_matrix\"].columns:\n                self.local_adata.obsm[\"design_matrix\"][col] = 0\n\n        # Reorder columns for consistency\n        self.local_adata.obsm[\"design_matrix\"] = self.local_adata.obsm[\"design_matrix\"][\n            shared_state[\"merged_columns\"]\n        ]\n\n        # Save the number of params in an uns field for easy access\n        self.local_adata.uns[\"n_params\"] = self.local_adata.obsm[\"design_matrix\"].shape[\n            1\n        ]\n\n        #### ----Step 2: Compute local log mean---- ####\n\n        with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n            return {\n                \"log_mean\": np.log(data_from_opener.X).mean(axis=0),\n                \"n_samples\": data_from_opener.n_obs,\n            }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocOderDesignComputeLogMean.order_design_cols_compute_local_log_mean","title":"<code>order_design_cols_compute_local_log_mean(data_from_opener, shared_state=None)</code>","text":"<p>Order design columns and compute the local log mean.</p> <p>This function also sets the contrast in the local AnnData, and saves the number of parameters in the uns field.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state with: - \"merged_columns\" a set containing the names of columns that the design     matrix should have. - \"contrast\" the contrast to be used for the DESeq2 model.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Local mean of logs and number of samples.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef order_design_cols_compute_local_log_mean(\n    self, data_from_opener, shared_state=None\n):\n    \"\"\"Order design columns and compute the local log mean.\n\n    This function also sets the contrast in the local AnnData,\n    and saves the number of parameters in the uns field.\n\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state with:\n        - \"merged_columns\" a set containing the names of columns that the design\n            matrix should have.\n        - \"contrast\" the contrast to be used for the DESeq2 model.\n\n    Returns\n    -------\n    dict\n        Local mean of logs and number of samples.\n    \"\"\"\n    #### ----Step 1: Order design columns---- ####\n\n    self.local_adata.uns[\"contrast\"] = shared_state[\"contrast\"]\n\n    for col in shared_state[\"merged_columns\"]:\n        if col not in self.local_adata.obsm[\"design_matrix\"].columns:\n            self.local_adata.obsm[\"design_matrix\"][col] = 0\n\n    # Reorder columns for consistency\n    self.local_adata.obsm[\"design_matrix\"] = self.local_adata.obsm[\"design_matrix\"][\n        shared_state[\"merged_columns\"]\n    ]\n\n    # Save the number of params in an uns field for easy access\n    self.local_adata.uns[\"n_params\"] = self.local_adata.obsm[\"design_matrix\"].shape[\n        1\n    ]\n\n    #### ----Step 2: Compute local log mean---- ####\n\n    with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n        return {\n            \"log_mean\": np.log(data_from_opener.X).mean(axis=0),\n            \"n_samples\": data_from_opener.n_obs,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocSetLocalDesign","title":"<code>LocSetLocalDesign</code>","text":"<p>Mixin to set the design matrices in centers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>class LocSetLocalDesign:\n    \"\"\"Mixin to set the design matrices in centers.\"\"\"\n\n    local_adata: ad.AnnData\n    design_factors: list[str]\n    continuous_factors: list[str] | None\n    ref_levels: dict[str, str] | None\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def set_local_design(\n        self,\n        data_from_opener,\n        shared_state,\n    ):\n        # pylint: disable=unused-argument\n        \"\"\"Set the design matrices in centers.\n\n        Returns their columns in order to harmonize them.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state with a \"design_columns\" key containing a dictionary with, for\n            each design factor, the names of its unique levels.\n\n        Returns\n        -------\n        dict\n            Local design columns.\n        \"\"\"\n        self.local_adata.obsm[\"design_matrix\"] = build_design_matrix(\n            metadata=self.local_adata.obs,\n            design_factors=self.design_factors,\n            continuous_factors=self.continuous_factors,\n            levels=shared_state[\"merged_levels\"],\n            ref_levels=self.ref_levels,\n        )\n        return {\"design_columns\": self.local_adata.obsm[\"design_matrix\"].columns}\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#fedpydeseq2.core.deseq2_core.build_design_matrix.substeps.LocSetLocalDesign.set_local_design","title":"<code>set_local_design(data_from_opener, shared_state)</code>","text":"<p>Set the design matrices in centers.</p> <p>Returns their columns in order to harmonize them.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state with a \"design_columns\" key containing a dictionary with, for each design factor, the names of its unique levels.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Local design columns.</p> Source code in <code>fedpydeseq2/core/deseq2_core/build_design_matrix/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef set_local_design(\n    self,\n    data_from_opener,\n    shared_state,\n):\n    # pylint: disable=unused-argument\n    \"\"\"Set the design matrices in centers.\n\n    Returns their columns in order to harmonize them.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state with a \"design_columns\" key containing a dictionary with, for\n        each design factor, the names of its unique levels.\n\n    Returns\n    -------\n    dict\n        Local design columns.\n    \"\"\"\n    self.local_adata.obsm[\"design_matrix\"] = build_design_matrix(\n        metadata=self.local_adata.obs,\n        design_factors=self.design_factors,\n        continuous_factors=self.continuous_factors,\n        levels=shared_state[\"merged_levels\"],\n        ref_levels=self.ref_levels,\n    )\n    return {\"design_columns\": self.local_adata.obsm[\"design_matrix\"].columns}\n</code></pre>"},{"location":"api/core/deseq2_core/build_design_matrix/build_design_matrix/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 local_levels dict A dictionary whose keys are the names of the categorical factors, and whose values are the list of values taken by this factor in a given center. For example \\(\\texttt{\\{stage: [Advanced, Non-advanced], gender: [female]\\}}\\). Each center Server 2 merged_levels dict A dictionary whose keys are the names of the categorical factors and whose values are arrays containing the list of values taken by this factor across all centers. For example \\(\\texttt{\\{stage: [Advanced , Non-advanced], gender: [female, male]\\}}\\). Server Center 3 design_columns Index \\((p,)\\) The name of the columns of the local design matrix, before aggregation. They are of the form intercept, factor for continuous factors, and factor_level_vs_factor_ref_level otherwise. For example, \\(\\texttt{[intercept, stage\\_Advanced\\_vs\\_Non-advanced, gender\\_male\\_vs\\_female]}\\). Each center Server 4 merged_columns Index \\((p,)\\) The union of the design columns across all centers. Local design matrices will then be updated to include all columns. Server Center 4 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server Center 5 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 5 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server"},{"location":"api/core/deseq2_core/build_design_matrix/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 local_levels dict A dictionary whose keys are the names of the categorical factors, and whose values are the list of values taken by this factor in a given center. For example \\(\\texttt{\\{stage: [Advanced, Non-advanced], gender: [female]\\}}\\). Each center Server 2 merged_levels dict A dictionary whose keys are the names of the categorical factors and whose values are arrays containing the list of values taken by this factor across all centers. For example \\(\\texttt{\\{stage: [Advanced , Non-advanced], gender: [female, male]\\}}\\). Server Center 3 design_columns Index \\((p,)\\) The name of the columns of the local design matrix, before aggregation. They are of the form intercept, factor for continuous factors, and factor_level_vs_factor_ref_level otherwise. For example, \\(\\texttt{[intercept, stage\\_Advanced\\_vs\\_Non-advanced, gender\\_male\\_vs\\_female]}\\). Each center Server 4 merged_columns Index \\((p,)\\) The union of the design columns across all centers. Local design matrices will then be updated to include all columns. Server Center 4 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server Center 5 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 5 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>compute_cook_distance</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#api","title":"API","text":"<p>Pipe step computing the cooks distance.</p>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.compute_cook_distance","title":"<code>compute_cook_distance</code>","text":""},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.compute_cook_distance.ComputeCookDistances","title":"<code>ComputeCookDistances</code>","text":"<p>               Bases: <code>ComputeTrimmedMean</code>, <code>LocComputeSqerror</code>, <code>LocGetNormedCounts</code>, <code>AggComputeDispersionForCook</code></p> <p>Mixin class to compute Cook's distances.</p> <p>Methods:</p> Name Description <code>compute_cook_distance</code> <p>The method to compute Cook's distances.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/compute_cook_distance.py</code> <pre><code>class ComputeCookDistances(\n    ComputeTrimmedMean,\n    LocComputeSqerror,\n    LocGetNormedCounts,\n    AggComputeDispersionForCook,\n):\n    \"\"\"Mixin class to compute Cook's distances.\n\n    Methods\n    -------\n    compute_cook_distance\n        The method to compute Cook's distances.\n    \"\"\"\n\n    trimmed_mean_num_iter: int\n\n    @log_organisation_method\n    def compute_cook_distance(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Compute Cook's distances.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            list of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[dict]\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. The new local state contains Cook's distances.\n\n        dispersion_for_cook_shared_state: dict\n            Shared state with the dispersion values for Cook's distances, in a\n            \"cooks_dispersions\" key.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        local_states, agg_shared_state, round_idx = self.compute_trim_mean(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n            layer_used=\"normed_counts\",\n            mode=\"cooks\",\n            trim_ratio=None,\n            n_iter=self.trimmed_mean_num_iter,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_compute_sqerror,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=agg_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local sqerror\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, agg_shared_state, round_idx = self.compute_trim_mean(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n            layer_used=\"sqerror\",\n            mode=\"cooks\",\n            trim_ratio=None,\n            n_iter=self.trimmed_mean_num_iter,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_get_normed_count_means,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=agg_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get normed count means\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        dispersion_for_cook_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_compute_dispersion_for_cook,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Compute dispersion for Cook distances\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        return local_states, dispersion_for_cook_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.compute_cook_distance.ComputeCookDistances.compute_cook_distance","title":"<code>compute_cook_distance(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Compute Cook's distances.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>list of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. The new local state contains Cook's distances.</p> <code>dispersion_for_cook_shared_state</code> <code>dict</code> <p>Shared state with the dispersion values for Cook's distances, in a \"cooks_dispersions\" key.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/compute_cook_distance.py</code> <pre><code>@log_organisation_method\ndef compute_cook_distance(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Compute Cook's distances.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        list of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[dict]\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. The new local state contains Cook's distances.\n\n    dispersion_for_cook_shared_state: dict\n        Shared state with the dispersion values for Cook's distances, in a\n        \"cooks_dispersions\" key.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    local_states, agg_shared_state, round_idx = self.compute_trim_mean(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n        layer_used=\"normed_counts\",\n        mode=\"cooks\",\n        trim_ratio=None,\n        n_iter=self.trimmed_mean_num_iter,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_compute_sqerror,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=agg_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Compute local sqerror\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, agg_shared_state, round_idx = self.compute_trim_mean(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n        layer_used=\"sqerror\",\n        mode=\"cooks\",\n        trim_ratio=None,\n        n_iter=self.trimmed_mean_num_iter,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_get_normed_count_means,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=agg_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get normed count means\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    dispersion_for_cook_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_compute_dispersion_for_cook,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Compute dispersion for Cook distances\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    return local_states, dispersion_for_cook_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.AggComputeDispersionForCook","title":"<code>AggComputeDispersionForCook</code>","text":"<p>Compute the dispersion for Cook's distance calculation.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>class AggComputeDispersionForCook:\n    \"\"\"Compute the dispersion for Cook's distance calculation.\"\"\"\n\n    @remote\n    @log_remote\n    @prepare_cooks_agg\n    def agg_compute_dispersion_for_cook(\n        self,\n        shared_states: list[dict],\n    ) -&gt; dict:\n        \"\"\"Compute the dispersion for Cook's distance calculation.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            list of shared states with the following keys:\n            - mean_normed_counts: mean of the normalized counts\n            - n_samples: number of samples\n            - varEst: variance estimate\n\n        Returns\n        -------\n        dict\n            Because it is decorated, the dictionary will have the following key:\n            - cooks_dispersions: dispersion values\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.AggComputeDispersionForCook.agg_compute_dispersion_for_cook","title":"<code>agg_compute_dispersion_for_cook(shared_states)</code>","text":"<p>Compute the dispersion for Cook's distance calculation.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>list of shared states with the following keys: - mean_normed_counts: mean of the normalized counts - n_samples: number of samples - varEst: variance estimate</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Because it is decorated, the dictionary will have the following key: - cooks_dispersions: dispersion values</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>@remote\n@log_remote\n@prepare_cooks_agg\ndef agg_compute_dispersion_for_cook(\n    self,\n    shared_states: list[dict],\n) -&gt; dict:\n    \"\"\"Compute the dispersion for Cook's distance calculation.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        list of shared states with the following keys:\n        - mean_normed_counts: mean of the normalized counts\n        - n_samples: number of samples\n        - varEst: variance estimate\n\n    Returns\n    -------\n    dict\n        Because it is decorated, the dictionary will have the following key:\n        - cooks_dispersions: dispersion values\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.LocComputeSqerror","title":"<code>LocComputeSqerror</code>","text":"<p>Compute the squared error between the normalized counts and the trimmed mean.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>class LocComputeSqerror:\n    \"\"\"Compute the squared error between the normalized counts and the trimmed mean.\"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_compute_sqerror(\n        self,\n        data_from_opener,\n        shared_state=dict,\n    ) -&gt; None:\n        \"\"\"Compute the squared error between the normalized counts and the trimmed mean.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict, optional\n            Results to save in the local states.\n        \"\"\"\n        cell_means = shared_state[\"trimmed_mean_normed_counts\"]\n        if isinstance(cell_means, pd.DataFrame):\n            cell_means.index = self.local_adata.var_names\n            self.local_adata.varm[\"cell_means\"] = cell_means\n        else:\n            # In this case, the cell means are not computed per\n            # level but overall\n            self.local_adata.varm[\"cell_means\"] = cell_means\n        set_sqerror_layer(self.local_adata)\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.LocComputeSqerror.local_compute_sqerror","title":"<code>local_compute_sqerror(data_from_opener, shared_state=dict)</code>","text":"<p>Compute the squared error between the normalized counts and the trimmed mean.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Results to save in the local states.</p> <code>dict</code> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_compute_sqerror(\n    self,\n    data_from_opener,\n    shared_state=dict,\n) -&gt; None:\n    \"\"\"Compute the squared error between the normalized counts and the trimmed mean.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict, optional\n        Results to save in the local states.\n    \"\"\"\n    cell_means = shared_state[\"trimmed_mean_normed_counts\"]\n    if isinstance(cell_means, pd.DataFrame):\n        cell_means.index = self.local_adata.var_names\n        self.local_adata.varm[\"cell_means\"] = cell_means\n    else:\n        # In this case, the cell means are not computed per\n        # level but overall\n        self.local_adata.varm[\"cell_means\"] = cell_means\n    set_sqerror_layer(self.local_adata)\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.LocGetNormedCounts","title":"<code>LocGetNormedCounts</code>","text":"<p>Get the mean of the normalized counts.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>class LocGetNormedCounts:\n    \"\"\"Get the mean of the normalized counts.\"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    @prepare_cooks_local\n    def local_get_normed_count_means(\n        self,\n        data_from_opener,\n        shared_state=dict,\n    ) -&gt; dict:\n        \"\"\"Send local normed counts means.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict, optional\n            Dictionary with the following keys:\n            - varEst: variance estimate for Cook's distance calculation\n\n        Returns\n        -------\n        dict\n            Because of the decorator, dictionary with the following keys:\n            - mean_normed_counts: mean of the normalized counts\n            - n_samples: number of samples\n            - varEst: variance estimate\n        \"\"\"\n        return {}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#fedpydeseq2.core.deseq2_core.compute_cook_distance.substeps.LocGetNormedCounts.local_get_normed_count_means","title":"<code>local_get_normed_count_means(data_from_opener, shared_state=dict)</code>","text":"<p>Send local normed counts means.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with the following keys: - varEst: variance estimate for Cook's distance calculation</p> <code>dict</code> <p>Returns:</p> Type Description <code>dict</code> <p>Because of the decorator, dictionary with the following keys: - mean_normed_counts: mean of the normalized counts - n_samples: number of samples - varEst: variance estimate</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_cook_distance/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\n@prepare_cooks_local\ndef local_get_normed_count_means(\n    self,\n    data_from_opener,\n    shared_state=dict,\n) -&gt; dict:\n    \"\"\"Send local normed counts means.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict, optional\n        Dictionary with the following keys:\n        - varEst: variance estimate for Cook's distance calculation\n\n    Returns\n    -------\n    dict\n        Because of the decorator, dictionary with the following keys:\n        - mean_normed_counts: mean of the normalized counts\n        - n_samples: number of samples\n        - varEst: variance estimate\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_cook_distance/compute_cook_distance/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the normed counts will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the normed counts will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 1 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{min}}_{g,l})^{(k)}\\). Each center Server 2 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k (Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k (Z^{\\texttt{min}}_{g,l})^{(k)}\\). Server Center 2 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 3 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 3 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 4 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 4 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 5 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 5 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the normed counts across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\). Each center Server 6 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center 8 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the squared errors will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the squared errors will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 8 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Let \\(R^{\\texttt{trim}}_k\\) be the matrix of square errors of size \\((n,G)\\) whose entries are \\((Z^{(k)}_{ig} - \\overline{Z}^{\\texttt{trim}}_{g,l})^2\\) if \\(i \\in \\mathcal{I}_{k,l}\\) for some \\(l \\in \\mathcal{L}_{\\geq 3}\\) and \\(\\texttt{nan}\\) otherwise, for \\(1 \\leq i\\leq n_k\\) and \\(1 \\leq g \\leq G\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center squared errors for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\(R^{\\texttt{max}}_{k,g,l}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center squared errors for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\(R^{\\texttt{min}}_{k,g,l}\\). Each center Server 9 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 9 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the squared errors of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k R^{\\texttt{max}}_{k,g,l}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the squared errors of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k R^{\\texttt{min}}_{k,g,l}\\). Server Center 10 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose squared errors value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 10 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 11 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose squared errors value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose squared errors value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 11 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 12 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the squared errors across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose squared errors value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\).  \"scale\", equal to \\(1.51\\). Each center Server 12 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 13 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). If use_lvl is \\(\\texttt{True}\\), for each gene \\(g\\), it is the maximum across all admissible levels of the trimmed mean of the squared error for the gene and level in question, with trim ratio \\(r_{\\texttt{trim}}\\), scaled (multiplied) by the scale factor \\(1.51\\). If use_level is \\(\\texttt{False}\\), the trimmed mean of the squared error scaled by \\(1.51\\) with trim ratio \\(r_{\\texttt{trim}}\\). Server Center 14 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 14 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is passed on from the previous shared state. Each center Server 14 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 14 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 14 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 15 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 15 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center"},{"location":"api/core/deseq2_core/compute_cook_distance/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the normed counts will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the normed counts will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 1 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{min}}_{g,l})^{(k)}\\). Each center Server 2 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k (Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k (Z^{\\texttt{min}}_{g,l})^{(k)}\\). Server Center 2 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 3 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 3 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 4 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 4 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 5 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 5 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the normed counts across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\). Each center Server 6 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center 8 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the squared errors will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the squared errors will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 8 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Let \\(R^{\\texttt{trim}}_k\\) be the matrix of square errors of size \\((n,G)\\) whose entries are \\((Z^{(k)}_{ig} - \\overline{Z}^{\\texttt{trim}}_{g,l})^2\\) if \\(i \\in \\mathcal{I}_{k,l}\\) for some \\(l \\in \\mathcal{L}_{\\geq 3}\\) and \\(\\texttt{nan}\\) otherwise, for \\(1 \\leq i\\leq n_k\\) and \\(1 \\leq g \\leq G\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center squared errors for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\(R^{\\texttt{max}}_{k,g,l}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center squared errors for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\(R^{\\texttt{min}}_{k,g,l}\\). Each center Server 9 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 9 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the squared errors of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k R^{\\texttt{max}}_{k,g,l}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the squared errors of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k R^{\\texttt{min}}_{k,g,l}\\). Server Center 10 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose squared errors value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 10 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 11 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose squared errors value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the squared errors of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose squared errors value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 11 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 12 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the squared errors across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose squared errors value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\).  \"scale\", equal to \\(1.51\\). Each center Server 12 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 13 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). If use_lvl is \\(\\texttt{True}\\), for each gene \\(g\\), it is the maximum across all admissible levels of the trimmed mean of the squared error for the gene and level in question, with trim ratio \\(r_{\\texttt{trim}}\\), scaled (multiplied) by the scale factor \\(1.51\\). If use_level is \\(\\texttt{False}\\), the trimmed mean of the squared error scaled by \\(1.51\\) with trim ratio \\(r_{\\texttt{trim}}\\). Server Center 14 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 14 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is passed on from the previous shared state. Each center Server 14 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 14 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 14 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 15 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 15 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>compute_size_factors</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#api","title":"API","text":"<p>Module to implement the computation of size factors.</p>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.compute_size_factors","title":"<code>compute_size_factors</code>","text":"<p>Module containing the steps for the computation of rough dispersions.</p>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.compute_size_factors.ComputeSizeFactors","title":"<code>ComputeSizeFactors</code>","text":"<p>               Bases: <code>AggLogMeans</code>, <code>LocSetSizeFactorsComputeGramAndFeatures</code></p> <p>Mixin class to implement the computation of size factors.</p> <p>Methods:</p> Name Description <code>compute_size_factors</code> <p>The method to compute the size factors, that must be used in the main pipeline. It sets the size factors in the local AnnData and computes the Gram matrix and feature vector in order to start the next step, i.e., the computation of rough dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/compute_size_factors.py</code> <pre><code>class ComputeSizeFactors(\n    AggLogMeans,\n    LocSetSizeFactorsComputeGramAndFeatures,\n):\n    \"\"\"Mixin class to implement the computation of size factors.\n\n    Methods\n    -------\n    compute_size_factors\n        The method to compute the size factors, that must be used in the main\n        pipeline. It sets the size factors in the local AnnData and computes the\n        Gram matrix and feature vector in order to start the next step, i.e.,\n        the computation of rough dispersions.\n    \"\"\"\n\n    @log_organisation_method\n    def compute_size_factors(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Compute size factors.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_states: list\n            Shared states which are the output of the \"build_design_matrix\" step.\n            These shared states contain the following fields:\n            - \"log_mean\" : the log mean of the gene expressions.\n            - \"n_samples\" : the number of samples in each client.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_states: dict\n            Shared states which contain the local information necessary to start\n            running the compute rough dispersions step. These shared states contain\n            a \"local_gram_matrix\" and a \"local_features\" key.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        # ---- Aggregate means of log gene expressions ----#\n\n        log_mean_aggregated_state, round_idx = aggregation_step(\n            aggregation_method=self.aggregate_log_means,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Aggregating local log means\",\n            clean_models=clean_models,\n        )\n\n        # ---- Set local size factors and return next shared states ---- #\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_set_size_factors_compute_gram_and_features,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=log_mean_aggregated_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=(\n                \"Setting local size factors and \"\n                \"computing Gram matrices and feature vectors\"\n            ),\n            clean_models=clean_models,\n        )\n\n        return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.compute_size_factors.ComputeSizeFactors.compute_size_factors","title":"<code>compute_size_factors(train_data_nodes, aggregation_node, local_states, shared_states, round_idx, clean_models)</code>","text":"<p>Compute size factors.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>shared_states</code> <p>Shared states which are the output of the \"build_design_matrix\" step. These shared states contain the following fields: - \"log_mean\" : the log mean of the gene expressions. - \"n_samples\" : the number of samples in each client.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>shared_states</code> <code>dict</code> <p>Shared states which contain the local information necessary to start running the compute rough dispersions step. These shared states contain a \"local_gram_matrix\" and a \"local_features\" key.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/compute_size_factors.py</code> <pre><code>@log_organisation_method\ndef compute_size_factors(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    shared_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Compute size factors.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_states: list\n        Shared states which are the output of the \"build_design_matrix\" step.\n        These shared states contain the following fields:\n        - \"log_mean\" : the log mean of the gene expressions.\n        - \"n_samples\" : the number of samples in each client.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_states: dict\n        Shared states which contain the local information necessary to start\n        running the compute rough dispersions step. These shared states contain\n        a \"local_gram_matrix\" and a \"local_features\" key.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    # ---- Aggregate means of log gene expressions ----#\n\n    log_mean_aggregated_state, round_idx = aggregation_step(\n        aggregation_method=self.aggregate_log_means,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Aggregating local log means\",\n        clean_models=clean_models,\n    )\n\n    # ---- Set local size factors and return next shared states ---- #\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_set_size_factors_compute_gram_and_features,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=log_mean_aggregated_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=(\n            \"Setting local size factors and \"\n            \"computing Gram matrices and feature vectors\"\n        ),\n        clean_models=clean_models,\n    )\n\n    return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.substeps","title":"<code>substeps</code>","text":"<p>Module containing the substeps for the computation of size factors.</p>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.substeps.AggLogMeans","title":"<code>AggLogMeans</code>","text":"<p>Mixin to compute the global mean given the local results.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/substeps.py</code> <pre><code>class AggLogMeans:\n    \"\"\"Mixin to compute the global mean given the local results.\"\"\"\n\n    @remote\n    @log_remote\n    def aggregate_log_means(self, shared_states):\n        \"\"\"Compute the global mean given the local results.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (local_mean, n_samples) from training nodes.\n\n        Returns\n        -------\n        dict\n            Global mean of log counts, and new all-zero genes if in refit mode.\n        \"\"\"\n        tot_mean = aggregate_means(\n            [state[\"log_mean\"] for state in shared_states],\n            [state[\"n_samples\"] for state in shared_states],\n        )\n\n        return {\"global_log_mean\": tot_mean}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.substeps.AggLogMeans.aggregate_log_means","title":"<code>aggregate_log_means(shared_states)</code>","text":"<p>Compute the global mean given the local results.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (local_mean, n_samples) from training nodes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Global mean of log counts, and new all-zero genes if in refit mode.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/substeps.py</code> <pre><code>@remote\n@log_remote\ndef aggregate_log_means(self, shared_states):\n    \"\"\"Compute the global mean given the local results.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (local_mean, n_samples) from training nodes.\n\n    Returns\n    -------\n    dict\n        Global mean of log counts, and new all-zero genes if in refit mode.\n    \"\"\"\n    tot_mean = aggregate_means(\n        [state[\"log_mean\"] for state in shared_states],\n        [state[\"n_samples\"] for state in shared_states],\n    )\n\n    return {\"global_log_mean\": tot_mean}\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.substeps.LocSetSizeFactorsComputeGramAndFeatures","title":"<code>LocSetSizeFactorsComputeGramAndFeatures</code>","text":"<p>Mixin to set local size factors and return local Gram matrices and features.</p> <p>This Mixin implements the method to perform the transition between the compute_size_factors and compute_rough_dispersions steps. It sets the size factors in the local AnnData and computes the Gram matrix and feature vector.</p> <p>Methods:</p> Name Description <code>local_set_size_factors_compute_gram_and_features</code> <p>The method to set the size factors and compute the Gram matrix and feature.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/substeps.py</code> <pre><code>class LocSetSizeFactorsComputeGramAndFeatures:\n    \"\"\"Mixin to set local size factors and return local Gram matrices and features.\n\n    This Mixin implements the method to perform the transition between the\n    compute_size_factors and compute_rough_dispersions steps. It sets the size\n    factors in the local AnnData and computes the Gram matrix and feature vector.\n\n    Methods\n    -------\n    local_set_size_factors_compute_gram_and_features\n        The method to set the size factors and compute the Gram matrix and feature.\n    \"\"\"\n\n    local_adata: ad.AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_set_size_factors_compute_gram_and_features(\n        self,\n        data_from_opener,\n        shared_state,\n    ) -&gt; dict:\n        # pylint: disable=unused-argument\n        \"\"\"Set local size factor and compute Gram matrix and feature vector.\n\n        This is a local method, used to fit the rough dispersions.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state containing the \"global_log_mean\" key.\n\n        Returns\n        -------\n        dict\n            Local gram matrices and feature vectors to be shared via shared_state to\n            the aggregation node.\n        \"\"\"\n        #### ---- Compute size factors ---- ####\n\n        global_log_means = shared_state[\"global_log_mean\"]\n        # Filter out genes with -\u221e log means\n        filtered_genes = ~np.isinf(global_log_means)\n\n        log_ratios = (\n            np.log(self.local_adata.X[:, filtered_genes])\n            - global_log_means[filtered_genes]\n        )\n        # Compute sample-wise median of log ratios\n        log_medians = np.median(log_ratios, axis=1)\n        # Return raw counts divided by size factors (exponential of log ratios)\n        # and size factors\n        self.local_adata.obsm[\"size_factors\"] = np.exp(log_medians)\n        self.local_adata.layers[\"normed_counts\"] = (\n            self.local_adata.X / self.local_adata.obsm[\"size_factors\"][:, None]\n        )\n\n        design = self.local_adata.obsm[\"design_matrix\"].values\n\n        return {\n            \"local_gram_matrix\": design.T @ design,\n            \"local_features\": design.T @ self.local_adata.layers[\"normed_counts\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#fedpydeseq2.core.deseq2_core.compute_size_factors.substeps.LocSetSizeFactorsComputeGramAndFeatures.local_set_size_factors_compute_gram_and_features","title":"<code>local_set_size_factors_compute_gram_and_features(data_from_opener, shared_state)</code>","text":"<p>Set local size factor and compute Gram matrix and feature vector.</p> <p>This is a local method, used to fit the rough dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing the \"global_log_mean\" key.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Local gram matrices and feature vectors to be shared via shared_state to the aggregation node.</p> Source code in <code>fedpydeseq2/core/deseq2_core/compute_size_factors/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_set_size_factors_compute_gram_and_features(\n    self,\n    data_from_opener,\n    shared_state,\n) -&gt; dict:\n    # pylint: disable=unused-argument\n    \"\"\"Set local size factor and compute Gram matrix and feature vector.\n\n    This is a local method, used to fit the rough dispersions.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state containing the \"global_log_mean\" key.\n\n    Returns\n    -------\n    dict\n        Local gram matrices and feature vectors to be shared via shared_state to\n        the aggregation node.\n    \"\"\"\n    #### ---- Compute size factors ---- ####\n\n    global_log_means = shared_state[\"global_log_mean\"]\n    # Filter out genes with -\u221e log means\n    filtered_genes = ~np.isinf(global_log_means)\n\n    log_ratios = (\n        np.log(self.local_adata.X[:, filtered_genes])\n        - global_log_means[filtered_genes]\n    )\n    # Compute sample-wise median of log ratios\n    log_medians = np.median(log_ratios, axis=1)\n    # Return raw counts divided by size factors (exponential of log ratios)\n    # and size factors\n    self.local_adata.obsm[\"size_factors\"] = np.exp(log_medians)\n    self.local_adata.layers[\"normed_counts\"] = (\n        self.local_adata.X / self.local_adata.obsm[\"size_factors\"][:, None]\n    )\n\n    design = self.local_adata.obsm[\"design_matrix\"].values\n\n    return {\n        \"local_gram_matrix\": design.T @ design,\n        \"local_features\": design.T @ self.local_adata.layers[\"normed_counts\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/compute_size_factors/compute_size_factors/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 0 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 0 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 1 global_log_mean nparray \\((G,)\\) The mean of the log of the counts across all samples in all centers \\(\\overline{\\log(Y)}_g =\\sum_{k=1}^{K}\\tfrac{n_k}{n}\\overline{\\log(Y)}^{(k)}_{g}\\). Server Center 2 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 2 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server"},{"location":"api/core/deseq2_core/compute_size_factors/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 0 log_mean nparray \\((G,)\\) For each gene, the mean of the log of the counts across all samples in a center \\(\\overline{\\log(Y)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}\\log(Y^{(k)}_{ig})\\). Each center Server 0 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 1 global_log_mean nparray \\((G,)\\) The mean of the log of the counts across all samples in all centers \\(\\overline{\\log(Y)}_g =\\sum_{k=1}^{K}\\tfrac{n_k}{n}\\overline{\\log(Y)}^{(k)}_{g}\\). Server Center 2 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 2 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>run_deseq2_lfc_dispersions</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#api","title":"API","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions","title":"<code>compute_MAP_dispersions</code>","text":"<p>Module containing the mixin class to compute MAP dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.compute_MAP_dispersions","title":"<code>compute_MAP_dispersions</code>","text":"<p>Main module to compute dispersions by minimizing the MLE using a grid search.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.compute_MAP_dispersions.ComputeMAPDispersions","title":"<code>ComputeMAPDispersions</code>","text":"<p>               Bases: <code>LocFilterMAPDispersions</code>, <code>ComputeDispersionsGridSearch</code></p> <p>Mixin class to implement the computation of MAP dispersions.</p> <p>Methods:</p> Name Description <code>fit_MAP_dispersions</code> <p>A method to fit the MAP dispersions and filter them. The filtering is done by removing the dispersions that are too far from the trend curve.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_MAP_dispersions/compute_MAP_dispersions.py</code> <pre><code>class ComputeMAPDispersions(\n    LocFilterMAPDispersions,\n    ComputeDispersionsGridSearch,\n):\n    \"\"\"Mixin class to implement the computation of MAP dispersions.\n\n    Methods\n    -------\n    fit_MAP_dispersions\n        A method to fit the MAP dispersions and filter them.\n        The filtering is done by removing the dispersions that are too far from the\n        trend curve.\n    \"\"\"\n\n    @log_organisation_method\n    def fit_MAP_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_state,\n        round_idx,\n        clean_models,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Fit MAP dispersions, and apply filtering.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_state: dict\n            Contains the output of the trend fitting,\n            that is a dictionary with a \"fitted_dispersion\" field containing\n            the fitted dispersions from the trend curve, a \"prior_disp_var\" field\n            containing the prior variance of the dispersions, and a \"_squared_logres\"\n            field containing the squared residuals of the trend fitting.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        refit_mode: bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s. (default: False).\n\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        local_states, shared_state, round_idx = self.fit_dispersions(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            shared_state=shared_state,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            fit_mode=\"MAP\",\n            refit_mode=refit_mode,\n        )\n\n        # Filter the MAP dispersions.\n        local_states, _, round_idx = local_step(\n            local_method=self.filter_outlier_genes,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Filter MAP dispersions.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.compute_MAP_dispersions.ComputeMAPDispersions.fit_MAP_dispersions","title":"<code>fit_MAP_dispersions(train_data_nodes, aggregation_node, local_states, shared_state, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Fit MAP dispersions, and apply filtering.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>shared_state</code> <p>Contains the output of the trend fitting, that is a dictionary with a \"fitted_dispersion\" field containing the fitted dispersions from the trend curve, a \"prior_disp_var\" field containing the prior variance of the dispersions, and a \"_squared_logres\" field containing the squared residuals of the trend fitting.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_MAP_dispersions/compute_MAP_dispersions.py</code> <pre><code>@log_organisation_method\ndef fit_MAP_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    shared_state,\n    round_idx,\n    clean_models,\n    refit_mode: bool = False,\n):\n    \"\"\"Fit MAP dispersions, and apply filtering.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_state: dict\n        Contains the output of the trend fitting,\n        that is a dictionary with a \"fitted_dispersion\" field containing\n        the fitted dispersions from the trend curve, a \"prior_disp_var\" field\n        containing the prior variance of the dispersions, and a \"_squared_logres\"\n        field containing the squared residuals of the trend fitting.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    refit_mode: bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s. (default: False).\n\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    local_states, shared_state, round_idx = self.fit_dispersions(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_state=shared_state,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        fit_mode=\"MAP\",\n        refit_mode=refit_mode,\n    )\n\n    # Filter the MAP dispersions.\n    local_states, _, round_idx = local_step(\n        local_method=self.filter_outlier_genes,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Filter MAP dispersions.\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.substeps.LocFilterMAPDispersions","title":"<code>LocFilterMAPDispersions</code>","text":"<p>Mixin to filter MAP dispersions and obtain the final dispersion estimates.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_MAP_dispersions/substeps.py</code> <pre><code>class LocFilterMAPDispersions:\n    \"\"\"Mixin to filter MAP dispersions and obtain the final dispersion estimates.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def filter_outlier_genes(\n        self,\n        data_from_opener,\n        shared_state,\n        refit_mode: bool = False,\n    ) -&gt; None:\n        \"\"\"Filter out outlier genes.\n\n        Avoids shrinking the dispersions of genes that are too far from the trend curve.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Not used.\n\n        shared_state : dict\n            Contains:\n            - \"MAP_dispersions\": MAP dispersions,\n\n        refit_mode : bool\n            Whether to run the pipeline on `refit_adata`s instead of `local_adata`s.\n            (default: False).\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        adata.varm[\"MAP_dispersions\"] = shared_state[\"MAP_dispersions\"].copy()\n\n        adata.varm[\"dispersions\"] = adata.varm[\"MAP_dispersions\"].copy()\n        adata.varm[\"_outlier_genes\"] = np.log(\n            adata.varm[\"genewise_dispersions\"]\n        ) &gt; np.log(adata.varm[\"fitted_dispersions\"]) + 2 * np.sqrt(\n            adata.uns[\"_squared_logres\"]\n        )\n        adata.varm[\"dispersions\"][adata.varm[\"_outlier_genes\"]] = adata.varm[\n            \"genewise_dispersions\"\n        ][adata.varm[\"_outlier_genes\"]]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_MAP_dispersions.substeps.LocFilterMAPDispersions.filter_outlier_genes","title":"<code>filter_outlier_genes(data_from_opener, shared_state, refit_mode=False)</code>","text":"<p>Filter out outlier genes.</p> <p>Avoids shrinking the dispersions of genes that are too far from the trend curve.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Contains: - \"MAP_dispersions\": MAP dispersions,</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_MAP_dispersions/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef filter_outlier_genes(\n    self,\n    data_from_opener,\n    shared_state,\n    refit_mode: bool = False,\n) -&gt; None:\n    \"\"\"Filter out outlier genes.\n\n    Avoids shrinking the dispersions of genes that are too far from the trend curve.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Not used.\n\n    shared_state : dict\n        Contains:\n        - \"MAP_dispersions\": MAP dispersions,\n\n    refit_mode : bool\n        Whether to run the pipeline on `refit_adata`s instead of `local_adata`s.\n        (default: False).\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    adata.varm[\"MAP_dispersions\"] = shared_state[\"MAP_dispersions\"].copy()\n\n    adata.varm[\"dispersions\"] = adata.varm[\"MAP_dispersions\"].copy()\n    adata.varm[\"_outlier_genes\"] = np.log(\n        adata.varm[\"genewise_dispersions\"]\n    ) &gt; np.log(adata.varm[\"fitted_dispersions\"]) + 2 * np.sqrt(\n        adata.uns[\"_squared_logres\"]\n    )\n    adata.varm[\"dispersions\"][adata.varm[\"_outlier_genes\"]] = adata.varm[\n        \"genewise_dispersions\"\n    ][adata.varm[\"_outlier_genes\"]]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior","title":"<code>compute_dispersion_prior</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.compute_dispersion_prior","title":"<code>compute_dispersion_prior</code>","text":"<p>Module containing the steps for fitting the dispersion trend.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.compute_dispersion_prior.ComputeDispersionPrior","title":"<code>ComputeDispersionPrior</code>","text":"<p>               Bases: <code>AggFitDispersionTrendAndPrior</code>, <code>LocGetMeanDispersionAndMean</code>, <code>LocUpdateFittedDispersions</code></p> <p>Mixin class to implement the fit of the dispersion trend.</p> <p>Methods:</p> Name Description <code>compute_dispersion_prior</code> <p>The method to fit the dispersion trend.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/compute_dispersion_prior.py</code> <pre><code>class ComputeDispersionPrior(\n    AggFitDispersionTrendAndPrior,\n    LocGetMeanDispersionAndMean,\n    LocUpdateFittedDispersions,\n):\n    \"\"\"Mixin class to implement the fit of the dispersion trend.\n\n    Methods\n    -------\n    compute_dispersion_prior\n        The method to fit the dispersion trend.\n    \"\"\"\n\n    @log_organisation_method\n    def compute_dispersion_prior(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        genewise_dispersions_shared_state,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Fit the dispersion trend.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            list of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        genewise_dispersions_shared_state: dict\n            Shared state with a \"genewise_dispersions\" key.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        dispersion_trend_share_state: dict\n            Shared states with:\n            - \"fitted_dispersions\": the fitted dispersions,\n            - \"prior_disp_var\": the prior dispersion variance.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        # --- Return means and dispersions ---#\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.get_local_mean_and_dispersion,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=genewise_dispersions_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get local means and dispersions\",\n            clean_models=clean_models,\n        )\n\n        # ---- Fit dispersion trend ----#\n\n        dispersion_trend_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_fit_dispersion_trend_and_prior_dispersion,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Fitting dispersion trend\",\n            clean_models=clean_models,\n        )\n\n        return local_states, dispersion_trend_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.compute_dispersion_prior.ComputeDispersionPrior.compute_dispersion_prior","title":"<code>compute_dispersion_prior(train_data_nodes, aggregation_node, local_states, genewise_dispersions_shared_state, round_idx, clean_models)</code>","text":"<p>Fit the dispersion trend.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>list of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>genewise_dispersions_shared_state</code> <p>Shared state with a \"genewise_dispersions\" key.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>dispersion_trend_share_state</code> <code>dict</code> <p>Shared states with: - \"fitted_dispersions\": the fitted dispersions, - \"prior_disp_var\": the prior dispersion variance.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/compute_dispersion_prior.py</code> <pre><code>@log_organisation_method\ndef compute_dispersion_prior(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    genewise_dispersions_shared_state,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Fit the dispersion trend.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        list of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    genewise_dispersions_shared_state: dict\n        Shared state with a \"genewise_dispersions\" key.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    dispersion_trend_share_state: dict\n        Shared states with:\n        - \"fitted_dispersions\": the fitted dispersions,\n        - \"prior_disp_var\": the prior dispersion variance.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    # --- Return means and dispersions ---#\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.get_local_mean_and_dispersion,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=genewise_dispersions_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get local means and dispersions\",\n        clean_models=clean_models,\n    )\n\n    # ---- Fit dispersion trend ----#\n\n    dispersion_trend_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_fit_dispersion_trend_and_prior_dispersion,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Fitting dispersion trend\",\n        clean_models=clean_models,\n    )\n\n    return local_states, dispersion_trend_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps","title":"<code>substeps</code>","text":"<p>Module containing the substeps for the computation of size factors.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.AggFitDispersionTrendAndPrior","title":"<code>AggFitDispersionTrendAndPrior</code>","text":"<p>Mixin class to implement the fit of the dispersion trend.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>class AggFitDispersionTrendAndPrior:\n    \"\"\"Mixin class to implement the fit of the dispersion trend.\"\"\"\n\n    min_disp: float\n\n    @remote\n    @log_remote\n    def agg_fit_dispersion_trend_and_prior_dispersion(self, shared_states):\n        \"\"\"Fit the dispersion trend, and compute the dispersion prior.\n\n        Parameters\n        ----------\n        shared_states : dict\n            Shared states from the local step with the following keys:\n            - genewise_dispersions: np.ndarray of shape (n_genes,)\n            - n_params: int\n            - non_zero: np.ndarray of shape (n_genes,)\n            - mean_normed_counts: np.ndarray of shape (n_genes,)\n            - n_obs: int\n\n        Returns\n        -------\n        dict\n            dict with the following keys:\n            - prior_disp_var: float\n                The prior dispersion variance.\n            - _squared_logres: float\n                The squared log-residuals.\n            - trend_coeffs: np.ndarray of shape (2,)\n                The coefficients of the parametric dispersion trend.\n            - fitted_dispersions: np.ndarray of shape (n_genes,)\n                The fitted dispersions, computed from the dispersion trend.\n            - disp_function_type: str\n                The type of dispersion function (parametric or mean).\n            - mean_disp: float, optional\n                The mean dispersion (if \"mean\" fit type).\n        \"\"\"\n        genewise_dispersions = shared_states[0][\"genewise_dispersions\"]\n        n_params = shared_states[0][\"n_params\"]\n        non_zero = shared_states[0][\"non_zero\"]\n        n_total_obs = sum([state[\"n_obs\"] for state in shared_states])\n        mean_normed_counts = (\n            sum(\n                [\n                    state[\"mean_normed_counts\"] * state[\"n_obs\"]\n                    for state in shared_states\n                ]\n            )\n            / n_total_obs\n        )\n\n        # Exclude all-zero counts\n        targets = pd.Series(\n            genewise_dispersions.copy(),\n        )\n        targets = targets[non_zero]\n        covariates = pd.Series(1 / mean_normed_counts[non_zero], index=targets.index)\n\n        for gene in targets.index:\n            if (\n                np.isinf(covariates.loc[gene]).any()\n                or np.isnan(covariates.loc[gene]).any()\n            ):\n                targets.drop(labels=[gene], inplace=True)\n                covariates.drop(labels=[gene], inplace=True)\n\n        # Initialize coefficients\n        old_coeffs = pd.Series([0.1, 0.1])\n        coeffs = pd.Series([1.0, 1.0])\n        mean_disp = None\n\n        disp_function_type = \"parametric\"\n        while (coeffs &gt; 1e-10).all() and (\n            np.log(np.abs(coeffs / old_coeffs)) ** 2\n        ).sum() &gt;= 1e-6:\n            old_coeffs = coeffs\n            (\n                coeffs,\n                predictions,\n                converged,\n            ) = DefaultInference().dispersion_trend_gamma_glm(covariates, targets)\n\n            if not converged or (coeffs &lt;= 1e-10).any():\n                warnings.warn(\n                    \"The dispersion trend curve fitting did not converge. \"\n                    \"Switching to a mean-based dispersion trend.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                mean_disp = trim_mean(\n                    genewise_dispersions[genewise_dispersions &gt; 10 * self.min_disp],\n                    proportiontocut=0.001,\n                )\n                disp_function_type = \"mean\"\n\n            pred_ratios = genewise_dispersions[covariates.index] / predictions\n\n            targets.drop(\n                targets[(pred_ratios &lt; 1e-4) | (pred_ratios &gt;= 15)].index,\n                inplace=True,\n            )\n            covariates.drop(\n                covariates[(pred_ratios &lt; 1e-4) | (pred_ratios &gt;= 15)].index,\n                inplace=True,\n            )\n\n        fitted_dispersions = np.full_like(genewise_dispersions, np.NaN)\n\n        fitted_dispersions[non_zero] = disp_function(\n            mean_normed_counts[non_zero],\n            disp_function_type=disp_function_type,\n            coeffs=coeffs,\n            mean_disp=mean_disp,\n        )\n\n        disp_residuals = np.log(genewise_dispersions[non_zero]) - np.log(\n            fitted_dispersions[non_zero]\n        )\n\n        # Compute squared log-residuals and prior variance based on genes whose\n        # dispersions are above 100 * min_disp. This is to reproduce DESeq2's behaviour.\n        above_min_disp = genewise_dispersions[non_zero] &gt;= (100 * self.min_disp)\n\n        _squared_logres = mean_absolute_deviation(disp_residuals[above_min_disp]) ** 2\n\n        prior_disp_var = np.maximum(\n            _squared_logres - polygamma(1, (n_total_obs - n_params) / 2),\n            0.25,\n        )\n\n        return {\n            \"prior_disp_var\": prior_disp_var,\n            \"_squared_logres\": _squared_logres,\n            \"trend_coeffs\": coeffs,\n            \"fitted_dispersions\": fitted_dispersions,\n            \"disp_function_type\": disp_function_type,\n            \"mean_disp\": mean_disp,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.AggFitDispersionTrendAndPrior.agg_fit_dispersion_trend_and_prior_dispersion","title":"<code>agg_fit_dispersion_trend_and_prior_dispersion(shared_states)</code>","text":"<p>Fit the dispersion trend, and compute the dispersion prior.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>dict</code> <p>Shared states from the local step with the following keys: - genewise_dispersions: np.ndarray of shape (n_genes,) - n_params: int - non_zero: np.ndarray of shape (n_genes,) - mean_normed_counts: np.ndarray of shape (n_genes,) - n_obs: int</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict with the following keys: - prior_disp_var: float     The prior dispersion variance. - _squared_logres: float     The squared log-residuals. - trend_coeffs: np.ndarray of shape (2,)     The coefficients of the parametric dispersion trend. - fitted_dispersions: np.ndarray of shape (n_genes,)     The fitted dispersions, computed from the dispersion trend. - disp_function_type: str     The type of dispersion function (parametric or mean). - mean_disp: float, optional     The mean dispersion (if \"mean\" fit type).</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_fit_dispersion_trend_and_prior_dispersion(self, shared_states):\n    \"\"\"Fit the dispersion trend, and compute the dispersion prior.\n\n    Parameters\n    ----------\n    shared_states : dict\n        Shared states from the local step with the following keys:\n        - genewise_dispersions: np.ndarray of shape (n_genes,)\n        - n_params: int\n        - non_zero: np.ndarray of shape (n_genes,)\n        - mean_normed_counts: np.ndarray of shape (n_genes,)\n        - n_obs: int\n\n    Returns\n    -------\n    dict\n        dict with the following keys:\n        - prior_disp_var: float\n            The prior dispersion variance.\n        - _squared_logres: float\n            The squared log-residuals.\n        - trend_coeffs: np.ndarray of shape (2,)\n            The coefficients of the parametric dispersion trend.\n        - fitted_dispersions: np.ndarray of shape (n_genes,)\n            The fitted dispersions, computed from the dispersion trend.\n        - disp_function_type: str\n            The type of dispersion function (parametric or mean).\n        - mean_disp: float, optional\n            The mean dispersion (if \"mean\" fit type).\n    \"\"\"\n    genewise_dispersions = shared_states[0][\"genewise_dispersions\"]\n    n_params = shared_states[0][\"n_params\"]\n    non_zero = shared_states[0][\"non_zero\"]\n    n_total_obs = sum([state[\"n_obs\"] for state in shared_states])\n    mean_normed_counts = (\n        sum(\n            [\n                state[\"mean_normed_counts\"] * state[\"n_obs\"]\n                for state in shared_states\n            ]\n        )\n        / n_total_obs\n    )\n\n    # Exclude all-zero counts\n    targets = pd.Series(\n        genewise_dispersions.copy(),\n    )\n    targets = targets[non_zero]\n    covariates = pd.Series(1 / mean_normed_counts[non_zero], index=targets.index)\n\n    for gene in targets.index:\n        if (\n            np.isinf(covariates.loc[gene]).any()\n            or np.isnan(covariates.loc[gene]).any()\n        ):\n            targets.drop(labels=[gene], inplace=True)\n            covariates.drop(labels=[gene], inplace=True)\n\n    # Initialize coefficients\n    old_coeffs = pd.Series([0.1, 0.1])\n    coeffs = pd.Series([1.0, 1.0])\n    mean_disp = None\n\n    disp_function_type = \"parametric\"\n    while (coeffs &gt; 1e-10).all() and (\n        np.log(np.abs(coeffs / old_coeffs)) ** 2\n    ).sum() &gt;= 1e-6:\n        old_coeffs = coeffs\n        (\n            coeffs,\n            predictions,\n            converged,\n        ) = DefaultInference().dispersion_trend_gamma_glm(covariates, targets)\n\n        if not converged or (coeffs &lt;= 1e-10).any():\n            warnings.warn(\n                \"The dispersion trend curve fitting did not converge. \"\n                \"Switching to a mean-based dispersion trend.\",\n                UserWarning,\n                stacklevel=2,\n            )\n            mean_disp = trim_mean(\n                genewise_dispersions[genewise_dispersions &gt; 10 * self.min_disp],\n                proportiontocut=0.001,\n            )\n            disp_function_type = \"mean\"\n\n        pred_ratios = genewise_dispersions[covariates.index] / predictions\n\n        targets.drop(\n            targets[(pred_ratios &lt; 1e-4) | (pred_ratios &gt;= 15)].index,\n            inplace=True,\n        )\n        covariates.drop(\n            covariates[(pred_ratios &lt; 1e-4) | (pred_ratios &gt;= 15)].index,\n            inplace=True,\n        )\n\n    fitted_dispersions = np.full_like(genewise_dispersions, np.NaN)\n\n    fitted_dispersions[non_zero] = disp_function(\n        mean_normed_counts[non_zero],\n        disp_function_type=disp_function_type,\n        coeffs=coeffs,\n        mean_disp=mean_disp,\n    )\n\n    disp_residuals = np.log(genewise_dispersions[non_zero]) - np.log(\n        fitted_dispersions[non_zero]\n    )\n\n    # Compute squared log-residuals and prior variance based on genes whose\n    # dispersions are above 100 * min_disp. This is to reproduce DESeq2's behaviour.\n    above_min_disp = genewise_dispersions[non_zero] &gt;= (100 * self.min_disp)\n\n    _squared_logres = mean_absolute_deviation(disp_residuals[above_min_disp]) ** 2\n\n    prior_disp_var = np.maximum(\n        _squared_logres - polygamma(1, (n_total_obs - n_params) / 2),\n        0.25,\n    )\n\n    return {\n        \"prior_disp_var\": prior_disp_var,\n        \"_squared_logres\": _squared_logres,\n        \"trend_coeffs\": coeffs,\n        \"fitted_dispersions\": fitted_dispersions,\n        \"disp_function_type\": disp_function_type,\n        \"mean_disp\": mean_disp,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.LocGetMeanDispersionAndMean","title":"<code>LocGetMeanDispersionAndMean</code>","text":"<p>Mixin to get the local mean and dispersion.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>class LocGetMeanDispersionAndMean:\n    \"\"\"Mixin to get the local mean and dispersion.\"\"\"\n\n    local_adata: ad.AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_local_mean_and_dispersion(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        # pylint: disable=unused-argument\n        \"\"\"Return local gene means and dispersion.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state returned by the last step of gene-wise dispersion computation.\n            Contains a \"genewise_dispersions\" key with the gene-wise dispersions.\n\n        Returns\n        -------\n        dict\n            Local results to be shared via shared_state to the aggregation node. dict\n            with the following keys:\n            - mean_normed_counts: np.ndarray[float] of shape (n_genes,)\n                The mean normed counts.\n            - n_obs: int\n                The number of observations.\n            - non_zero: np.ndarray[bool] of shape (n_genes,)\n                Mask of the genes with non zero counts.\n            - genewise_dispersions: np.ndarray[float] of shape (n_genes,)\n                The genewise dispersions.\n            - num_vars: int\n                The number of variables.\n        \"\"\"\n        # Save gene-wise dispersions from the previous step.\n        # Dispersions of all-zero genes should already be NaN.\n        self.local_adata.varm[\"genewise_dispersions\"] = shared_state[\n            \"genewise_dispersions\"\n        ]\n\n        # TODO: these could be gathered earlier and sent directly to the aggregation\n        # node.\n        return {\n            \"mean_normed_counts\": self.local_adata.layers[\"normed_counts\"].mean(0),\n            \"n_obs\": self.local_adata.n_obs,\n            \"non_zero\": self.local_adata.varm[\"non_zero\"],\n            \"genewise_dispersions\": self.local_adata.varm[\"genewise_dispersions\"],\n            \"n_params\": self.local_adata.uns[\"n_params\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.LocGetMeanDispersionAndMean.get_local_mean_and_dispersion","title":"<code>get_local_mean_and_dispersion(data_from_opener, shared_state)</code>","text":"<p>Return local gene means and dispersion.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state returned by the last step of gene-wise dispersion computation. Contains a \"genewise_dispersions\" key with the gene-wise dispersions.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Local results to be shared via shared_state to the aggregation node. dict with the following keys: - mean_normed_counts: np.ndarray[float] of shape (n_genes,)     The mean normed counts. - n_obs: int     The number of observations. - non_zero: np.ndarray[bool] of shape (n_genes,)     Mask of the genes with non zero counts. - genewise_dispersions: np.ndarray[float] of shape (n_genes,)     The genewise dispersions. - num_vars: int     The number of variables.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_local_mean_and_dispersion(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    # pylint: disable=unused-argument\n    \"\"\"Return local gene means and dispersion.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state returned by the last step of gene-wise dispersion computation.\n        Contains a \"genewise_dispersions\" key with the gene-wise dispersions.\n\n    Returns\n    -------\n    dict\n        Local results to be shared via shared_state to the aggregation node. dict\n        with the following keys:\n        - mean_normed_counts: np.ndarray[float] of shape (n_genes,)\n            The mean normed counts.\n        - n_obs: int\n            The number of observations.\n        - non_zero: np.ndarray[bool] of shape (n_genes,)\n            Mask of the genes with non zero counts.\n        - genewise_dispersions: np.ndarray[float] of shape (n_genes,)\n            The genewise dispersions.\n        - num_vars: int\n            The number of variables.\n    \"\"\"\n    # Save gene-wise dispersions from the previous step.\n    # Dispersions of all-zero genes should already be NaN.\n    self.local_adata.varm[\"genewise_dispersions\"] = shared_state[\n        \"genewise_dispersions\"\n    ]\n\n    # TODO: these could be gathered earlier and sent directly to the aggregation\n    # node.\n    return {\n        \"mean_normed_counts\": self.local_adata.layers[\"normed_counts\"].mean(0),\n        \"n_obs\": self.local_adata.n_obs,\n        \"non_zero\": self.local_adata.varm[\"non_zero\"],\n        \"genewise_dispersions\": self.local_adata.varm[\"genewise_dispersions\"],\n        \"n_params\": self.local_adata.uns[\"n_params\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.LocUpdateFittedDispersions","title":"<code>LocUpdateFittedDispersions</code>","text":"<p>Mixin to update the fitted dispersions after replacing outliers.</p> <p>To use in refit mode only</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>class LocUpdateFittedDispersions:\n    \"\"\"Mixin to update the fitted dispersions after replacing outliers.\n\n    To use in refit mode only\n    \"\"\"\n\n    local_adata: ad.AnnData\n    refit_adata: ad.AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_update_fitted_dispersions(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; None:\n        \"\"\"Update the fitted dispersions after replacing outliers.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            A dictionary with a \"fitted_dispersions\" key, containing the dispersions\n            fitted before replacing the outliers.\n        \"\"\"\n        # Start by updating gene-wise dispersions\n        self.refit_adata.varm[\"genewise_dispersions\"] = shared_state[\n            \"genewise_dispersions\"\n        ]\n\n        # Update the fitted dispersions\n        non_zero = self.refit_adata.varm[\"non_zero\"]\n        self.refit_adata.uns[\"disp_function_type\"] = self.local_adata.uns[\n            \"disp_function_type\"\n        ]\n\n        fitted_dispersions = np.full_like(\n            self.refit_adata.varm[\"genewise_dispersions\"], np.NaN\n        )\n\n        fitted_dispersions[non_zero] = disp_function(\n            self.refit_adata.varm[\"_normed_means\"][non_zero],\n            disp_function_type=self.refit_adata.uns[\"disp_function_type\"],\n            coeffs=self.refit_adata.uns[\"trend_coeffs\"],\n            mean_disp=(\n                self.refit_adata.uns[\"mean_disp\"]\n                if self.refit_adata.uns[\"disp_function_type\"] == \"parametric\"\n                else None\n            ),\n        )\n\n        self.refit_adata.varm[\"fitted_dispersions\"] = fitted_dispersions\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.substeps.LocUpdateFittedDispersions.loc_update_fitted_dispersions","title":"<code>loc_update_fitted_dispersions(data_from_opener, shared_state)</code>","text":"<p>Update the fitted dispersions after replacing outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>A dictionary with a \"fitted_dispersions\" key, containing the dispersions fitted before replacing the outliers.</p> required Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_update_fitted_dispersions(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; None:\n    \"\"\"Update the fitted dispersions after replacing outliers.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        A dictionary with a \"fitted_dispersions\" key, containing the dispersions\n        fitted before replacing the outliers.\n    \"\"\"\n    # Start by updating gene-wise dispersions\n    self.refit_adata.varm[\"genewise_dispersions\"] = shared_state[\n        \"genewise_dispersions\"\n    ]\n\n    # Update the fitted dispersions\n    non_zero = self.refit_adata.varm[\"non_zero\"]\n    self.refit_adata.uns[\"disp_function_type\"] = self.local_adata.uns[\n        \"disp_function_type\"\n    ]\n\n    fitted_dispersions = np.full_like(\n        self.refit_adata.varm[\"genewise_dispersions\"], np.NaN\n    )\n\n    fitted_dispersions[non_zero] = disp_function(\n        self.refit_adata.varm[\"_normed_means\"][non_zero],\n        disp_function_type=self.refit_adata.uns[\"disp_function_type\"],\n        coeffs=self.refit_adata.uns[\"trend_coeffs\"],\n        mean_disp=(\n            self.refit_adata.uns[\"mean_disp\"]\n            if self.refit_adata.uns[\"disp_function_type\"] == \"parametric\"\n            else None\n        ),\n    )\n\n    self.refit_adata.varm[\"fitted_dispersions\"] = fitted_dispersions\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.utils","title":"<code>utils</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_dispersion_prior.utils.disp_function","title":"<code>disp_function(x, disp_function_type, coeffs=None, mean_disp=None)</code>","text":"<p>Return the dispersion trend function at x.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_dispersion_prior/utils.py</code> <pre><code>def disp_function(\n    x,\n    disp_function_type,\n    coeffs: Union[\"pd.Series[float]\", np.ndarray] | None = None,\n    mean_disp: float | None = None,\n) -&gt; float | np.ndarray:\n    \"\"\"Return the dispersion trend function at x.\"\"\"\n    if disp_function_type == \"parametric\":\n        assert coeffs is not None, \"coeffs must be provided for parametric dispersion.\"\n        return dispersion_trend(x, coeffs=coeffs)\n    elif disp_function_type == \"mean\":\n        assert mean_disp is not None, \"mean_disp must be provided for mean dispersion.\"\n        return np.full_like(x, mean_disp)\n    else:\n        raise ValueError(\n            \"disp_function_type must be 'parametric' or 'mean',\"\n            f\" got {disp_function_type}\"\n        )\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions","title":"<code>compute_genewise_dispersions</code>","text":"<p>Module containing the mixin class to compute genewise dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions","title":"<code>compute_MoM_dispersions</code>","text":"<p>Module to implement the computation of MoM dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_MoM_dispersions","title":"<code>compute_MoM_dispersions</code>","text":"<p>Main module to compute method of moments (MoM) dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_MoM_dispersions.ComputeMoMDispersions","title":"<code>ComputeMoMDispersions</code>","text":"<p>               Bases: <code>ComputeRoughDispersions</code>, <code>LocInvSizeMean</code>, <code>AggMomentsDispersion</code></p> <p>Mixin class to implement the computation of MoM dispersions.</p> <p>Relies on the ComputeRoughDispersions class, in addition to substeps.</p> <p>Methods:</p> Name Description <code>compute_MoM_dispersions</code> <p>The method to compute the MoM dispersions, that must be used in the main pipeline.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/compute_MoM_dispersions.py</code> <pre><code>class ComputeMoMDispersions(\n    ComputeRoughDispersions,\n    LocInvSizeMean,\n    AggMomentsDispersion,\n):\n    \"\"\"Mixin class to implement the computation of MoM dispersions.\n\n    Relies on the ComputeRoughDispersions class, in addition to substeps.\n\n    Methods\n    -------\n    compute_MoM_dispersions\n        The method to compute the MoM dispersions, that must be used in the main\n        pipeline.\n    \"\"\"\n\n    @log_organisation_method\n    def compute_MoM_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n        clean_models,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Compute method of moments dispersions.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        gram_features_shared_states: list\n            The list of shared states outputed by the compute_size_factors step.\n            They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        refit_mode: bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        mom_dispersions_shared_state: dict\n            Shared states containing MoM dispersions.\n\n        round_idx: int\n            The updated round number.\n        \"\"\"\n        ###### Fit rough dispersions ######\n\n        local_states, shared_states, round_idx = self.compute_rough_dispersions(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            gram_features_shared_states=gram_features_shared_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=refit_mode,\n        )\n\n        ###### Compute moments dispersions ######\n\n        # ---- Compute local means for moments dispersions---- #\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_inverse_size_mean,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=shared_states,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local inverse size factor means.\",\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # ---- Compute moments dispersions and merge to get MoM dispersions ---- #\n\n        mom_dispersions_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.aggregate_moments_dispersions,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Compute global MoM dispersions\",\n            clean_models=clean_models,\n        )\n\n        return local_states, mom_dispersions_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_MoM_dispersions.ComputeMoMDispersions.compute_MoM_dispersions","title":"<code>compute_MoM_dispersions(train_data_nodes, aggregation_node, local_states, gram_features_shared_states, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Compute method of moments dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>gram_features_shared_states</code> <p>The list of shared states outputed by the compute_size_factors step. They contain a \"local_gram_matrix\" and a \"local_features\" fields.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>mom_dispersions_shared_state</code> <code>dict</code> <p>Shared states containing MoM dispersions.</p> <code>round_idx</code> <code>int</code> <p>The updated round number.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/compute_MoM_dispersions.py</code> <pre><code>@log_organisation_method\ndef compute_MoM_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    gram_features_shared_states,\n    round_idx,\n    clean_models,\n    refit_mode: bool = False,\n):\n    \"\"\"Compute method of moments dispersions.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    gram_features_shared_states: list\n        The list of shared states outputed by the compute_size_factors step.\n        They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    refit_mode: bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    mom_dispersions_shared_state: dict\n        Shared states containing MoM dispersions.\n\n    round_idx: int\n        The updated round number.\n    \"\"\"\n    ###### Fit rough dispersions ######\n\n    local_states, shared_states, round_idx = self.compute_rough_dispersions(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states=gram_features_shared_states,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        refit_mode=refit_mode,\n    )\n\n    ###### Compute moments dispersions ######\n\n    # ---- Compute local means for moments dispersions---- #\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_inverse_size_mean,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=shared_states,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Compute local inverse size factor means.\",\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    # ---- Compute moments dispersions and merge to get MoM dispersions ---- #\n\n    mom_dispersions_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.aggregate_moments_dispersions,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Compute global MoM dispersions\",\n        clean_models=clean_models,\n    )\n\n    return local_states, mom_dispersions_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_rough_dispersions","title":"<code>compute_rough_dispersions</code>","text":"<p>Module to compute rough dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_rough_dispersions.ComputeRoughDispersions","title":"<code>ComputeRoughDispersions</code>","text":"<p>               Bases: <code>AggRoughDispersion</code>, <code>LocRoughDispersion</code>, <code>AggCreateRoughDispersionsSystem</code></p> <p>Mixin class to implement the computation of rough dispersions.</p> <p>Methods:</p> Name Description <code>compute_rough_dispersions</code> <p>The method to compute the rough dispersions, that must be used in the main pipeline.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/compute_rough_dispersions.py</code> <pre><code>class ComputeRoughDispersions(\n    AggRoughDispersion,\n    LocRoughDispersion,\n    AggCreateRoughDispersionsSystem,\n):\n    \"\"\"Mixin class to implement the computation of rough dispersions.\n\n    Methods\n    -------\n    compute_rough_dispersions\n        The method to compute the rough dispersions, that must be used in the main\n        pipeline.\n    \"\"\"\n\n    @log_organisation_method\n    def compute_rough_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n        clean_models,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Compute rough dispersions.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        gram_features_shared_states: list\n            The list of shared states outputed by the compute_size_factors step.\n            They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        refit_mode: bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        rough_dispersion_shared_state: dict\n            Shared states containing rough dispersions.\n\n        round_idx: int\n            The updated round number.\n        \"\"\"\n        # ---- Solve global linear system ---- #\n\n        rough_dispersion_system_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.create_rough_dispersions_system,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=gram_features_shared_states,\n            round_idx=round_idx,\n            description=\"Solving system for rough dispersions\",\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # ---- Compute local rough dispersions---- #\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_rough_dispersions,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=rough_dispersion_system_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Computing local rough dispersions\",\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # ---- Compute global rough dispersions---- #\n\n        rough_dispersion_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.aggregate_rough_dispersions,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Compute global rough dispersions\",\n            clean_models=clean_models,\n        )\n\n        return local_states, rough_dispersion_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.compute_rough_dispersions.ComputeRoughDispersions.compute_rough_dispersions","title":"<code>compute_rough_dispersions(train_data_nodes, aggregation_node, local_states, gram_features_shared_states, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Compute rough dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>gram_features_shared_states</code> <p>The list of shared states outputed by the compute_size_factors step. They contain a \"local_gram_matrix\" and a \"local_features\" fields.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>rough_dispersion_shared_state</code> <code>dict</code> <p>Shared states containing rough dispersions.</p> <code>round_idx</code> <code>int</code> <p>The updated round number.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/compute_rough_dispersions.py</code> <pre><code>@log_organisation_method\ndef compute_rough_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    gram_features_shared_states,\n    round_idx,\n    clean_models,\n    refit_mode: bool = False,\n):\n    \"\"\"Compute rough dispersions.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    gram_features_shared_states: list\n        The list of shared states outputed by the compute_size_factors step.\n        They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    refit_mode: bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    rough_dispersion_shared_state: dict\n        Shared states containing rough dispersions.\n\n    round_idx: int\n        The updated round number.\n    \"\"\"\n    # ---- Solve global linear system ---- #\n\n    rough_dispersion_system_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.create_rough_dispersions_system,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=gram_features_shared_states,\n        round_idx=round_idx,\n        description=\"Solving system for rough dispersions\",\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    # ---- Compute local rough dispersions---- #\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_rough_dispersions,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=rough_dispersion_system_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Computing local rough dispersions\",\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    # ---- Compute global rough dispersions---- #\n\n    rough_dispersion_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.aggregate_rough_dispersions,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Compute global rough dispersions\",\n        clean_models=clean_models,\n    )\n\n    return local_states, rough_dispersion_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps","title":"<code>substeps</code>","text":"<p>Module to implement the substeps for the rough dispersions step.</p> <p>This module contains all these substeps as mixin classes.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggCreateRoughDispersionsSystem","title":"<code>AggCreateRoughDispersionsSystem</code>","text":"<p>Mixin to solve the linear system for rough dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>class AggCreateRoughDispersionsSystem:\n    \"\"\"Mixin to solve the linear system for rough dispersions.\"\"\"\n\n    @remote\n    @log_remote\n    def create_rough_dispersions_system(self, shared_states, refit_mode: bool = False):\n        \"\"\"Solve the linear system in for rough dispersions.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (local_gram_matrix, local_features) from training nodes.\n\n        refit_mode : bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, there is no need to compute the Gram matrix which was\n            already computed in the compute_size_factors step (default: False).\n\n        Returns\n        -------\n        dict\n            The global feature vector and the global hat matrix if refit_mode is\n            ``False``.\n        \"\"\"\n        shared_state = {\n            \"global_feature_vector\": sum(\n                [state[\"local_features\"] for state in shared_states]\n            )\n        }\n        if not refit_mode:\n            shared_state[\"global_gram_matrix\"] = sum(\n                [state[\"local_gram_matrix\"] for state in shared_states]\n            )\n\n        return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggCreateRoughDispersionsSystem.create_rough_dispersions_system","title":"<code>create_rough_dispersions_system(shared_states, refit_mode=False)</code>","text":"<p>Solve the linear system in for rough dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (local_gram_matrix, local_features) from training nodes.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, there is no need to compute the Gram matrix which was already computed in the compute_size_factors step (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The global feature vector and the global hat matrix if refit_mode is <code>False</code>.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>@remote\n@log_remote\ndef create_rough_dispersions_system(self, shared_states, refit_mode: bool = False):\n    \"\"\"Solve the linear system in for rough dispersions.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (local_gram_matrix, local_features) from training nodes.\n\n    refit_mode : bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, there is no need to compute the Gram matrix which was\n        already computed in the compute_size_factors step (default: False).\n\n    Returns\n    -------\n    dict\n        The global feature vector and the global hat matrix if refit_mode is\n        ``False``.\n    \"\"\"\n    shared_state = {\n        \"global_feature_vector\": sum(\n            [state[\"local_features\"] for state in shared_states]\n        )\n    }\n    if not refit_mode:\n        shared_state[\"global_gram_matrix\"] = sum(\n            [state[\"local_gram_matrix\"] for state in shared_states]\n        )\n\n    return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggMomentsDispersion","title":"<code>AggMomentsDispersion</code>","text":"<p>Mixin to compute MoM dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>class AggMomentsDispersion:\n    \"\"\"Mixin to compute MoM dispersions.\"\"\"\n\n    local_adata: AnnData\n    max_disp: float\n    min_disp: float\n\n    @remote\n    @log_remote\n    def aggregate_moments_dispersions(self, shared_states):\n        \"\"\"Compute global moments dispersions.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (local_inverse_size_mean, local_counts_mean,\n            local_squared_squared_mean, local_n_obs, rough_dispersions)\n            from training nodes.\n\n        Returns\n        -------\n        dict\n            Global moments dispersions, the mask of all zero genes, the total\n            number of samples (used to set max_disp and lr), and\n            the total normed counts mean (used in the independent filtering\n            step).\n        \"\"\"\n        tot_n_obs = sum([state[\"local_n_obs\"] for state in shared_states])\n\n        # Compute the mean of inverse size factors\n        tot_inv_size_mean = (\n            sum(\n                [\n                    state[\"local_n_obs\"] * state[\"local_inverse_size_mean\"]\n                    for state in shared_states\n                ]\n            )\n            / tot_n_obs\n        )\n\n        # Compute the mean and variance of normalized counts\n\n        tot_counts_mean = (\n            sum(\n                [\n                    state[\"local_n_obs\"] * state[\"local_counts_mean\"]\n                    for state in shared_states\n                ]\n            )\n            / tot_n_obs\n        )\n        non_zero = tot_counts_mean != 0\n\n        tot_squared_mean = (\n            sum(\n                [\n                    state[\"local_n_obs\"] * state[\"local_squared_squared_mean\"]\n                    for state in shared_states\n                ]\n            )\n            / tot_n_obs\n        )\n\n        counts_variance = (\n            tot_n_obs / (tot_n_obs - 1) * (tot_squared_mean - tot_counts_mean**2)\n        )\n\n        moments_dispersions = np.zeros(\n            counts_variance.shape, dtype=counts_variance.dtype\n        )\n        moments_dispersions[non_zero] = (\n            counts_variance[non_zero] - tot_inv_size_mean * tot_counts_mean[non_zero]\n        ) / tot_counts_mean[non_zero] ** 2\n\n        # Get rough dispersions from the first center\n        rough_dispersions = shared_states[0][\"rough_dispersions\"]\n\n        # Compute the maximum dispersion\n        max_disp = np.maximum(self.max_disp, tot_n_obs)\n\n        # Return moment estimate\n        alpha_hat = np.minimum(rough_dispersions, moments_dispersions)\n        MoM_dispersions = np.clip(alpha_hat, self.min_disp, max_disp)\n\n        # Set MoM dispersions of all zero genes to NaN\n\n        MoM_dispersions[~non_zero] = np.nan\n        return {\n            \"MoM_dispersions\": MoM_dispersions,\n            \"non_zero\": non_zero,\n            \"tot_num_samples\": tot_n_obs,\n            \"tot_counts_mean\": tot_counts_mean,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggMomentsDispersion.aggregate_moments_dispersions","title":"<code>aggregate_moments_dispersions(shared_states)</code>","text":"<p>Compute global moments dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (local_inverse_size_mean, local_counts_mean, local_squared_squared_mean, local_n_obs, rough_dispersions) from training nodes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Global moments dispersions, the mask of all zero genes, the total number of samples (used to set max_disp and lr), and the total normed counts mean (used in the independent filtering step).</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>@remote\n@log_remote\ndef aggregate_moments_dispersions(self, shared_states):\n    \"\"\"Compute global moments dispersions.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (local_inverse_size_mean, local_counts_mean,\n        local_squared_squared_mean, local_n_obs, rough_dispersions)\n        from training nodes.\n\n    Returns\n    -------\n    dict\n        Global moments dispersions, the mask of all zero genes, the total\n        number of samples (used to set max_disp and lr), and\n        the total normed counts mean (used in the independent filtering\n        step).\n    \"\"\"\n    tot_n_obs = sum([state[\"local_n_obs\"] for state in shared_states])\n\n    # Compute the mean of inverse size factors\n    tot_inv_size_mean = (\n        sum(\n            [\n                state[\"local_n_obs\"] * state[\"local_inverse_size_mean\"]\n                for state in shared_states\n            ]\n        )\n        / tot_n_obs\n    )\n\n    # Compute the mean and variance of normalized counts\n\n    tot_counts_mean = (\n        sum(\n            [\n                state[\"local_n_obs\"] * state[\"local_counts_mean\"]\n                for state in shared_states\n            ]\n        )\n        / tot_n_obs\n    )\n    non_zero = tot_counts_mean != 0\n\n    tot_squared_mean = (\n        sum(\n            [\n                state[\"local_n_obs\"] * state[\"local_squared_squared_mean\"]\n                for state in shared_states\n            ]\n        )\n        / tot_n_obs\n    )\n\n    counts_variance = (\n        tot_n_obs / (tot_n_obs - 1) * (tot_squared_mean - tot_counts_mean**2)\n    )\n\n    moments_dispersions = np.zeros(\n        counts_variance.shape, dtype=counts_variance.dtype\n    )\n    moments_dispersions[non_zero] = (\n        counts_variance[non_zero] - tot_inv_size_mean * tot_counts_mean[non_zero]\n    ) / tot_counts_mean[non_zero] ** 2\n\n    # Get rough dispersions from the first center\n    rough_dispersions = shared_states[0][\"rough_dispersions\"]\n\n    # Compute the maximum dispersion\n    max_disp = np.maximum(self.max_disp, tot_n_obs)\n\n    # Return moment estimate\n    alpha_hat = np.minimum(rough_dispersions, moments_dispersions)\n    MoM_dispersions = np.clip(alpha_hat, self.min_disp, max_disp)\n\n    # Set MoM dispersions of all zero genes to NaN\n\n    MoM_dispersions[~non_zero] = np.nan\n    return {\n        \"MoM_dispersions\": MoM_dispersions,\n        \"non_zero\": non_zero,\n        \"tot_num_samples\": tot_n_obs,\n        \"tot_counts_mean\": tot_counts_mean,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggRoughDispersion","title":"<code>AggRoughDispersion</code>","text":"<p>Mixin to aggregate local rough dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>class AggRoughDispersion:\n    \"\"\"Mixin to aggregate local rough dispersions.\"\"\"\n\n    @remote\n    @log_remote\n    def aggregate_rough_dispersions(self, shared_states):\n        \"\"\"Aggregate local rough dispersions.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (rough_dispersions, n_obs, n_params) from training nodes.\n\n        Returns\n        -------\n        dict\n            Global rough dispersions.\n        \"\"\"\n        rough_dispersions = sum(\n            [state[\"local_rough_dispersions\"] for state in shared_states]\n        )\n\n        tot_obs = sum([state[\"local_n_obs\"] for state in shared_states])\n        n_params = shared_states[0][\"local_n_params\"]\n\n        if tot_obs &lt;= n_params:\n            raise ValueError(\n                \"The number of samples is smaller or equal to the number of design \"\n                \"variables, i.e., there are no replicates to estimate the \"\n                \"dispersions. Please use a design with fewer variables.\"\n            )\n\n        return {\n            \"rough_dispersions\": np.maximum(rough_dispersions / (tot_obs - n_params), 0)\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.AggRoughDispersion.aggregate_rough_dispersions","title":"<code>aggregate_rough_dispersions(shared_states)</code>","text":"<p>Aggregate local rough dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (rough_dispersions, n_obs, n_params) from training nodes.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Global rough dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>@remote\n@log_remote\ndef aggregate_rough_dispersions(self, shared_states):\n    \"\"\"Aggregate local rough dispersions.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (rough_dispersions, n_obs, n_params) from training nodes.\n\n    Returns\n    -------\n    dict\n        Global rough dispersions.\n    \"\"\"\n    rough_dispersions = sum(\n        [state[\"local_rough_dispersions\"] for state in shared_states]\n    )\n\n    tot_obs = sum([state[\"local_n_obs\"] for state in shared_states])\n    n_params = shared_states[0][\"local_n_params\"]\n\n    if tot_obs &lt;= n_params:\n        raise ValueError(\n            \"The number of samples is smaller or equal to the number of design \"\n            \"variables, i.e., there are no replicates to estimate the \"\n            \"dispersions. Please use a design with fewer variables.\"\n        )\n\n    return {\n        \"rough_dispersions\": np.maximum(rough_dispersions / (tot_obs - n_params), 0)\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.LocInvSizeMean","title":"<code>LocInvSizeMean</code>","text":"<p>Mixin to compute local means of inverse size factors.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>class LocInvSizeMean:\n    \"\"\"Mixin to compute local means of inverse size factors.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_inverse_size_mean(\n        self, data_from_opener, shared_state=None, refit_mode: bool = False\n    ) -&gt; dict:\n        \"\"\"Compute local means of inverse size factors, counts, and squared counts.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state containing rough dispersions from aggregator.\n\n        refit_mode : bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s (default: False).\n\n        Returns\n        -------\n        dict\n            dictionary containing all quantities required to compute MoM dispersions:\n            local inverse size factor means, counts means, squared counts means,\n            rough dispersions and number of samples.\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        adata.varm[\"_rough_dispersions\"] = shared_state[\"rough_dispersions\"]\n\n        return {\n            \"local_inverse_size_mean\": (1 / adata.obsm[\"size_factors\"]).mean(),\n            \"local_counts_mean\": adata.layers[\"normed_counts\"].mean(0),\n            \"local_squared_squared_mean\": (adata.layers[\"normed_counts\"] ** 2).mean(0),\n            \"local_n_obs\": adata.n_obs,\n            # Pass rough dispersions to the aggregation node, to compute MoM dispersions\n            \"rough_dispersions\": shared_state[\"rough_dispersions\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.LocInvSizeMean.local_inverse_size_mean","title":"<code>local_inverse_size_mean(data_from_opener, shared_state=None, refit_mode=False)</code>","text":"<p>Compute local means of inverse size factors, counts, and squared counts.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing rough dispersions from aggregator.</p> <code>None</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing all quantities required to compute MoM dispersions: local inverse size factor means, counts means, squared counts means, rough dispersions and number of samples.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_inverse_size_mean(\n    self, data_from_opener, shared_state=None, refit_mode: bool = False\n) -&gt; dict:\n    \"\"\"Compute local means of inverse size factors, counts, and squared counts.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state containing rough dispersions from aggregator.\n\n    refit_mode : bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s (default: False).\n\n    Returns\n    -------\n    dict\n        dictionary containing all quantities required to compute MoM dispersions:\n        local inverse size factor means, counts means, squared counts means,\n        rough dispersions and number of samples.\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    adata.varm[\"_rough_dispersions\"] = shared_state[\"rough_dispersions\"]\n\n    return {\n        \"local_inverse_size_mean\": (1 / adata.obsm[\"size_factors\"]).mean(),\n        \"local_counts_mean\": adata.layers[\"normed_counts\"].mean(0),\n        \"local_squared_squared_mean\": (adata.layers[\"normed_counts\"] ** 2).mean(0),\n        \"local_n_obs\": adata.n_obs,\n        # Pass rough dispersions to the aggregation node, to compute MoM dispersions\n        \"rough_dispersions\": shared_state[\"rough_dispersions\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.LocRoughDispersion","title":"<code>LocRoughDispersion</code>","text":"<p>Mixin to compute local rough dispersions.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>class LocRoughDispersion:\n    \"\"\"Mixin to compute local rough dispersions.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_rough_dispersions(\n        self, data_from_opener, shared_state, refit_mode: bool = False\n    ) -&gt; dict:\n        \"\"\"Compute local rough dispersions, and save the global gram matrix.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state containing\n                - the gram matrix, if refit_mode is ``False``,\n                - the global feature vector.\n\n        refit_mode : bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s (default: False).\n\n        Returns\n        -------\n        dict\n            Dictionary containing local rough dispersions, number of samples and\n            number of parameters (i.e. number of columns in the design matrix).\n        \"\"\"\n        if not refit_mode:\n            global_gram_matrix = shared_state[\"global_gram_matrix\"]\n            self.local_adata.uns[\"_global_gram_matrix\"] = global_gram_matrix\n        else:\n            global_gram_matrix = self.local_adata.uns[\"_global_gram_matrix\"]\n\n        beta_rough_dispersions = np.linalg.solve(\n            global_gram_matrix, shared_state[\"global_feature_vector\"]\n        )\n\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n        adata.varm[\"_beta_rough_dispersions\"] = beta_rough_dispersions.T\n        # Save the rough dispersions beta so that we can reconstruct y_hat\n        set_y_hat(adata)\n\n        # Save global beta in the local data because so it can be used later in\n        # fit_lin_mu. Do it before clipping.\n\n        y_hat = np.maximum(adata.layers[\"_y_hat\"], 1)\n        unnormed_alpha_rde = (\n            ((adata.layers[\"normed_counts\"] - y_hat) ** 2 - y_hat) / (y_hat**2)\n        ).sum(0)\n        return {\n            \"local_rough_dispersions\": unnormed_alpha_rde,\n            \"local_n_obs\": adata.n_obs,\n            \"local_n_params\": adata.uns[\"n_params\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_MoM_dispersions.substeps.LocRoughDispersion.local_rough_dispersions","title":"<code>local_rough_dispersions(data_from_opener, shared_state, refit_mode=False)</code>","text":"<p>Compute local rough dispersions, and save the global gram matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing     - the gram matrix, if refit_mode is <code>False</code>,     - the global feature vector.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing local rough dispersions, number of samples and number of parameters (i.e. number of columns in the design matrix).</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_MoM_dispersions/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_rough_dispersions(\n    self, data_from_opener, shared_state, refit_mode: bool = False\n) -&gt; dict:\n    \"\"\"Compute local rough dispersions, and save the global gram matrix.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state containing\n            - the gram matrix, if refit_mode is ``False``,\n            - the global feature vector.\n\n    refit_mode : bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s (default: False).\n\n    Returns\n    -------\n    dict\n        Dictionary containing local rough dispersions, number of samples and\n        number of parameters (i.e. number of columns in the design matrix).\n    \"\"\"\n    if not refit_mode:\n        global_gram_matrix = shared_state[\"global_gram_matrix\"]\n        self.local_adata.uns[\"_global_gram_matrix\"] = global_gram_matrix\n    else:\n        global_gram_matrix = self.local_adata.uns[\"_global_gram_matrix\"]\n\n    beta_rough_dispersions = np.linalg.solve(\n        global_gram_matrix, shared_state[\"global_feature_vector\"]\n    )\n\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n    adata.varm[\"_beta_rough_dispersions\"] = beta_rough_dispersions.T\n    # Save the rough dispersions beta so that we can reconstruct y_hat\n    set_y_hat(adata)\n\n    # Save global beta in the local data because so it can be used later in\n    # fit_lin_mu. Do it before clipping.\n\n    y_hat = np.maximum(adata.layers[\"_y_hat\"], 1)\n    unnormed_alpha_rde = (\n        ((adata.layers[\"normed_counts\"] - y_hat) ** 2 - y_hat) / (y_hat**2)\n    ).sum(0)\n    return {\n        \"local_rough_dispersions\": unnormed_alpha_rde,\n        \"local_n_obs\": adata.n_obs,\n        \"local_n_params\": adata.uns[\"n_params\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_genewise_dispersions","title":"<code>compute_genewise_dispersions</code>","text":"<p>Main module to compute genewise dispersions.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_genewise_dispersions.ComputeGenewiseDispersions","title":"<code>ComputeGenewiseDispersions</code>","text":"<p>               Bases: <code>ComputeDispersionsGridSearch</code>, <code>ComputeMoMDispersions</code>, <code>LocLinMu</code>, <code>GetNumReplicates</code>, <code>ComputeLFC</code>, <code>LocSetMuHat</code></p> <p>Mixin class to implement the computation of both genewise and MAP dispersions.</p> <p>The switch between genewise and MAP dispersions is done by setting the <code>fit_mode</code> argument in the <code>fit_dispersions</code> to either \"MLE\" or \"MAP\".</p> <p>Methods:</p> Name Description <code>fit_gene_wise_dispersions</code> <p>A method to fit gene-wise dispersions using a grid search. Performs four steps: 1. Compute the first dispersions estimates using a method of moments (MoM) approach. 2. Compute the number of replicates for each combination of factors. This step is necessary to compute the mean estimate in one case, and in downstream steps (cooks distance, etc). 3. Compute an estimate of the mean from these dispersions. 4. Fit the dispersions using a grid search.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_genewise_dispersions.py</code> <pre><code>class ComputeGenewiseDispersions(\n    ComputeDispersionsGridSearch,\n    ComputeMoMDispersions,\n    LocLinMu,\n    GetNumReplicates,\n    ComputeLFC,\n    LocSetMuHat,\n):\n    \"\"\"Mixin class to implement the computation of both genewise and MAP dispersions.\n\n    The switch between genewise and MAP dispersions is done by setting the `fit_mode`\n    argument in the `fit_dispersions` to either \"MLE\" or \"MAP\".\n\n    Methods\n    -------\n    fit_gene_wise_dispersions\n        A method to fit gene-wise dispersions using a grid search.\n        Performs four steps:\n        1. Compute the first dispersions estimates using a\n        method of moments (MoM) approach.\n        2. Compute the number of replicates for each combination of factors.\n        This step is necessary to compute the mean estimate in one case, and\n        in downstream steps (cooks distance, etc).\n        3. Compute an estimate of the mean from these dispersions.\n        4. Fit the dispersions using a grid search.\n    \"\"\"\n\n    @log_organisation_method\n    def fit_genewise_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n        clean_models,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Fit the gene-wise dispersions.\n\n        Performs four steps:\n        1. Compute the first dispersions estimates using a\n        method of moments (MoM) approach.\n        2. Compute the number of replicates for each combination of factors.\n        This step is necessary to compute the mean estimate in one case, and\n        in downstream steps (cooks distance, etc).\n        3. Compute an estimate of the mean from these dispersions.\n        4. Fit the dispersions using a grid search.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        gram_features_shared_states: list\n            The list of shared states outputed by the compute_size_factors step.\n            They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        refit_mode: bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s. (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_state: dict or list[dict]\n            A dictionary containing:\n            - \"genewise_dispersions\": The MLE dispersions, to be stored locally at\n            - \"lower_log_bounds\": log lower bounds for the grid search (only used in\n            internal loop),\n            - \"upper_log_bounds\": log upper bounds for the grid search (only used in\n            internal loop).\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        # ---- Compute MoM dispersions ---- #\n        (\n            local_states,\n            mom_dispersions_shared_state,\n            round_idx,\n        ) = self.compute_MoM_dispersions(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            gram_features_shared_states,\n            round_idx,\n            clean_models,\n            refit_mode=refit_mode,\n        )\n\n        # ---- Compute the initial mu estimates ---- #\n\n        # 1 - Compute the linear mu estimates.\n\n        local_states, linear_shared_states, round_idx = local_step(\n            local_method=self.fit_lin_mu,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=mom_dispersions_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local linear mu estimates.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # 2 - Compute IRLS estimates.\n        local_states, round_idx = self.compute_lfc(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n            lfc_mode=\"mu_init\",\n            refit_mode=refit_mode,\n        )\n\n        # 3 - Compare the number of replicates to the number of design matrix columns\n        # and decide whether to use the IRLS estimates or the linear estimates.\n\n        # Compute the number of replicates\n        local_states, round_idx = self.get_num_replicates(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.set_mu_hat,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Pick between linear and irls mu_hat.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # ---- Fit dispersions ---- #\n        local_states, shared_state, round_idx = self.fit_dispersions(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            shared_state=None,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            fit_mode=\"MLE\",\n            refit_mode=refit_mode,\n        )\n\n        return local_states, shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.compute_genewise_dispersions.ComputeGenewiseDispersions.fit_genewise_dispersions","title":"<code>fit_genewise_dispersions(train_data_nodes, aggregation_node, local_states, gram_features_shared_states, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Fit the gene-wise dispersions.</p> <p>Performs four steps: 1. Compute the first dispersions estimates using a method of moments (MoM) approach. 2. Compute the number of replicates for each combination of factors. This step is necessary to compute the mean estimate in one case, and in downstream steps (cooks distance, etc). 3. Compute an estimate of the mean from these dispersions. 4. Fit the dispersions using a grid search.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>gram_features_shared_states</code> <p>The list of shared states outputed by the compute_size_factors step. They contain a \"local_gram_matrix\" and a \"local_features\" fields.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>shared_state</code> <code>dict or list[dict]</code> <p>A dictionary containing: - \"genewise_dispersions\": The MLE dispersions, to be stored locally at - \"lower_log_bounds\": log lower bounds for the grid search (only used in internal loop), - \"upper_log_bounds\": log upper bounds for the grid search (only used in internal loop).</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/compute_genewise_dispersions.py</code> <pre><code>@log_organisation_method\ndef fit_genewise_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    gram_features_shared_states,\n    round_idx,\n    clean_models,\n    refit_mode: bool = False,\n):\n    \"\"\"Fit the gene-wise dispersions.\n\n    Performs four steps:\n    1. Compute the first dispersions estimates using a\n    method of moments (MoM) approach.\n    2. Compute the number of replicates for each combination of factors.\n    This step is necessary to compute the mean estimate in one case, and\n    in downstream steps (cooks distance, etc).\n    3. Compute an estimate of the mean from these dispersions.\n    4. Fit the dispersions using a grid search.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    gram_features_shared_states: list\n        The list of shared states outputed by the compute_size_factors step.\n        They contain a \"local_gram_matrix\" and a \"local_features\" fields.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    refit_mode: bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s. (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_state: dict or list[dict]\n        A dictionary containing:\n        - \"genewise_dispersions\": The MLE dispersions, to be stored locally at\n        - \"lower_log_bounds\": log lower bounds for the grid search (only used in\n        internal loop),\n        - \"upper_log_bounds\": log upper bounds for the grid search (only used in\n        internal loop).\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    # ---- Compute MoM dispersions ---- #\n    (\n        local_states,\n        mom_dispersions_shared_state,\n        round_idx,\n    ) = self.compute_MoM_dispersions(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n        clean_models,\n        refit_mode=refit_mode,\n    )\n\n    # ---- Compute the initial mu estimates ---- #\n\n    # 1 - Compute the linear mu estimates.\n\n    local_states, linear_shared_states, round_idx = local_step(\n        local_method=self.fit_lin_mu,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=mom_dispersions_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Compute local linear mu estimates.\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    # 2 - Compute IRLS estimates.\n    local_states, round_idx = self.compute_lfc(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n        lfc_mode=\"mu_init\",\n        refit_mode=refit_mode,\n    )\n\n    # 3 - Compare the number of replicates to the number of design matrix columns\n    # and decide whether to use the IRLS estimates or the linear estimates.\n\n    # Compute the number of replicates\n    local_states, round_idx = self.get_num_replicates(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.set_mu_hat,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Pick between linear and irls mu_hat.\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    # ---- Fit dispersions ---- #\n    local_states, shared_state, round_idx = self.fit_dispersions(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_state=None,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        fit_mode=\"MLE\",\n        refit_mode=refit_mode,\n    )\n\n    return local_states, shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates","title":"<code>get_num_replicates</code>","text":"<p>Module containing the mixin class to compute the number of replicates.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.get_num_replicates","title":"<code>get_num_replicates</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.get_num_replicates.GetNumReplicates","title":"<code>GetNumReplicates</code>","text":"<p>               Bases: <code>LocGetDesignMatrixLevels</code>, <code>AggGetCountsLvlForCells</code>, <code>LocFinalizeCellCounts</code></p> <p>Mixin class to get the number of replicates for each combination of factors.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/get_num_replicates.py</code> <pre><code>class GetNumReplicates(\n    LocGetDesignMatrixLevels, AggGetCountsLvlForCells, LocFinalizeCellCounts\n):\n    \"\"\"Mixin class to get the number of replicates for each combination of factors.\"\"\"\n\n    @log_organisation_method\n    def get_num_replicates(\n        self, train_data_nodes, aggregation_node, local_states, round_idx, clean_models\n    ):\n        \"\"\"Compute the number of replicates for each combination of factors.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states, to store the number of replicates and cell level codes.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_get_design_matrix_levels,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get local matrix design level\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n        counts_lvl_share_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_get_counts_lvl_for_cells,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Compute counts level\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, _, round_idx = local_step(\n            local_method=self.loc_finalize_cell_counts,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=counts_lvl_share_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Finalize cell counts\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.get_num_replicates.GetNumReplicates.get_num_replicates","title":"<code>get_num_replicates(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Compute the number of replicates for each combination of factors.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states, to store the number of replicates and cell level codes.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/get_num_replicates.py</code> <pre><code>@log_organisation_method\ndef get_num_replicates(\n    self, train_data_nodes, aggregation_node, local_states, round_idx, clean_models\n):\n    \"\"\"Compute the number of replicates for each combination of factors.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states, to store the number of replicates and cell level codes.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_get_design_matrix_levels,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get local matrix design level\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n    counts_lvl_share_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_get_counts_lvl_for_cells,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Compute counts level\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, _, round_idx = local_step(\n        local_method=self.loc_finalize_cell_counts,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=counts_lvl_share_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Finalize cell counts\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.AggGetCountsLvlForCells","title":"<code>AggGetCountsLvlForCells</code>","text":"<p>Mixin that aggregate the counts of the design matrix values.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>class AggGetCountsLvlForCells:\n    \"\"\"Mixin that aggregate the counts of the design matrix values.\"\"\"\n\n    @remote\n    @log_remote\n    def agg_get_counts_lvl_for_cells(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Aggregate the counts of the design matrix values.\n\n        Parameters\n        ----------\n        shared_states : list(dict)\n            List of shared states with the following key:\n            - unique_counts: unique values and counts of the local design matrix\n\n        Returns\n        -------\n        dict\n            Dictionary with keys labeling the different values taken by the\n            overall design matrix. Each values of the dictionary contains the\n            sum of the counts of the corresponding design matrix value and the level.\n        \"\"\"\n        concat_unique_cont = pd.concat(\n            [shared_state[\"unique_counts\"] for shared_state in shared_states], axis=1\n        )\n        counts_by_lvl = concat_unique_cont.fillna(0).sum(axis=1).astype(int)\n\n        return {\"counts_by_lvl\": counts_by_lvl}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.AggGetCountsLvlForCells.agg_get_counts_lvl_for_cells","title":"<code>agg_get_counts_lvl_for_cells(shared_states)</code>","text":"<p>Aggregate the counts of the design matrix values.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list(dict)</code> <p>List of shared states with the following key: - unique_counts: unique values and counts of the local design matrix</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys labeling the different values taken by the overall design matrix. Each values of the dictionary contains the sum of the counts of the corresponding design matrix value and the level.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_get_counts_lvl_for_cells(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Aggregate the counts of the design matrix values.\n\n    Parameters\n    ----------\n    shared_states : list(dict)\n        List of shared states with the following key:\n        - unique_counts: unique values and counts of the local design matrix\n\n    Returns\n    -------\n    dict\n        Dictionary with keys labeling the different values taken by the\n        overall design matrix. Each values of the dictionary contains the\n        sum of the counts of the corresponding design matrix value and the level.\n    \"\"\"\n    concat_unique_cont = pd.concat(\n        [shared_state[\"unique_counts\"] for shared_state in shared_states], axis=1\n    )\n    counts_by_lvl = concat_unique_cont.fillna(0).sum(axis=1).astype(int)\n\n    return {\"counts_by_lvl\": counts_by_lvl}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.LocFinalizeCellCounts","title":"<code>LocFinalizeCellCounts</code>","text":"<p>Mixin that finalize the cell counts.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>class LocFinalizeCellCounts:\n    \"\"\"Mixin that finalize the cell counts.\"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_finalize_cell_counts(self, data_from_opener, shared_state=dict) -&gt; None:\n        \"\"\"Finalize the cell counts.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Dictionary with keys labeling the different values taken by the\n            overall design matrix. Each values of the dictionary contains the\n            sum of the counts of the corresponding design matrix value and the level.\n        \"\"\"\n        counts_by_lvl = shared_state[\"counts_by_lvl\"]\n\n        # In order to keep the same objects 'num_replicates' and 'cells' used in\n        # PyDESeq2, we provide names (0, 1, 2...) to the possible values of the\n        # design matrix, called \"lvl\".\n        # The index of 'num_replicates' is the lvl names (0,1,2...) and its values\n        # the counts of these lvl\n        # 'cells' index is the index of the cells in the adata and its values the lvl\n        # name (0,1,2..) of the cell.\n        self.local_adata.uns[\"num_replicates\"] = pd.Series(counts_by_lvl.values)\n        self.local_adata.obs[\"cells\"] = [\n            np.argwhere(counts_by_lvl.index == tuple(design))[0, 0]\n            for design in self.local_adata.obsm[\"design_matrix\"].values\n        ]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.LocFinalizeCellCounts.loc_finalize_cell_counts","title":"<code>loc_finalize_cell_counts(data_from_opener, shared_state=dict)</code>","text":"<p>Finalize the cell counts.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with keys labeling the different values taken by the overall design matrix. Each values of the dictionary contains the sum of the counts of the corresponding design matrix value and the level.</p> <code>dict</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_finalize_cell_counts(self, data_from_opener, shared_state=dict) -&gt; None:\n    \"\"\"Finalize the cell counts.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Dictionary with keys labeling the different values taken by the\n        overall design matrix. Each values of the dictionary contains the\n        sum of the counts of the corresponding design matrix value and the level.\n    \"\"\"\n    counts_by_lvl = shared_state[\"counts_by_lvl\"]\n\n    # In order to keep the same objects 'num_replicates' and 'cells' used in\n    # PyDESeq2, we provide names (0, 1, 2...) to the possible values of the\n    # design matrix, called \"lvl\".\n    # The index of 'num_replicates' is the lvl names (0,1,2...) and its values\n    # the counts of these lvl\n    # 'cells' index is the index of the cells in the adata and its values the lvl\n    # name (0,1,2..) of the cell.\n    self.local_adata.uns[\"num_replicates\"] = pd.Series(counts_by_lvl.values)\n    self.local_adata.obs[\"cells\"] = [\n        np.argwhere(counts_by_lvl.index == tuple(design))[0, 0]\n        for design in self.local_adata.obsm[\"design_matrix\"].values\n    ]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.LocGetDesignMatrixLevels","title":"<code>LocGetDesignMatrixLevels</code>","text":"<p>Mixin to get the unique values of the local design matrix.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>class LocGetDesignMatrixLevels:\n    \"\"\"Mixin to get the unique values of the local design matrix.\"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_get_design_matrix_levels(self, data_from_opener, shared_state=dict) -&gt; dict:\n        \"\"\"Get the values of the local design matrix.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n        shared_state : dict\n            Not used.\n\n        Returns\n        -------\n        dict\n            Dictionary with the following key:\n            - unique_counts: unique values and counts of the local design matrix\n        \"\"\"\n        unique_counts = self.local_adata.obsm[\"design_matrix\"].value_counts()\n\n        return {\"unique_counts\": unique_counts}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.get_num_replicates.substeps.LocGetDesignMatrixLevels.loc_get_design_matrix_levels","title":"<code>loc_get_design_matrix_levels(data_from_opener, shared_state=dict)</code>","text":"<p>Get the values of the local design matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> <code>dict</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following key: - unique_counts: unique values and counts of the local design matrix</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/get_num_replicates/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_get_design_matrix_levels(self, data_from_opener, shared_state=dict) -&gt; dict:\n    \"\"\"Get the values of the local design matrix.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n    shared_state : dict\n        Not used.\n\n    Returns\n    -------\n    dict\n        Dictionary with the following key:\n        - unique_counts: unique values and counts of the local design matrix\n    \"\"\"\n    unique_counts = self.local_adata.obsm[\"design_matrix\"].value_counts()\n\n    return {\"unique_counts\": unique_counts}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.substeps.LocLinMu","title":"<code>LocLinMu</code>","text":"<p>Mixin to fit linear mu estimates locally.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/substeps.py</code> <pre><code>class LocLinMu:\n    \"\"\"Mixin to fit linear mu estimates locally.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n    min_mu: float\n    max_disp: float\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def fit_lin_mu(\n        self, data_from_opener, shared_state, min_mu=0.5, refit_mode: bool = False\n    ):\n        \"\"\"Fit linear mu estimates and store them locally.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Not used.\n\n        shared_state : dict\n            Contains values to be saved in local adata:\n            - \"MoM_dispersions\": MoM dispersions,\n            - \"nom_zero\": Mask of all zero genes,\n            - \"tot_num_samples\": Total number of samples.\n\n        min_mu : float\n            Lower threshold for fitted means, for numerical stability.\n            (default: ``0.5``).\n\n        refit_mode : bool\n            Whether to run the pipeline in refit mode. If True, the pipeline will be run\n            on `refit_adata`s instead of `local_adata`s. (default: ``False``).\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # save MoM dispersions computed in the previous step\n        adata.varm[\"_MoM_dispersions\"] = shared_state[\"MoM_dispersions\"]\n\n        # save mask of all zero genes.\n        # TODO: check that we should also do this in refit mode\n        adata.varm[\"non_zero\"] = shared_state[\"non_zero\"]\n\n        if not refit_mode:  # In refit mode, those are unchanged\n            # save the total number of samples\n            self.local_adata.uns[\"tot_num_samples\"] = shared_state[\"tot_num_samples\"]\n\n            # use it to set max_disp\n            self.local_adata.uns[\"max_disp\"] = max(\n                self.max_disp, self.local_adata.uns[\"tot_num_samples\"]\n            )\n\n        # save the base_mean for independent filtering\n        adata.varm[\"_normed_means\"] = shared_state[\"tot_counts_mean\"]\n\n        # compute mu_hat\n        set_fit_lin_mu_hat(adata, min_mu=min_mu)\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.substeps.LocLinMu.fit_lin_mu","title":"<code>fit_lin_mu(data_from_opener, shared_state, min_mu=0.5, refit_mode=False)</code>","text":"<p>Fit linear mu estimates and store them locally.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Contains values to be saved in local adata: - \"MoM_dispersions\": MoM dispersions, - \"nom_zero\": Mask of all zero genes, - \"tot_num_samples\": Total number of samples.</p> required <code>min_mu</code> <code>float</code> <p>Lower threshold for fitted means, for numerical stability. (default: <code>0.5</code>).</p> <code>0.5</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: <code>False</code>).</p> <code>False</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef fit_lin_mu(\n    self, data_from_opener, shared_state, min_mu=0.5, refit_mode: bool = False\n):\n    \"\"\"Fit linear mu estimates and store them locally.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Not used.\n\n    shared_state : dict\n        Contains values to be saved in local adata:\n        - \"MoM_dispersions\": MoM dispersions,\n        - \"nom_zero\": Mask of all zero genes,\n        - \"tot_num_samples\": Total number of samples.\n\n    min_mu : float\n        Lower threshold for fitted means, for numerical stability.\n        (default: ``0.5``).\n\n    refit_mode : bool\n        Whether to run the pipeline in refit mode. If True, the pipeline will be run\n        on `refit_adata`s instead of `local_adata`s. (default: ``False``).\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # save MoM dispersions computed in the previous step\n    adata.varm[\"_MoM_dispersions\"] = shared_state[\"MoM_dispersions\"]\n\n    # save mask of all zero genes.\n    # TODO: check that we should also do this in refit mode\n    adata.varm[\"non_zero\"] = shared_state[\"non_zero\"]\n\n    if not refit_mode:  # In refit mode, those are unchanged\n        # save the total number of samples\n        self.local_adata.uns[\"tot_num_samples\"] = shared_state[\"tot_num_samples\"]\n\n        # use it to set max_disp\n        self.local_adata.uns[\"max_disp\"] = max(\n            self.max_disp, self.local_adata.uns[\"tot_num_samples\"]\n        )\n\n    # save the base_mean for independent filtering\n    adata.varm[\"_normed_means\"] = shared_state[\"tot_counts_mean\"]\n\n    # compute mu_hat\n    set_fit_lin_mu_hat(adata, min_mu=min_mu)\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.substeps.LocSetMuHat","title":"<code>LocSetMuHat</code>","text":"<p>Mixin to set mu estimates locally.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/substeps.py</code> <pre><code>class LocSetMuHat:\n    \"\"\"Mixin to set mu estimates locally.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def set_mu_hat(\n        self,\n        data_from_opener,\n        shared_state,\n        refit_mode: bool = False,\n    ) -&gt; None:\n        \"\"\"Pick between linear and IRLS mu estimates.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Not used.\n\n        shared_state : dict\n            Not used.\n\n        refit_mode : bool\n            Whether to run on `refit_adata`s instead of `local_adata`s.\n            (default: ``False``).\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n        # TODO make sure that the adata has the num_replicates and the n_params\n        set_mu_hat_layer(adata)\n        del adata.layers[\"_fit_lin_mu_hat\"]\n        del adata.layers[\"_irls_mu_hat\"]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_genewise_dispersions.substeps.LocSetMuHat.set_mu_hat","title":"<code>set_mu_hat(data_from_opener, shared_state, refit_mode=False)</code>","text":"<p>Pick between linear and IRLS mu estimates.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: <code>False</code>).</p> <code>False</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_genewise_dispersions/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef set_mu_hat(\n    self,\n    data_from_opener,\n    shared_state,\n    refit_mode: bool = False,\n) -&gt; None:\n    \"\"\"Pick between linear and IRLS mu estimates.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Not used.\n\n    shared_state : dict\n        Not used.\n\n    refit_mode : bool\n        Whether to run on `refit_adata`s instead of `local_adata`s.\n        (default: ``False``).\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n    # TODO make sure that the adata has the num_replicates and the n_params\n    set_mu_hat_layer(adata)\n    del adata.layers[\"_fit_lin_mu_hat\"]\n    del adata.layers[\"_irls_mu_hat\"]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc","title":"<code>compute_lfc</code>","text":"<p>Module which contains the Mixin in charge of fitting log fold changes.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.compute_lfc","title":"<code>compute_lfc</code>","text":"<p>Module containing the ComputeLFC method.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.compute_lfc.ComputeLFC","title":"<code>ComputeLFC</code>","text":"<p>               Bases: <code>LocGetGramMatrixAndLogFeatures</code>, <code>AggCreateBetaInit</code>, <code>LocSaveLFC</code>, <code>FedProxQuasiNewton</code>, <code>FedIRLS</code></p> <p>Mixin class to implement the LFC computation algorithm.</p> <p>The goal of this class is to implement the IRLS algorithm specifically applied to the negative binomial distribution, with fixed dispersion parameter, and in the case where it fails, to catch it with the FedProxQuasiNewton algorithm.</p> <p>This class also initializes the beta parameters and computes the final hat matrix.</p> <p>Methods:</p> Name Description <code>compute_lfc</code> <p>The main method to compute the log fold changes by running the IRLS algorithm and catching it with the FedProxQuasiNewton algorithm.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/compute_lfc.py</code> <pre><code>class ComputeLFC(\n    LocGetGramMatrixAndLogFeatures,\n    AggCreateBetaInit,\n    LocSaveLFC,\n    FedProxQuasiNewton,\n    FedIRLS,\n):\n    r\"\"\"Mixin class to implement the LFC computation algorithm.\n\n    The goal of this class is to implement the IRLS algorithm specifically applied\n    to the negative binomial distribution, with fixed dispersion parameter, and\n    in the case where it fails, to catch it with the FedProxQuasiNewton algorithm.\n\n    This class also initializes the beta parameters and computes the final hat matrix.\n\n    Methods\n    -------\n    compute_lfc\n        The main method to compute the log fold changes by\n        running the IRLS algorithm and catching it with the\n        FedProxQuasiNewton algorithm.\n    \"\"\"\n\n    @log_organisation_method\n    def compute_lfc(\n        self,\n        train_data_nodes: list,\n        aggregation_node: AggregationNode,\n        local_states: dict,\n        round_idx: int,\n        clean_models: bool = True,\n        lfc_mode: Literal[\"lfc\", \"mu_init\"] = \"lfc\",\n        refit_mode: bool = False,\n    ):\n        \"\"\"Compute the log fold changes.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            If True, the models are cleaned.\n\n        lfc_mode: Literal[\"lfc\", \"mu_init\"]\n            The mode of the IRLS algorithm (\"lfc\" or \"mu_init\").\n\n        refit_mode: bool\n            Whether to run the pipeline in refit mode, after cooks outliers were\n            replaced. If True, the pipeline will be run on `refit_adata`s instead of\n            `local_adata`s. (default: False).\n\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        #### ---- Initialization ---- ####\n\n        # ---- Compute initial local beta estimates ---- #\n\n        local_states, local_beta_init_shared_states, round_idx = local_step(\n            local_method=self.get_gram_matrix_and_log_features,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Create local initialization beta.\",\n            clean_models=clean_models,\n            method_params={\n                \"lfc_mode\": lfc_mode,\n                \"refit_mode\": refit_mode,\n            },\n        )\n\n        # ---- Compute initial global beta estimates ---- #\n\n        global_irls_summands_nlls_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.create_beta_init,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=local_beta_init_shared_states,\n            description=\"Create initialization beta paramater.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        #### ---- Run IRLS ---- #####\n        (\n            local_states,\n            irls_result_shared_state,\n            round_idx,\n        ) = self.run_fed_irls(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            input_shared_state=global_irls_summands_nlls_shared_state,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=refit_mode,\n        )\n\n        #### ---- Catch with FedProxQuasiNewton ----####\n\n        local_states, PQN_shared_state, round_idx = self.run_fed_PQN(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            PQN_shared_state=irls_result_shared_state,\n            first_iteration_mode=\"irls_catch\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=refit_mode,\n        )\n\n        # ---- Compute final hat matrix summands ---- #\n\n        (\n            local_states,\n            _,\n            round_idx,\n        ) = local_step(\n            local_method=self.save_lfc_to_local,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=PQN_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local hat matrix summands and last nll.\",\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.compute_lfc.ComputeLFC.compute_lfc","title":"<code>compute_lfc(train_data_nodes, aggregation_node, local_states, round_idx, clean_models=True, lfc_mode='lfc', refit_mode=False)</code>","text":"<p>Compute the log fold changes.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <code>list</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <code>AggregationNode</code> <p>The aggregation node.</p> required <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <code>int</code> <p>The current round.</p> required <code>clean_models</code> <code>bool</code> <p>If True, the models are cleaned.</p> <code>True</code> <code>lfc_mode</code> <code>Literal['lfc', 'mu_init']</code> <p>The mode of the IRLS algorithm (\"lfc\" or \"mu_init\").</p> <code>'lfc'</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline in refit mode, after cooks outliers were replaced. If True, the pipeline will be run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/compute_lfc.py</code> <pre><code>@log_organisation_method\ndef compute_lfc(\n    self,\n    train_data_nodes: list,\n    aggregation_node: AggregationNode,\n    local_states: dict,\n    round_idx: int,\n    clean_models: bool = True,\n    lfc_mode: Literal[\"lfc\", \"mu_init\"] = \"lfc\",\n    refit_mode: bool = False,\n):\n    \"\"\"Compute the log fold changes.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        If True, the models are cleaned.\n\n    lfc_mode: Literal[\"lfc\", \"mu_init\"]\n        The mode of the IRLS algorithm (\"lfc\" or \"mu_init\").\n\n    refit_mode: bool\n        Whether to run the pipeline in refit mode, after cooks outliers were\n        replaced. If True, the pipeline will be run on `refit_adata`s instead of\n        `local_adata`s. (default: False).\n\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    #### ---- Initialization ---- ####\n\n    # ---- Compute initial local beta estimates ---- #\n\n    local_states, local_beta_init_shared_states, round_idx = local_step(\n        local_method=self.get_gram_matrix_and_log_features,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Create local initialization beta.\",\n        clean_models=clean_models,\n        method_params={\n            \"lfc_mode\": lfc_mode,\n            \"refit_mode\": refit_mode,\n        },\n    )\n\n    # ---- Compute initial global beta estimates ---- #\n\n    global_irls_summands_nlls_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.create_beta_init,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=local_beta_init_shared_states,\n        description=\"Create initialization beta paramater.\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    #### ---- Run IRLS ---- #####\n    (\n        local_states,\n        irls_result_shared_state,\n        round_idx,\n    ) = self.run_fed_irls(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        local_states=local_states,\n        input_shared_state=global_irls_summands_nlls_shared_state,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        refit_mode=refit_mode,\n    )\n\n    #### ---- Catch with FedProxQuasiNewton ----####\n\n    local_states, PQN_shared_state, round_idx = self.run_fed_PQN(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        local_states=local_states,\n        PQN_shared_state=irls_result_shared_state,\n        first_iteration_mode=\"irls_catch\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        refit_mode=refit_mode,\n    )\n\n    # ---- Compute final hat matrix summands ---- #\n\n    (\n        local_states,\n        _,\n        round_idx,\n    ) = local_step(\n        local_method=self.save_lfc_to_local,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=PQN_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Compute local hat matrix summands and last nll.\",\n        clean_models=clean_models,\n        method_params={\"refit_mode\": refit_mode},\n    )\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps","title":"<code>substeps</code>","text":"<p>Module to implement the substeps for the fitting of log fold changes.</p> <p>This module contains all these substeps as mixin classes.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.AggCreateBetaInit","title":"<code>AggCreateBetaInit</code>","text":"<p>Mixin to create the beta init.</p> <p>Methods:</p> Name Description <code>create_beta_init</code> <p>A remote method. Creates the beta init (initialization value for the ComputeLFC algorithm) and returns the initialization state for the IRLS algorithm containing this initialization value and other necessary quantities.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>class AggCreateBetaInit:\n    \"\"\"Mixin to create the beta init.\n\n    Methods\n    -------\n    create_beta_init\n        A remote method. Creates the beta init (initialization value for the\n        ComputeLFC algorithm) and returns the initialization state for the\n        IRLS algorithm containing this initialization value and\n        other necessary quantities.\n    \"\"\"\n\n    @remote\n    @log_remote\n    def create_beta_init(self, shared_states: list[dict]) -&gt; dict[str, Any]:\n        \"\"\"Create the beta init.\n\n        It does so either by solving the least squares regression system if\n        the gram matrix is full rank, or by aggregating the log means if the\n        gram matrix is not full rank.\n\n        Parameters\n        ----------\n        shared_states: list[dict]\n            A list of dictionaries containing the following\n            keys:\n            - gram_full_rank: bool\n                Whether the gram matrix is full rank.\n            - n_non_zero_genes: int\n                The number of non zero genes.\n            - n_params: int\n                The number of parameters.\n            If the gram matrix is full rank, the state contains:\n            -  local_log_features: ndarray\n                The local log features, only if the gram matrix is full rank.\n            - global_gram_matrix: ndarray\n                The global gram matrix, only if the gram matrix is full rank.\n            If the gram matrix is not full rank, the state contains:\n            - normed_log_means: ndarray\n                The normed log means, only if the gram matrix is not full rank.\n            - n_obs: int\n                The number of observations, only if the gram matrix is not full rank.\n\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary containing all the necessary info to run IRLS.\n            It contains the following fields:\n            - beta: ndarray\n                The initial beta, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)). Is set to False initially, and will\n                be set to True if the gene has diverged.\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)). Is set to True initially, and will be\n                set to False if the gene has converged or diverged.\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n        \"\"\"\n        # Get the global quantities\n        gram_full_rank = shared_states[0][\"gram_full_rank\"]\n        n_non_zero_genes = shared_states[0][\"n_non_zero_genes\"]\n\n        # Step 1: Get the beta init\n        # Condition on whether or not the gram matrix is full rank\n        if gram_full_rank:\n            # Get global gram matrix\n            global_gram_matrix = shared_states[0][\"global_gram_matrix\"]\n\n            # Aggregate the feature vectors\n            feature_vectors = sum(\n                [state[\"local_log_features\"] for state in shared_states]\n            )\n\n            # Solve the system\n            beta_init = np.linalg.solve(global_gram_matrix, feature_vectors.T).T\n\n        else:\n            # Aggregate the log means\n            tot_counts = sum([state[\"n_obs\"] for state in shared_states])\n            beta_init = (\n                sum(\n                    [\n                        state[\"normed_log_means\"] * state[\"n_obs\"]\n                        for state in shared_states\n                    ]\n                )\n                / tot_counts\n            )\n\n        # Step 2: instantiate other necessary quantities\n        irls_diverged_mask = np.full(n_non_zero_genes, False)\n        irls_mask = np.full(n_non_zero_genes, True)\n        global_nll = np.full(n_non_zero_genes, 1000.0)\n\n        return {\n            \"beta\": beta_init,\n            \"irls_diverged_mask\": irls_diverged_mask,\n            \"irls_mask\": irls_mask,\n            \"global_nll\": global_nll,\n            \"round_number_irls\": 0,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.AggCreateBetaInit.create_beta_init","title":"<code>create_beta_init(shared_states)</code>","text":"<p>Create the beta init.</p> <p>It does so either by solving the least squares regression system if the gram matrix is full rank, or by aggregating the log means if the gram matrix is not full rank.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>A list of dictionaries containing the following keys: - gram_full_rank: bool     Whether the gram matrix is full rank. - n_non_zero_genes: int     The number of non zero genes. - n_params: int     The number of parameters. If the gram matrix is full rank, the state contains: -  local_log_features: ndarray     The local log features, only if the gram matrix is full rank. - global_gram_matrix: ndarray     The global gram matrix, only if the gram matrix is full rank. If the gram matrix is not full rank, the state contains: - normed_log_means: ndarray     The normed log means, only if the gram matrix is not full rank. - n_obs: int     The number of observations, only if the gram matrix is not full rank.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing all the necessary info to run IRLS. It contains the following fields: - beta: ndarray     The initial beta, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). Is set to False initially, and will     be set to True if the gene has diverged. - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). Is set to True initially, and will be     set to False if the gene has converged or diverged. - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape     (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>@remote\n@log_remote\ndef create_beta_init(self, shared_states: list[dict]) -&gt; dict[str, Any]:\n    \"\"\"Create the beta init.\n\n    It does so either by solving the least squares regression system if\n    the gram matrix is full rank, or by aggregating the log means if the\n    gram matrix is not full rank.\n\n    Parameters\n    ----------\n    shared_states: list[dict]\n        A list of dictionaries containing the following\n        keys:\n        - gram_full_rank: bool\n            Whether the gram matrix is full rank.\n        - n_non_zero_genes: int\n            The number of non zero genes.\n        - n_params: int\n            The number of parameters.\n        If the gram matrix is full rank, the state contains:\n        -  local_log_features: ndarray\n            The local log features, only if the gram matrix is full rank.\n        - global_gram_matrix: ndarray\n            The global gram matrix, only if the gram matrix is full rank.\n        If the gram matrix is not full rank, the state contains:\n        - normed_log_means: ndarray\n            The normed log means, only if the gram matrix is not full rank.\n        - n_obs: int\n            The number of observations, only if the gram matrix is not full rank.\n\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary containing all the necessary info to run IRLS.\n        It contains the following fields:\n        - beta: ndarray\n            The initial beta, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)). Is set to False initially, and will\n            be set to True if the gene has diverged.\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)). Is set to True initially, and will be\n            set to False if the gene has converged or diverged.\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n    \"\"\"\n    # Get the global quantities\n    gram_full_rank = shared_states[0][\"gram_full_rank\"]\n    n_non_zero_genes = shared_states[0][\"n_non_zero_genes\"]\n\n    # Step 1: Get the beta init\n    # Condition on whether or not the gram matrix is full rank\n    if gram_full_rank:\n        # Get global gram matrix\n        global_gram_matrix = shared_states[0][\"global_gram_matrix\"]\n\n        # Aggregate the feature vectors\n        feature_vectors = sum(\n            [state[\"local_log_features\"] for state in shared_states]\n        )\n\n        # Solve the system\n        beta_init = np.linalg.solve(global_gram_matrix, feature_vectors.T).T\n\n    else:\n        # Aggregate the log means\n        tot_counts = sum([state[\"n_obs\"] for state in shared_states])\n        beta_init = (\n            sum(\n                [\n                    state[\"normed_log_means\"] * state[\"n_obs\"]\n                    for state in shared_states\n                ]\n            )\n            / tot_counts\n        )\n\n    # Step 2: instantiate other necessary quantities\n    irls_diverged_mask = np.full(n_non_zero_genes, False)\n    irls_mask = np.full(n_non_zero_genes, True)\n    global_nll = np.full(n_non_zero_genes, 1000.0)\n\n    return {\n        \"beta\": beta_init,\n        \"irls_diverged_mask\": irls_diverged_mask,\n        \"irls_mask\": irls_mask,\n        \"global_nll\": global_nll,\n        \"round_number_irls\": 0,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.LocGetGramMatrixAndLogFeatures","title":"<code>LocGetGramMatrixAndLogFeatures</code>","text":"<p>Mixin accessing the quantities to compute the initial beta of ComputeLFC.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> <p>Methods:</p> Name Description <code>get_gram_matrix_and_log_features</code> <p>A remote_data method. Creates the local quantities necessary to compute the initial beta. If the gram matrix is full rank, it shares the features vector and the gram matrix. If the gram matrix is not full rank, it shares the normed log means and the number of observations.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>class LocGetGramMatrixAndLogFeatures:\n    \"\"\"Mixin accessing the quantities to compute the initial beta of ComputeLFC.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        The local AnnData object.\n\n    Methods\n    -------\n    get_gram_matrix_and_log_features\n        A remote_data method. Creates the local quantities necessary\n        to compute the initial beta.\n        If the gram matrix is full rank, it shares the features vector\n        and the gram matrix. If the gram matrix is not full rank, it shares\n        the normed log means and the number of observations.\n    \"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_gram_matrix_and_log_features(\n        self,\n        data_from_opener: AnnData,\n        shared_state: dict[str, Any],\n        lfc_mode: Literal[\"lfc\", \"mu_init\"],\n        refit_mode: bool = False,\n    ):\n        \"\"\"Create the local quantities necessary to compute the initial beta.\n\n        To do so, we assume that the local_adata.uns contains the following fields:\n        - n_params: int\n            The number of parameters.\n        - _global_gram_matrix: ndarray\n            The global gram matrix.\n\n        From the IRLS mode, we will set the following fields:\n        - _irls_mu_param_name: str\n            The name of the mu parameter, to save at the end of the IRLS run\n            This is None if we do not want to save the mu parameter.\n        - _irls_beta_param_name: str\n            The name of the beta parameter, to save as a varm at the end of the\n            fed irls run\n            This is None if we do not want to save the beta parameter.\n        - _irls_disp_param_name: str\n            The name of the dispersion parameter.\n        - _lfc_mode: str\n            The mode of the IRLS algorithm. This is used to set the previous fields.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Not used, all the necessary info is stored in the local adata.\n\n        lfc_mode : Literal[\"lfc\", \"mu_init\"]\n            The mode of the IRLS algorithm (\"lfc\", or \"mu_init\").\n\n        refit_mode : bool\n            Whether to run the pipeline on `refit_adata` instead of `local_adata`.\n\n        Returns\n        -------\n        dict\n            The state to share to the server.\n            It always contains the following fields:\n            - gram_full_rank: bool\n                Whether the gram matrix is full rank.\n            - n_non_zero_genes: int\n                The number of non zero genes.\n            - n_params: int\n                The number of parameters.\n            - If the gram matrix is full rank, the state contains:\n                - local_log_features: ndarray\n                    The local log features.\n                - global_gram_matrix: ndarray\n                    The global gram matrix.\n            - If the gram matrix is not full rank, the state contains:\n                - normed_log_means: ndarray\n                    The normed log means.\n                - n_obs: int\n                    The number of observations.\n        \"\"\"\n        global_gram_matrix = self.local_adata.uns[\"_global_gram_matrix\"]\n\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # Elements to pass on to the next steps of the method\n        if lfc_mode == \"lfc\":\n            adata.uns[\"_irls_mu_param_name\"] = \"_mu_LFC\"\n            adata.uns[\"_irls_beta_param_name\"] = \"LFC\"\n            adata.uns[\"_irls_disp_param_name\"] = \"dispersions\"\n            adata.uns[\"_lfc_mode\"] = \"lfc\"\n        elif lfc_mode == \"mu_init\":\n            adata.uns[\"_irls_mu_param_name\"] = \"_irls_mu_hat\"\n            adata.uns[\"_irls_beta_param_name\"] = \"_mu_hat_LFC\"\n            adata.uns[\"_irls_disp_param_name\"] = \"_MoM_dispersions\"\n            adata.uns[\"_lfc_mode\"] = \"mu_init\"\n\n        else:\n            raise NotImplementedError(\n                f\"Only 'lfc' and 'mu_init' irls modes are supported, got {lfc_mode}.\"\n            )\n\n        # Get non zero genes\n        non_zero_genes_names = adata.var_names[adata.varm[\"non_zero\"]]\n\n        # See if gram matrix is full rank\n        gram_full_rank = (\n            np.linalg.matrix_rank(global_gram_matrix) == adata.uns[\"n_params\"]\n        )\n        # If the gram matrix is full rank, share the features vector and the gram\n        # matrix\n\n        shared_state = {\n            \"gram_full_rank\": gram_full_rank,\n            \"n_non_zero_genes\": len(non_zero_genes_names),\n        }\n\n        if gram_full_rank:\n            # Make log features\n            design = adata.obsm[\"design_matrix\"].values\n            log_counts = np.log(\n                adata[:, non_zero_genes_names].layers[\"normed_counts\"] + 0.1\n            )\n            log_features = (design.T @ log_counts).T\n            shared_state.update(\n                {\n                    \"local_log_features\": log_features,\n                    \"global_gram_matrix\": global_gram_matrix,\n                }\n            )\n        else:\n            # TODO: check that this is correctly recomputed in refit mode\n            if \"normed_log_means\" not in adata.varm:\n                with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n                    log_counts = np.log(adata.layers[\"normed_counts\"])\n                    adata.varm[\"normed_log_means\"] = log_counts.mean(0)\n            normed_log_means = adata.varm[\"normed_log_means\"]\n            n_obs = adata.n_obs\n            shared_state.update({\"normed_log_means\": normed_log_means, \"n_obs\": n_obs})\n        return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.LocGetGramMatrixAndLogFeatures.get_gram_matrix_and_log_features","title":"<code>get_gram_matrix_and_log_features(data_from_opener, shared_state, lfc_mode, refit_mode=False)</code>","text":"<p>Create the local quantities necessary to compute the initial beta.</p> <p>To do so, we assume that the local_adata.uns contains the following fields: - n_params: int     The number of parameters. - _global_gram_matrix: ndarray     The global gram matrix.</p> <p>From the IRLS mode, we will set the following fields: - _irls_mu_param_name: str     The name of the mu parameter, to save at the end of the IRLS run     This is None if we do not want to save the mu parameter. - _irls_beta_param_name: str     The name of the beta parameter, to save as a varm at the end of the     fed irls run     This is None if we do not want to save the beta parameter. - _irls_disp_param_name: str     The name of the dispersion parameter. - _lfc_mode: str     The mode of the IRLS algorithm. This is used to set the previous fields.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used, all the necessary info is stored in the local adata.</p> required <code>lfc_mode</code> <code>Literal['lfc', 'mu_init']</code> <p>The mode of the IRLS algorithm (\"lfc\", or \"mu_init\").</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline on <code>refit_adata</code> instead of <code>local_adata</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The state to share to the server. It always contains the following fields: - gram_full_rank: bool     Whether the gram matrix is full rank. - n_non_zero_genes: int     The number of non zero genes. - n_params: int     The number of parameters. - If the gram matrix is full rank, the state contains:     - local_log_features: ndarray         The local log features.     - global_gram_matrix: ndarray         The global gram matrix. - If the gram matrix is not full rank, the state contains:     - normed_log_means: ndarray         The normed log means.     - n_obs: int         The number of observations.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_gram_matrix_and_log_features(\n    self,\n    data_from_opener: AnnData,\n    shared_state: dict[str, Any],\n    lfc_mode: Literal[\"lfc\", \"mu_init\"],\n    refit_mode: bool = False,\n):\n    \"\"\"Create the local quantities necessary to compute the initial beta.\n\n    To do so, we assume that the local_adata.uns contains the following fields:\n    - n_params: int\n        The number of parameters.\n    - _global_gram_matrix: ndarray\n        The global gram matrix.\n\n    From the IRLS mode, we will set the following fields:\n    - _irls_mu_param_name: str\n        The name of the mu parameter, to save at the end of the IRLS run\n        This is None if we do not want to save the mu parameter.\n    - _irls_beta_param_name: str\n        The name of the beta parameter, to save as a varm at the end of the\n        fed irls run\n        This is None if we do not want to save the beta parameter.\n    - _irls_disp_param_name: str\n        The name of the dispersion parameter.\n    - _lfc_mode: str\n        The mode of the IRLS algorithm. This is used to set the previous fields.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Not used, all the necessary info is stored in the local adata.\n\n    lfc_mode : Literal[\"lfc\", \"mu_init\"]\n        The mode of the IRLS algorithm (\"lfc\", or \"mu_init\").\n\n    refit_mode : bool\n        Whether to run the pipeline on `refit_adata` instead of `local_adata`.\n\n    Returns\n    -------\n    dict\n        The state to share to the server.\n        It always contains the following fields:\n        - gram_full_rank: bool\n            Whether the gram matrix is full rank.\n        - n_non_zero_genes: int\n            The number of non zero genes.\n        - n_params: int\n            The number of parameters.\n        - If the gram matrix is full rank, the state contains:\n            - local_log_features: ndarray\n                The local log features.\n            - global_gram_matrix: ndarray\n                The global gram matrix.\n        - If the gram matrix is not full rank, the state contains:\n            - normed_log_means: ndarray\n                The normed log means.\n            - n_obs: int\n                The number of observations.\n    \"\"\"\n    global_gram_matrix = self.local_adata.uns[\"_global_gram_matrix\"]\n\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # Elements to pass on to the next steps of the method\n    if lfc_mode == \"lfc\":\n        adata.uns[\"_irls_mu_param_name\"] = \"_mu_LFC\"\n        adata.uns[\"_irls_beta_param_name\"] = \"LFC\"\n        adata.uns[\"_irls_disp_param_name\"] = \"dispersions\"\n        adata.uns[\"_lfc_mode\"] = \"lfc\"\n    elif lfc_mode == \"mu_init\":\n        adata.uns[\"_irls_mu_param_name\"] = \"_irls_mu_hat\"\n        adata.uns[\"_irls_beta_param_name\"] = \"_mu_hat_LFC\"\n        adata.uns[\"_irls_disp_param_name\"] = \"_MoM_dispersions\"\n        adata.uns[\"_lfc_mode\"] = \"mu_init\"\n\n    else:\n        raise NotImplementedError(\n            f\"Only 'lfc' and 'mu_init' irls modes are supported, got {lfc_mode}.\"\n        )\n\n    # Get non zero genes\n    non_zero_genes_names = adata.var_names[adata.varm[\"non_zero\"]]\n\n    # See if gram matrix is full rank\n    gram_full_rank = (\n        np.linalg.matrix_rank(global_gram_matrix) == adata.uns[\"n_params\"]\n    )\n    # If the gram matrix is full rank, share the features vector and the gram\n    # matrix\n\n    shared_state = {\n        \"gram_full_rank\": gram_full_rank,\n        \"n_non_zero_genes\": len(non_zero_genes_names),\n    }\n\n    if gram_full_rank:\n        # Make log features\n        design = adata.obsm[\"design_matrix\"].values\n        log_counts = np.log(\n            adata[:, non_zero_genes_names].layers[\"normed_counts\"] + 0.1\n        )\n        log_features = (design.T @ log_counts).T\n        shared_state.update(\n            {\n                \"local_log_features\": log_features,\n                \"global_gram_matrix\": global_gram_matrix,\n            }\n        )\n    else:\n        # TODO: check that this is correctly recomputed in refit mode\n        if \"normed_log_means\" not in adata.varm:\n            with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n                log_counts = np.log(adata.layers[\"normed_counts\"])\n                adata.varm[\"normed_log_means\"] = log_counts.mean(0)\n        normed_log_means = adata.varm[\"normed_log_means\"]\n        n_obs = adata.n_obs\n        shared_state.update({\"normed_log_means\": normed_log_means, \"n_obs\": n_obs})\n    return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.LocSaveLFC","title":"<code>LocSaveLFC</code>","text":"<p>Mixin to create the local quantities to compute the final hat matrix.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> <code>num_jobs</code> <code>int</code> <p>The number of cpus to use.</p> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity of the joblib backend.</p> <code>joblib_backend</code> <code>str</code> <p>The backend to use for the joblib parallelization.</p> <code>irls_batch_size</code> <code>int</code> <p>The batch size to use for the IRLS algorithm.</p> <code>min_mu</code> <code>float</code> <p>The minimum value for the mu parameter.</p> <p>Methods:</p> Name Description <code>make_local_final_hat_matrix_summands</code> <p>A remote_data method. Creates the local quantities to compute the final hat matrix, which must be computed on all genes. This step is expected to be applied after catching the IRLS method with the fed prox quasi newton method, and takes as an input a shared state from the last iteration of that method.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>class LocSaveLFC:\n    \"\"\"Mixin to create the local quantities to compute the final hat matrix.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        The local AnnData object.\n    num_jobs : int\n        The number of cpus to use.\n    joblib_verbosity : int\n        The verbosity of the joblib backend.\n    joblib_backend : str\n        The backend to use for the joblib parallelization.\n    irls_batch_size : int\n        The batch size to use for the IRLS algorithm.\n    min_mu : float\n        The minimum value for the mu parameter.\n\n    Methods\n    -------\n    make_local_final_hat_matrix_summands\n        A remote_data method. Creates the local quantities to compute the\n        final hat matrix, which must be computed on all genes. This step\n        is expected to be applied after catching the IRLS method\n        with the fed prox quasi newton method, and takes as an input a\n        shared state from the last iteration of that method.\n    \"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n    min_mu: float\n    irls_num_iter: int\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def save_lfc_to_local(\n        self,\n        data_from_opener: AnnData,\n        shared_state: dict[str, Any],\n        refit_mode: bool = False,\n    ):\n        \"\"\"Create the local quantities to compute the final hat matrix.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            The shared state.\n            The shared state is a dictionary containing the following\n            keys:\n            - beta: ndarray\n                The current beta, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the irsl method has diverged.\n                In that case, these genes are caught with the fed prox newton\n                method.\n                (shape: (n_non_zero_genes,)).\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the fed prox newton method has\n                diverged. These genes are not caught by any method, and the\n                returned beta value is the output of the PQN method, even\n                though it has not converged.\n\n        refit_mode : bool\n            Whether to run the pipeline on `refit_adata` instead of `local_adata`.\n            (default: False).\n        \"\"\"\n        beta = shared_state[\"beta\"]\n\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # TODO keeping this in memory for now, see if need for removal at the end\n        adata.uns[\"_irls_diverged_mask\"] = shared_state[\"irls_diverged_mask\"]\n        adata.uns[\"_PQN_diverged_mask\"] = shared_state[\"PQN_diverged_mask\"]\n\n        # Get the param names stored in the local adata\n        mu_param_name = adata.uns[\"_irls_mu_param_name\"]\n        beta_param_name = adata.uns[\"_irls_beta_param_name\"]\n        # ---- Step 2: Store the mu, the diagonal of the hat matrix  ---- #\n        # ----           and beta in the adata                       ---- #\n\n        design_column_names = adata.obsm[\"design_matrix\"].columns\n\n        non_zero_genes_names = adata.var_names[adata.varm[\"non_zero\"]]\n\n        beta_dataframe = pd.DataFrame(\n            np.NaN, index=adata.var_names, columns=design_column_names\n        )\n        beta_dataframe.loc[non_zero_genes_names, :] = beta\n\n        adata.varm[beta_param_name] = beta_dataframe\n\n        if mu_param_name is not None:\n            set_mu_layer(\n                local_adata=adata,\n                lfc_param_name=beta_param_name,\n                mu_param_name=mu_param_name,\n                n_jobs=self.num_jobs,\n                joblib_verbosity=self.joblib_verbosity,\n                joblib_backend=self.joblib_backend,\n                batch_size=self.irls_batch_size,\n            )\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.substeps.LocSaveLFC.save_lfc_to_local","title":"<code>save_lfc_to_local(data_from_opener, shared_state, refit_mode=False)</code>","text":"<p>Create the local quantities to compute the final hat matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>The shared state. The shared state is a dictionary containing the following keys: - beta: ndarray     The current beta, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if the irsl method has diverged.     In that case, these genes are caught with the fed prox newton     method.     (shape: (n_non_zero_genes,)). - PQN_diverged_mask: ndarray     A boolean mask indicating if the fed prox newton method has     diverged. These genes are not caught by any method, and the     returned beta value is the output of the PQN method, even     though it has not converged.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run the pipeline on <code>refit_adata</code> instead of <code>local_adata</code>. (default: False).</p> <code>False</code> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef save_lfc_to_local(\n    self,\n    data_from_opener: AnnData,\n    shared_state: dict[str, Any],\n    refit_mode: bool = False,\n):\n    \"\"\"Create the local quantities to compute the final hat matrix.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        The shared state.\n        The shared state is a dictionary containing the following\n        keys:\n        - beta: ndarray\n            The current beta, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the irsl method has diverged.\n            In that case, these genes are caught with the fed prox newton\n            method.\n            (shape: (n_non_zero_genes,)).\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the fed prox newton method has\n            diverged. These genes are not caught by any method, and the\n            returned beta value is the output of the PQN method, even\n            though it has not converged.\n\n    refit_mode : bool\n        Whether to run the pipeline on `refit_adata` instead of `local_adata`.\n        (default: False).\n    \"\"\"\n    beta = shared_state[\"beta\"]\n\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # TODO keeping this in memory for now, see if need for removal at the end\n    adata.uns[\"_irls_diverged_mask\"] = shared_state[\"irls_diverged_mask\"]\n    adata.uns[\"_PQN_diverged_mask\"] = shared_state[\"PQN_diverged_mask\"]\n\n    # Get the param names stored in the local adata\n    mu_param_name = adata.uns[\"_irls_mu_param_name\"]\n    beta_param_name = adata.uns[\"_irls_beta_param_name\"]\n    # ---- Step 2: Store the mu, the diagonal of the hat matrix  ---- #\n    # ----           and beta in the adata                       ---- #\n\n    design_column_names = adata.obsm[\"design_matrix\"].columns\n\n    non_zero_genes_names = adata.var_names[adata.varm[\"non_zero\"]]\n\n    beta_dataframe = pd.DataFrame(\n        np.NaN, index=adata.var_names, columns=design_column_names\n    )\n    beta_dataframe.loc[non_zero_genes_names, :] = beta\n\n    adata.varm[beta_param_name] = beta_dataframe\n\n    if mu_param_name is not None:\n        set_mu_layer(\n            local_adata=adata,\n            lfc_param_name=beta_param_name,\n            mu_param_name=mu_param_name,\n            n_jobs=self.num_jobs,\n            joblib_verbosity=self.joblib_verbosity,\n            joblib_backend=self.joblib_backend,\n            batch_size=self.irls_batch_size,\n        )\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.utils","title":"<code>utils</code>","text":"<p>Module to implement the utilities of the IRLS algorithm.</p> <p>Most of these functions have the _batch suffix, which means that they are vectorized to work over batches of genes in the parralel_backend file in the same module.</p>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.compute_lfc.utils.make_irls_nll_batch","title":"<code>make_irls_nll_batch(beta, design_matrix, size_factors, dispersions, counts, min_mu=0.5)</code>","text":"<p>Compute the negative binomial log likelihood from LFC estimates.</p> <p>Used in ComputeLFC to compute the deviance score. This function is vectorized to work over batches of genes.</p> <p>Parameters:</p> Name Type Description Default <code>beta</code> <code>ndarray</code> <p>Current LFC estimate, of shape (batch_size, n_params).</p> required <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <code>dispersions</code> <code>ndarray</code> <p>The dispersions, of shape (batch_size).</p> required <code>counts</code> <code>ndarray</code> <p>The counts, of shape (n_obs,batch_size).</p> required <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability. (default: <code>0.5</code>).</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Local negative binomial log-likelihoods, of shape (batch_size).</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/compute_lfc/utils.py</code> <pre><code>def make_irls_nll_batch(\n    beta: np.ndarray,\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n    dispersions: np.ndarray,\n    counts: np.ndarray,\n    min_mu: float = 0.5,\n) -&gt; np.ndarray:\n    \"\"\"Compute the negative binomial log likelihood from LFC estimates.\n\n    Used in ComputeLFC to compute the deviance score. This function is vectorized to\n    work over batches of genes.\n\n    Parameters\n    ----------\n    beta : np.ndarray\n        Current LFC estimate, of shape (batch_size, n_params).\n    design_matrix : np.ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : np.ndarray\n        The size factors, of shape (n_obs).\n    dispersions : np.ndarray\n        The dispersions, of shape (batch_size).\n    counts : np.ndarray\n        The counts, of shape (n_obs,batch_size).\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n        (default: ``0.5``).\n\n    Returns\n    -------\n    np.ndarray\n        Local negative binomial log-likelihoods, of shape\n        (batch_size).\n    \"\"\"\n    mu = np.maximum(\n        size_factors[:, None] * np.exp(design_matrix @ beta.T),\n        min_mu,\n    )\n    return grid_nb_nll(\n        counts,\n        mu,\n        dispersions,\n    )\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions","title":"<code>deseq2_lfc_dispersions</code>","text":""},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions.DESeq2LFCDispersions","title":"<code>DESeq2LFCDispersions</code>","text":"<p>               Bases: <code>ComputeGenewiseDispersions</code>, <code>ComputeDispersionPrior</code>, <code>ComputeMAPDispersions</code>, <code>ComputeLFC</code></p> <p>Mixin class to compute the log fold change and the dispersions with DESeq2.</p> <p>This class encapsulates the steps to compute the log fold change and the dispersions from a given count matrix and a design matrix.</p> <p>Methods:</p> Name Description <code>run_deseq2_lfc_dispersions</code> <p>The method to compute the log fold change and the dispersions. It starts from the design matrix and the count matrix. It returns the shared states by the local nodes after the computation of Cook's distances. It is meant to be run two times in the main pipeline if Cook's refitting is applied/</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions.py</code> <pre><code>class DESeq2LFCDispersions(\n    ComputeGenewiseDispersions,\n    ComputeDispersionPrior,\n    ComputeMAPDispersions,\n    ComputeLFC,\n):\n    \"\"\"Mixin class to compute the log fold change and the dispersions with DESeq2.\n\n    This class encapsulates the steps to compute the log fold change and the\n    dispersions from a given count matrix and a design matrix.\n\n    Methods\n    -------\n    run_deseq2_lfc_dispersions\n        The method to compute the log fold change and the dispersions.\n        It starts from the design matrix and the count matrix.\n        It returns the shared states by the local nodes after the computation of Cook's\n        distances.\n        It is meant to be run two times in the main pipeline if Cook's refitting\n        is applied/\n    \"\"\"\n\n    @log_organisation_method\n    def run_deseq2_lfc_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        gram_features_shared_states,\n        round_idx,\n        clean_models,\n        refit_mode=False,\n    ):\n        \"\"\"Run the DESeq2 pipeline to compute the log fold change and the dispersions.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[dict]\n            Local states. Required to propagate intermediate results.\n\n        gram_features_shared_states: list[dict]\n            Output of the \"compute_size_factor step\" if refit_mode is False.\n            Output of the \"replace_outliers\" step if refit_mode is True.\n            In both cases, contains a \"local_features\" key with the features vector\n            to input to compute_genewise_dispersion.\n            In the non refit mode case, it also contains a \"local_gram_matrix\" key\n             with the local gram matrix.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        refit_mode: bool\n            Whether we are refittinh Cooks outliers or not.\n\n\n        Returns\n        -------\n        local_states: dict\n            Local states updated with the results of the DESeq2 pipeline.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        #### Fit genewise dispersions ####\n\n        # Note : for optimization purposes, we could avoid two successive local\n        # steps here, at the cost of a more complex initialization of the\n        # fit_dispersions method.\n        logger.info(\"Fit genewise dispersions...\")\n        (\n            local_states,\n            genewise_dispersions_shared_state,\n            round_idx,\n        ) = self.fit_genewise_dispersions(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            gram_features_shared_states=gram_features_shared_states,\n            local_states=local_states,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=refit_mode,\n        )\n        logger.info(\"Finished fitting genewise dispersions.\")\n\n        if not refit_mode:\n            #### Fit dispersion trends ####\n            logger.info(\"Compute dispersion prior...\")\n            (\n                local_states,\n                dispersion_trend_share_state,\n                round_idx,\n            ) = self.compute_dispersion_prior(\n                train_data_nodes,\n                aggregation_node,\n                local_states,\n                genewise_dispersions_shared_state,\n                round_idx,\n                clean_models,\n            )\n            logger.info(\"Finished computing dispersion prior.\")\n        else:\n            # Just update the fitted dispersions\n            (\n                local_states,\n                dispersion_trend_share_state,\n                round_idx,\n            ) = local_step(\n                local_method=self.loc_update_fitted_dispersions,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                round_idx=round_idx,\n                input_local_states=local_states,\n                input_shared_state=genewise_dispersions_shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Update fitted dispersions\",\n                clean_models=clean_models,\n            )\n\n        #### Fit MAP dispersions ####\n        logger.info(\"Fit MAP dispersions...\")\n        (\n            local_states,\n            round_idx,\n        ) = self.fit_MAP_dispersions(\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            local_states=local_states,\n            shared_state=dispersion_trend_share_state if not refit_mode else None,\n            round_idx=round_idx,\n            clean_models=clean_models,\n            refit_mode=refit_mode,\n        )\n        logger.info(\"Finished fitting MAP dispersions.\")\n\n        #### Compute log fold changes ####\n        logger.info(\"Compute log fold changes...\")\n        local_states, round_idx = self.compute_lfc(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=True,\n            lfc_mode=\"lfc\",\n            refit_mode=refit_mode,\n        )\n        logger.info(\"Finished computing log fold changes.\")\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions.DESeq2LFCDispersions.run_deseq2_lfc_dispersions","title":"<code>run_deseq2_lfc_dispersions(train_data_nodes, aggregation_node, local_states, gram_features_shared_states, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Run the DESeq2 pipeline to compute the log fold change and the dispersions.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>gram_features_shared_states</code> <p>Output of the \"compute_size_factor step\" if refit_mode is False. Output of the \"replace_outliers\" step if refit_mode is True. In both cases, contains a \"local_features\" key with the features vector to input to compute_genewise_dispersion. In the non refit mode case, it also contains a \"local_gram_matrix\" key  with the local gram matrix.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>refit_mode</code> <p>Whether we are refittinh Cooks outliers or not.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states updated with the results of the DESeq2 pipeline.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions.py</code> <pre><code>@log_organisation_method\ndef run_deseq2_lfc_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    gram_features_shared_states,\n    round_idx,\n    clean_models,\n    refit_mode=False,\n):\n    \"\"\"Run the DESeq2 pipeline to compute the log fold change and the dispersions.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[dict]\n        Local states. Required to propagate intermediate results.\n\n    gram_features_shared_states: list[dict]\n        Output of the \"compute_size_factor step\" if refit_mode is False.\n        Output of the \"replace_outliers\" step if refit_mode is True.\n        In both cases, contains a \"local_features\" key with the features vector\n        to input to compute_genewise_dispersion.\n        In the non refit mode case, it also contains a \"local_gram_matrix\" key\n         with the local gram matrix.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    refit_mode: bool\n        Whether we are refittinh Cooks outliers or not.\n\n\n    Returns\n    -------\n    local_states: dict\n        Local states updated with the results of the DESeq2 pipeline.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    #### Fit genewise dispersions ####\n\n    # Note : for optimization purposes, we could avoid two successive local\n    # steps here, at the cost of a more complex initialization of the\n    # fit_dispersions method.\n    logger.info(\"Fit genewise dispersions...\")\n    (\n        local_states,\n        genewise_dispersions_shared_state,\n        round_idx,\n    ) = self.fit_genewise_dispersions(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        gram_features_shared_states=gram_features_shared_states,\n        local_states=local_states,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        refit_mode=refit_mode,\n    )\n    logger.info(\"Finished fitting genewise dispersions.\")\n\n    if not refit_mode:\n        #### Fit dispersion trends ####\n        logger.info(\"Compute dispersion prior...\")\n        (\n            local_states,\n            dispersion_trend_share_state,\n            round_idx,\n        ) = self.compute_dispersion_prior(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            genewise_dispersions_shared_state,\n            round_idx,\n            clean_models,\n        )\n        logger.info(\"Finished computing dispersion prior.\")\n    else:\n        # Just update the fitted dispersions\n        (\n            local_states,\n            dispersion_trend_share_state,\n            round_idx,\n        ) = local_step(\n            local_method=self.loc_update_fitted_dispersions,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=genewise_dispersions_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Update fitted dispersions\",\n            clean_models=clean_models,\n        )\n\n    #### Fit MAP dispersions ####\n    logger.info(\"Fit MAP dispersions...\")\n    (\n        local_states,\n        round_idx,\n    ) = self.fit_MAP_dispersions(\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        local_states=local_states,\n        shared_state=dispersion_trend_share_state if not refit_mode else None,\n        round_idx=round_idx,\n        clean_models=clean_models,\n        refit_mode=refit_mode,\n    )\n    logger.info(\"Finished fitting MAP dispersions.\")\n\n    #### Compute log fold changes ####\n    logger.info(\"Compute log fold changes...\")\n    local_states, round_idx = self.compute_lfc(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=True,\n        lfc_mode=\"lfc\",\n        refit_mode=refit_mode,\n    )\n    logger.info(\"Finished computing log fold changes.\")\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/deseq2_lfc_dispersions/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 0 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 0 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server 1 rough_dispersions nparray \\((G,)\\) A rough estimate of the dispersions obtained by essentially averaging local rough dispersions (\\(\\alpha^{\\texttt{MoM},2}_g = \\max(\\tfrac{1}{n-p}\\sum_k(\\alpha^{\\texttt{MoM},2}_g)^{(k)},0)\\)) Server Center 2 local_inverse_size_mean float \\(()\\) The average of the inverse of the size factors in a center \\(\\overline{1/\\gamma}^{(k)} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k} 1/\\gamma^{(k)}_{i}\\). Each center Server 2 local_counts_mean nparray \\((G,)\\) The averag e of the counts in a center, per gene \\(\\overline{Y}^{(k)}_{g} = \\frac{1}{n_k}\\sum_{i=1}^{n_k}Y^{(k)}_{ig}\\). Each center Server 2 local_squared_squared_mean nparray \\((G,)\\) The average of the square normed counts in a center, per gene \\(\\overline{Y^2} := \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}(Y^{(k)}_{ig})^2\\). Each center Server 2 local_n_obs int The number of observations in a center \\(n_k\\). Each center Server 2 rough_dispersions nparray \\((G,)\\) The rough estimates computed at the previous step and passed on. Each center Server 3 tot_counts_mean nparray \\((G,)\\) The average of the counts across all centers, per gene, obtained by summing the local counts means. Server Center 3 tot_num_samples int The total number of samples across all centers \\(n\\). Server Center 3 MoM_dispersions nparray \\((G,)\\) The dispersions obtained by the method of moments. First, the aggregated mean counts, squared mean counts and inverse size factors are computed (\\(\\overline{1/\\gamma} = \\sum_k{\\tfrac{n_k}{n}~\\overline{1/\\gamma}^{(k)}) }\\), \\(\\overline{Y}_g = \\sum_{k}{\\tfrac{n_k}{n}~\\overline{Y}^{(k)}_{g}}\\), \\(\\overline{Y^2} = \\sum_{k}{\\tfrac{n_k}{n}~\\overline{Y^2}^{(k)}_{g})}\\)). The variance is then estimated with \\(\\widehat{\\sigma}^2_g = \\tfrac{n}{n-1} (\\overline{Y^2}_g - \\overline{Y}_g^2)\\) and a method of moments estimate is the constructed as \\(\\alpha^{\\texttt{MoM},1}_g = \\tfrac{\\widehat{\\sigma}^2_g - \\overline{1/\\gamma}_g \\times \\overline{Y}_g}{\\overline{Y}_g^2}\\). The final MoM estimate \\(\\alpha^{\\texttt{MoM}}_g\\) for gene \\(g\\) is the clipped value of \\(\\min(\\alpha^{\\texttt{MoM},1}_g, \\alpha^{\\texttt{MoM},2}_g)\\) between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\). Server Center 3 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server Center 5 gram_full_rank bool \\(()\\) A boolean indicating whether the global Gramm matrix \\(G\\) is full rank. Each center Server 5 n_non_zero_genes int The number of genes with non-zero counts in at least one center. Each center Server 5 local_log_features nparray \\((G_{\\texttt{nz}}, p)\\) \\(\\Phi_{k} = (X^{(k)})^{\\top} \\log(Z^{(k)}+0.1)\\). Each center Server 5 global_gram_matrix nparray \\((p, p)\\) The sum of the local Gramm matrix across all centers \\(G = \\sum_k G^{(k)}\\). Each center Server 6 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 6 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 6 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 6 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 6 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 7 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 7 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 7 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 7 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 7 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 8 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 8 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 8 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 10 unique_counts Series \\((p,)\\) A pandas series indexed by unique level combinations (all design factor levels that coexist) and containing the number of samples in each level combination. In the single factor case the index will simply be the unique levels of the factor. Each center Server 11 counts_by_lvl Series \\((L,)\\) A pandas series indexed by the unique levels of the design factors across all centers, and whose values are the number of samples in each level combination. Server Center 14 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 14 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 14 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 14 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 14 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 15 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 15 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 15 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 16 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 16 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 16 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 16 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 16 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 17 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 17 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 17 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 18 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 18 n_obs int The number of samples in the center. Each center Server 18 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 18 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\), as computed by the grid search. Each center Server 18 n_params int The number of parameters in the design matrix, i.e., its number of columns. Each center Server 19 disp_function_type str The type of the dispersion function used to model the dispersions. It can be either \"parametric\", if the iterative scheme to fit the trend curve has converged, the LBFGS-B method used to fit the parameters has converged, and the coefficienst of the trend curve are non-negative; or \"mean\" otherwise. Server Center 19 mean_disp NoneType None if the dispersion function type is \"parametric\", and the trimmed mean of the genewise dispersions whose value is above \\(10\\times \\texttt{min\\_disp}\\), with trimming proportion \\(0.001\\) otherwise. Server Center 19 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server Center 19 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server Center 19 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server Center 19 trend_coeffs nparray \\((2,)\\) The coefficients of the trend curve fitted to the dispersions. We model the dispersions \\(\\alpha_g\\) of the gene \\(g\\) as a sample from an exponential distribution whose mean \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is a function of \\(\\overline{Z}_g\\) parametrized by two coefficients \\(\\alpha_0\\) and \\(\\text{a}_1\\); \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g) = \\alpha_0 + \\tfrac{\\text{a}_1}{\\overline{Z}_g}\\). \\(\\alpha_{\\texttt{trend}}\\) is called the trend curve. The coefficients are \\((\\alpha_0, \\text{a}_1)\\), and are obtained by starting with the set of non zero genes  and by iteratively i) minimizing the negative log likelihood of the exponential distribution on the set of genes with LBFGS-B and ii) removing the genes where the ratio between the dispersion \\(\\alpha_g\\) and the trend curve \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is above \\(15\\) or below \\(10^{-4}\\) (we consider these genes as outliers to this model), until the set of genes is stable (see \\citep{love2014deseq2} for more details). Server Center 20 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 20 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 20 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 20 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 20 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 20 reg nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), a regularization term that is equal to \\(0.5 \\times (\\log(\\alpha_g) - \\log(\\alpha^{\\texttt{trend}}_g))^2/\\sigma_{\\texttt{trend}}^2\\), and which comes from the prior on the distribution of the log dispersions around the trend curve. Each center Server 21 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 21 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 21 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 22 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 22 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 22 reg nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), a regularization term that is equal to \\(0.5 \\times (\\log(\\alpha_g) - \\log(\\alpha^{\\texttt{trend}}_g))^2/\\sigma_{\\texttt{trend}}^2\\), and which comes from the prior on the distribution of the log dispersions around the trend curve. Each center Server 22 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 22 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 22 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 23 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 23 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 23 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 25 gram_full_rank bool \\(()\\) A boolean indicating whether the global Gramm matrix \\(G\\) is full rank. Each center Server 25 n_non_zero_genes int The number of genes with non-zero counts in at least one center. Each center Server 25 local_log_features nparray \\((G_{\\texttt{nz}}, p)\\) \\(\\Phi_{k} = (X^{(k)})^{\\top} \\log(Z^{(k)}+0.1)\\). Each center Server 25 global_gram_matrix nparray \\((p, p)\\) The sum of the local Gramm matrix across all centers \\(G = \\sum_k G^{(k)}\\). Each center Server 26 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 26 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 26 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 26 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 26 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 27 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood at the \\(\\beta_g\\) parameter. Simply passed on. Each center Server 27 round_number_irls int The current round number of the IRLS algorithm. Simply passed on. Each center Server 27 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Simply passed on. Each center Server 27 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on. Each center Server 27 beta nparray \\((G_{\\texttt{nz}}, p)\\) The log fold change \\(\\beta\\) parameter. Not modified. Each center Server 27 local_features nparray \\((G_{\\texttt{act}}, p)\\) For each active gene \\(g\\), the projected features \\((X^{(k)})^{\\top} W^{(k)}_g \\phi^{(k)}_{g}\\), where \\(\\phi^{(k)}_{ig} = \\log\\left(\\frac{\\mu^{(k)}_{ig}}{\\gamma_{i}}\\right) + \\frac{Y^{(k)}_{ig} - \\mu^{(k)}_{ig}}{\\mu^{(k)}_{ig}}\\) and \\(\\phi^{(k)}_{g}\\) is the vector of \\((\\phi^{(k)}_{ig})_{1 \\leq i \\leq n_k}\\). Each center Server 27 local_hat_matrix nparray \\((G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 27 local_nll nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data. Each center Server 27 irls_gene_names Index \\((G_{\\texttt{act}},)\\) The gene names of the active genes. Each center Server 28 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 28 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 28 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 28 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 28 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 29 ascent_direction_on_mask NoneType The ascent direction on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 29 beta nparray \\((G_{\\texttt{nz}}, p)\\) For evert gene \\(g\\), the inital value of the \\(\\beta_g\\) parameter for the ProxQuasiNewton algorithm (PQN), which is either i) the value computed by IRLS if IRLS has converged on that gene (in that case, the gene will not be optimized), or ii) the initial value of the \\(\\beta_g\\) parameter for the IRLS algorithm, if IRLS has not converged on that gene. Each center Server 29 local_nll nparray \\((1, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\). Each center Server 29 local_fisher nparray \\((1, G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion, \\(n_k\\mathcal{I}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{gii} = \\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 29 local_gradient nparray \\((1, G_{\\texttt{act}}, p)\\) For each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, i.e., \\(\\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g) =   -(X^{(k)})^{\\top} Y^{(k)}_{g} + (X^{(k)})^{\\top} \\left( \\frac{1}{\\alpha_g} + Y^{(k)}_{g} \\right) \\frac{\\mu^{(k)}_{g}}{\\frac{1}{\\alpha_g} + \\mu^{(k)}_{g}}\\), where \\(\\mu^{(k)}_{g}\\) and \\(Y^{(k)}_{g}\\) are both vectors in \\(\\mathbb{R}^{n_k}\\) and \\(\\alpha_g\\) is a scalar. TODO ref equation. Each center Server 29 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Initialized to \\(\\texttt{False}\\). Each center Server 29 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). The genes which have diverged during the IRLS algorithm and those which have not converged at the end of the prescribed iterations are set to \\(\\texttt{True}\\). Each center Server 29 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, intialized at \\(\\texttt{np.nan}\\) since no nll has been computed at this stage. Each center Server 29 newton_decrement_on_mask NoneType The newton decrement on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 29 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, initialized at \\(0\\). Each center Server 29 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on during the Fed Proximal Quasi-Newton algorithm, since the server is stateless. Each center Server 30 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 30 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Server Center 30 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 30 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 30 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Server Center 30 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes. To compute the ascent direction, we first compute the global Fisher information matrix for each gene \\(g\\), \\(n \\mathcal{I}_g = \\sum_k n_k \\mathcal{I}^{(k)}_{g}\\) and the global gradient \\(\\nabla_{\\beta} \\mathcal{L}(\\beta_g) = \\sum_k \\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g)\\). From these two quantities, we build the Fisher information and gradients of the regularized negative log likelihood, with \\(L^2\\) regularization \\(\\lambda = 10^{-6}\\). We add another regularization term to the Fisher matrix which depends on the iteration number. The goal is for the ascent direction to be close to the gradient for small iterations, and to be close to the real natural gradient descent update near the end of the optimization. The ascent direction is then computed as from the regularized Fisher and gradient (see main paper). Roughly, it is the solution of the linear system \\(n~\\mathcal{I}^{\\lambda}_g \\Delta_g = \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\), where certain components are dropped in to handle boundary conditions as the optimization is constrained to values of \\(\\beta_g\\) in \\([-\\texttt{max\\_beta},\\texttt{max\\_beta}]\\). Server Center 30 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which remains unchanged during the first iteration of the Proximal Quasi-Newton algorithm. Server Center 30 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes. It is computed as \\(\\nu_g = \\Delta_g  \\cdot \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) where \\(\\Delta_g\\) is the ascent direction on the active genes and \\(\\mathcal{L}^{\\lambda}\\) is the regularized log likelihood. Server Center 31 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 31 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 31 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 31 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 31 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 31 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 31 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 31 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 31 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 31 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 31 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 32 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 32 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 32 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 32 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. For more details, see description of 19. Server Center 32 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. A gene \\(g\\) is considered to have diverged if for all step sizes \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and for the computed ascent direction \\(\\Delta_g\\), the Armijo condition is not satisfied, i.e., if \\(\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_g - \\delta \\Delta_g) \\leq \\delta ~ \\texttt{PQN\\_c1} ~ \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g) \\cdot \\Delta_g\\) where \\(\\mathcal{L}^{\\lambda}\\) is the regularized negative log likelihood, \\(\\texttt{PQN\\_c1}\\) is the Armijo parameter which can be set by the user (default is \\(10^{-4}\\)), and \\(\\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) is the gradient of the regularized negative log likelihood. Server Center 32 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). Genes that had converged with the relative error criterion or diverged (no stepsize satisfying the Armijo condition) have set to \\(\\texttt{False}\\). Server Center 32 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 32 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. Server Center 33 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 33 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 33 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 33 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 33 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 33 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 33 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 33 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 33 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 33 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 33 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 34 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 34 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 34 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center"},{"location":"api/core/deseq2_core/deseq2_lfc_dispersions/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 0 local_gram_matrix nparray \\((p, p)\\) The gram matrix of the local design matrix \\(G^{(k)} := (X^{(k)})^{\\top}X^{(k)}\\). Each center Server 0 local_features nparray \\((p, G)\\) \\(\\Phi^{(k)}  := X^{(k)\\top} Z^{(k)}\\). Each center Server 1 rough_dispersions nparray \\((G,)\\) A rough estimate of the dispersions obtained by essentially averaging local rough dispersions (\\(\\alpha^{\\texttt{MoM},2}_g = \\max(\\tfrac{1}{n-p}\\sum_k(\\alpha^{\\texttt{MoM},2}_g)^{(k)},0)\\)) Server Center 2 local_inverse_size_mean float \\(()\\) The average of the inverse of the size factors in a center \\(\\overline{1/\\gamma}^{(k)} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k} 1/\\gamma^{(k)}_{i}\\). Each center Server 2 local_counts_mean nparray \\((G,)\\) The averag e of the counts in a center, per gene \\(\\overline{Y}^{(k)}_{g} = \\frac{1}{n_k}\\sum_{i=1}^{n_k}Y^{(k)}_{ig}\\). Each center Server 2 local_squared_squared_mean nparray \\((G,)\\) The average of the square normed counts in a center, per gene \\(\\overline{Y^2} := \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}(Y^{(k)}_{ig})^2\\). Each center Server 2 local_n_obs int The number of observations in a center \\(n_k\\). Each center Server 2 rough_dispersions nparray \\((G,)\\) The rough estimates computed at the previous step and passed on. Each center Server 3 tot_counts_mean nparray \\((G,)\\) The average of the counts across all centers, per gene, obtained by summing the local counts means. Server Center 3 tot_num_samples int The total number of samples across all centers \\(n\\). Server Center 3 MoM_dispersions nparray \\((G,)\\) The dispersions obtained by the method of moments. First, the aggregated mean counts, squared mean counts and inverse size factors are computed (\\(\\overline{1/\\gamma} = \\sum_k{\\tfrac{n_k}{n}~\\overline{1/\\gamma}^{(k)}) }\\), \\(\\overline{Y}_g = \\sum_{k}{\\tfrac{n_k}{n}~\\overline{Y}^{(k)}_{g}}\\), \\(\\overline{Y^2} = \\sum_{k}{\\tfrac{n_k}{n}~\\overline{Y^2}^{(k)}_{g})}\\)). The variance is then estimated with \\(\\widehat{\\sigma}^2_g = \\tfrac{n}{n-1} (\\overline{Y^2}_g - \\overline{Y}_g^2)\\) and a method of moments estimate is the constructed as \\(\\alpha^{\\texttt{MoM},1}_g = \\tfrac{\\widehat{\\sigma}^2_g - \\overline{1/\\gamma}_g \\times \\overline{Y}_g}{\\overline{Y}_g^2}\\). The final MoM estimate \\(\\alpha^{\\texttt{MoM}}_g\\) for gene \\(g\\) is the clipped value of \\(\\min(\\alpha^{\\texttt{MoM},1}_g, \\alpha^{\\texttt{MoM},2}_g)\\) between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\). Server Center 3 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server Center 5 gram_full_rank bool \\(()\\) A boolean indicating whether the global Gramm matrix \\(G\\) is full rank. Each center Server 5 n_non_zero_genes int The number of genes with non-zero counts in at least one center. Each center Server 5 local_log_features nparray \\((G_{\\texttt{nz}}, p)\\) \\(\\Phi_{k} = (X^{(k)})^{\\top} \\log(Z^{(k)}+0.1)\\). Each center Server 5 global_gram_matrix nparray \\((p, p)\\) The sum of the local Gramm matrix across all centers \\(G = \\sum_k G^{(k)}\\). Each center Server 6 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 6 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 6 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 6 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 6 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 7 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 7 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 7 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 7 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 7 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 8 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 8 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 8 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 10 unique_counts Series \\((p,)\\) A pandas series indexed by unique level combinations (all design factor levels that coexist) and containing the number of samples in each level combination. In the single factor case the index will simply be the unique levels of the factor. Each center Server 11 counts_by_lvl Series \\((L,)\\) A pandas series indexed by the unique levels of the design factors across all centers, and whose values are the number of samples in each level combination. Server Center 14 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 14 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 14 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 14 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 14 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 15 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 15 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 15 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 16 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 16 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 16 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 16 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 16 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 17 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 17 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 17 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 18 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 18 n_obs int The number of samples in the center. Each center Server 18 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 18 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\), as computed by the grid search. Each center Server 18 n_params int The number of parameters in the design matrix, i.e., its number of columns. Each center Server 19 disp_function_type str The type of the dispersion function used to model the dispersions. It can be either \"parametric\", if the iterative scheme to fit the trend curve has converged, the LBFGS-B method used to fit the parameters has converged, and the coefficienst of the trend curve are non-negative; or \"mean\" otherwise. Server Center 19 mean_disp NoneType None if the dispersion function type is \"parametric\", and the trimmed mean of the genewise dispersions whose value is above \\(10\\times \\texttt{min\\_disp}\\), with trimming proportion \\(0.001\\) otherwise. Server Center 19 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server Center 19 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server Center 19 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server Center 19 trend_coeffs nparray \\((2,)\\) The coefficients of the trend curve fitted to the dispersions. We model the dispersions \\(\\alpha_g\\) of the gene \\(g\\) as a sample from an exponential distribution whose mean \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is a function of \\(\\overline{Z}_g\\) parametrized by two coefficients \\(\\alpha_0\\) and \\(\\text{a}_1\\); \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g) = \\alpha_0 + \\tfrac{\\text{a}_1}{\\overline{Z}_g}\\). \\(\\alpha_{\\texttt{trend}}\\) is called the trend curve. The coefficients are \\((\\alpha_0, \\text{a}_1)\\), and are obtained by starting with the set of non zero genes  and by iteratively i) minimizing the negative log likelihood of the exponential distribution on the set of genes with LBFGS-B and ii) removing the genes where the ratio between the dispersion \\(\\alpha_g\\) and the trend curve \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) is above \\(15\\) or below \\(10^{-4}\\) (we consider these genes as outliers to this model), until the set of genes is stable (see \\citep{love2014deseq2} for more details). Server Center 20 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 20 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 20 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 20 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 20 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 20 reg nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), a regularization term that is equal to \\(0.5 \\times (\\log(\\alpha_g) - \\log(\\alpha^{\\texttt{trend}}_g))^2/\\sigma_{\\texttt{trend}}^2\\), and which comes from the prior on the distribution of the log dispersions around the trend curve. Each center Server 21 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 21 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 21 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 22 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 22 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 22 reg nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), a regularization term that is equal to \\(0.5 \\times (\\log(\\alpha_g) - \\log(\\alpha^{\\texttt{trend}}_g))^2/\\sigma_{\\texttt{trend}}^2\\), and which comes from the prior on the distribution of the log dispersions around the trend curve. Each center Server 22 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 22 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 22 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 23 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 23 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 23 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 25 gram_full_rank bool \\(()\\) A boolean indicating whether the global Gramm matrix \\(G\\) is full rank. Each center Server 25 n_non_zero_genes int The number of genes with non-zero counts in at least one center. Each center Server 25 local_log_features nparray \\((G_{\\texttt{nz}}, p)\\) \\(\\Phi_{k} = (X^{(k)})^{\\top} \\log(Z^{(k)}+0.1)\\). Each center Server 25 global_gram_matrix nparray \\((p, p)\\) The sum of the local Gramm matrix across all centers \\(G = \\sum_k G^{(k)}\\). Each center Server 26 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 26 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 26 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 26 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 26 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 27 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood at the \\(\\beta_g\\) parameter. Simply passed on. Each center Server 27 round_number_irls int The current round number of the IRLS algorithm. Simply passed on. Each center Server 27 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Simply passed on. Each center Server 27 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on. Each center Server 27 beta nparray \\((G_{\\texttt{nz}}, p)\\) The log fold change \\(\\beta\\) parameter. Not modified. Each center Server 27 local_features nparray \\((G_{\\texttt{act}}, p)\\) For each active gene \\(g\\), the projected features \\((X^{(k)})^{\\top} W^{(k)}_g \\phi^{(k)}_{g}\\), where \\(\\phi^{(k)}_{ig} = \\log\\left(\\frac{\\mu^{(k)}_{ig}}{\\gamma_{i}}\\right) + \\frac{Y^{(k)}_{ig} - \\mu^{(k)}_{ig}}{\\mu^{(k)}_{ig}}\\) and \\(\\phi^{(k)}_{g}\\) is the vector of \\((\\phi^{(k)}_{ig})_{1 \\leq i \\leq n_k}\\). Each center Server 27 local_hat_matrix nparray \\((G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 27 local_nll nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data. Each center Server 27 irls_gene_names Index \\((G_{\\texttt{act}},)\\) The gene names of the active genes. Each center Server 28 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 28 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 28 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 28 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 28 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 29 ascent_direction_on_mask NoneType The ascent direction on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 29 beta nparray \\((G_{\\texttt{nz}}, p)\\) For evert gene \\(g\\), the inital value of the \\(\\beta_g\\) parameter for the ProxQuasiNewton algorithm (PQN), which is either i) the value computed by IRLS if IRLS has converged on that gene (in that case, the gene will not be optimized), or ii) the initial value of the \\(\\beta_g\\) parameter for the IRLS algorithm, if IRLS has not converged on that gene. Each center Server 29 local_nll nparray \\((1, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\). Each center Server 29 local_fisher nparray \\((1, G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion, \\(n_k\\mathcal{I}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{gii} = \\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 29 local_gradient nparray \\((1, G_{\\texttt{act}}, p)\\) For each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, i.e., \\(\\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g) =   -(X^{(k)})^{\\top} Y^{(k)}_{g} + (X^{(k)})^{\\top} \\left( \\frac{1}{\\alpha_g} + Y^{(k)}_{g} \\right) \\frac{\\mu^{(k)}_{g}}{\\frac{1}{\\alpha_g} + \\mu^{(k)}_{g}}\\), where \\(\\mu^{(k)}_{g}\\) and \\(Y^{(k)}_{g}\\) are both vectors in \\(\\mathbb{R}^{n_k}\\) and \\(\\alpha_g\\) is a scalar. TODO ref equation. Each center Server 29 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Initialized to \\(\\texttt{False}\\). Each center Server 29 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). The genes which have diverged during the IRLS algorithm and those which have not converged at the end of the prescribed iterations are set to \\(\\texttt{True}\\). Each center Server 29 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, intialized at \\(\\texttt{np.nan}\\) since no nll has been computed at this stage. Each center Server 29 newton_decrement_on_mask NoneType The newton decrement on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 29 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, initialized at \\(0\\). Each center Server 29 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on during the Fed Proximal Quasi-Newton algorithm, since the server is stateless. Each center Server 30 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 30 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Server Center 30 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 30 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 30 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Server Center 30 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes. To compute the ascent direction, we first compute the global Fisher information matrix for each gene \\(g\\), \\(n \\mathcal{I}_g = \\sum_k n_k \\mathcal{I}^{(k)}_{g}\\) and the global gradient \\(\\nabla_{\\beta} \\mathcal{L}(\\beta_g) = \\sum_k \\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g)\\). From these two quantities, we build the Fisher information and gradients of the regularized negative log likelihood, with \\(L^2\\) regularization \\(\\lambda = 10^{-6}\\). We add another regularization term to the Fisher matrix which depends on the iteration number. The goal is for the ascent direction to be close to the gradient for small iterations, and to be close to the real natural gradient descent update near the end of the optimization. The ascent direction is then computed as from the regularized Fisher and gradient (see main paper). Roughly, it is the solution of the linear system \\(n~\\mathcal{I}^{\\lambda}_g \\Delta_g = \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\), where certain components are dropped in to handle boundary conditions as the optimization is constrained to values of \\(\\beta_g\\) in \\([-\\texttt{max\\_beta},\\texttt{max\\_beta}]\\). Server Center 30 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which remains unchanged during the first iteration of the Proximal Quasi-Newton algorithm. Server Center 30 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes. It is computed as \\(\\nu_g = \\Delta_g  \\cdot \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) where \\(\\Delta_g\\) is the ascent direction on the active genes and \\(\\mathcal{L}^{\\lambda}\\) is the regularized log likelihood. Server Center 31 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 31 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 31 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 31 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 31 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 31 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 31 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 31 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 31 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 31 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 31 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 32 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 32 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 32 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 32 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. For more details, see description of 19. Server Center 32 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. A gene \\(g\\) is considered to have diverged if for all step sizes \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and for the computed ascent direction \\(\\Delta_g\\), the Armijo condition is not satisfied, i.e., if \\(\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_g - \\delta \\Delta_g) \\leq \\delta ~ \\texttt{PQN\\_c1} ~ \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g) \\cdot \\Delta_g\\) where \\(\\mathcal{L}^{\\lambda}\\) is the regularized negative log likelihood, \\(\\texttt{PQN\\_c1}\\) is the Armijo parameter which can be set by the user (default is \\(10^{-4}\\)), and \\(\\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) is the gradient of the regularized negative log likelihood. Server Center 32 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). Genes that had converged with the relative error criterion or diverged (no stepsize satisfying the Armijo condition) have set to \\(\\texttt{False}\\). Server Center 32 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 32 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. Server Center 33 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 33 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 33 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 33 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 33 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 33 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 33 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 33 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 33 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 33 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 33 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 34 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 34 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 34 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>run_deseq2_stats</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#api","title":"API","text":"<p>Module containing all the necessary steps to perform statistical analysis.</p>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj","title":"<code>compute_padj</code>","text":"<p>Module containing the Mixin to compute adjusted p-values.</p>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.compute_padj","title":"<code>compute_padj</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.compute_padj.ComputeAdjustedPValues","title":"<code>ComputeAdjustedPValues</code>","text":"<p>               Bases: <code>IndependentFiltering</code>, <code>PValueAdjustment</code></p> <p>Mixin class to implement the computation of adjusted p-values.</p> <p>Attributes:</p> Name Type Description <code>independent_filter</code> <code>bool</code> <p>A boolean flag to indicate whether to use independent filtering or not.</p> <p>Methods:</p> Name Description <code>compute_adjusted_p_values</code> <p>A method to compute adjusted p-values. Runs independent filtering if self.independent_filter is True. Runs BH method otherwise.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/compute_padj.py</code> <pre><code>class ComputeAdjustedPValues(IndependentFiltering, PValueAdjustment):\n    \"\"\"Mixin class to implement the computation of adjusted p-values.\n\n    Attributes\n    ----------\n    independent_filter: bool\n        A boolean flag to indicate whether to use independent filtering or not.\n\n    Methods\n    -------\n    compute_adjusted_p_values\n        A method to compute adjusted p-values.\n        Runs independent filtering if self.independent_filter is True.\n        Runs BH method otherwise.\n    \"\"\"\n\n    independent_filter: bool = False\n\n    @log_organisation_method\n    def compute_adjusted_p_values(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        wald_test_shared_state,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Compute adjusted p-values.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        wald_test_shared_state: dict\n            Shared states containing the Wald test results.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            If True, the models are cleaned.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        if self.independent_filter:\n            local_states, _, round_idx = local_step(\n                local_method=self.run_independent_filtering,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                round_idx=round_idx,\n                input_local_states=local_states,\n                input_shared_state=wald_test_shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Compute adjusted P values using independent filtering.\",\n                clean_models=clean_models,\n            )\n        else:\n            local_states, _, round_idx = local_step(\n                local_method=self.run_p_value_adjustment,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                round_idx=round_idx,\n                input_local_states=local_states,\n                input_shared_state=wald_test_shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Compute adjusted P values using BH method.\",\n                clean_models=clean_models,\n            )\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.compute_padj.ComputeAdjustedPValues.compute_adjusted_p_values","title":"<code>compute_adjusted_p_values(train_data_nodes, aggregation_node, local_states, wald_test_shared_state, round_idx, clean_models)</code>","text":"<p>Compute adjusted p-values.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>wald_test_shared_state</code> <p>Shared states containing the Wald test results.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>If True, the models are cleaned.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/compute_padj.py</code> <pre><code>@log_organisation_method\ndef compute_adjusted_p_values(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    wald_test_shared_state,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Compute adjusted p-values.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    wald_test_shared_state: dict\n        Shared states containing the Wald test results.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        If True, the models are cleaned.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    if self.independent_filter:\n        local_states, _, round_idx = local_step(\n            local_method=self.run_independent_filtering,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=wald_test_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute adjusted P values using independent filtering.\",\n            clean_models=clean_models,\n        )\n    else:\n        local_states, _, round_idx = local_step(\n            local_method=self.run_p_value_adjustment,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=wald_test_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute adjusted P values using BH method.\",\n            clean_models=clean_models,\n        )\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.substeps.IndependentFiltering","title":"<code>IndependentFiltering</code>","text":"<p>Mixin class implementing independent filtering.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object.</p> <code>alpha</code> <code>float</code> <p>Significance level.</p> <p>Methods:</p> Name Description <code>run_independent_filtering</code> <p>Run independent filtering on the p-values trend</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/substeps.py</code> <pre><code>class IndependentFiltering:\n    \"\"\"Mixin class implementing independent filtering.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n\n    alpha : float\n        Significance level.\n\n    Methods\n    -------\n    run_independent_filtering\n        Run independent filtering on the p-values trend\n    \"\"\"\n\n    local_adata: AnnData\n    alpha: float\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def run_independent_filtering(self, data_from_opener, shared_state: Any):\n        \"\"\"Run independent filtering on the p-values trend.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state containing the results of the wald tests, namely\n            - \"p_values\" : p-values\n            - \"wald_statistics\" : Wald statistics\n            - \"wald_se\" : Wald standard errors\n        \"\"\"\n        p_values = shared_state[\"p_values\"]\n        wald_statistics = shared_state[\"wald_statistics\"]\n        wald_se = shared_state[\"wald_se\"]\n\n        self.local_adata.varm[\"p_values\"] = p_values\n        self.local_adata.varm[\"wald_statistics\"] = wald_statistics\n        self.local_adata.varm[\"wald_se\"] = wald_se\n\n        base_mean = self.local_adata.varm[\"_normed_means\"]\n\n        lower_quantile = np.mean(base_mean == 0)\n\n        if lower_quantile &lt; 0.95:\n            upper_quantile = 0.95\n        else:\n            upper_quantile = 1\n\n        theta = np.linspace(lower_quantile, upper_quantile, 50)\n        cutoffs = np.quantile(base_mean, theta)\n\n        result = pd.DataFrame(\n            np.nan, index=self.local_adata.var_names, columns=np.arange(len(theta))\n        )\n\n        for i, cutoff in enumerate(cutoffs):\n            use = (base_mean &gt;= cutoff) &amp; (~np.isnan(p_values))\n            U2 = p_values[use]\n            if not len(U2) == 0:\n                result.loc[use, i] = false_discovery_control(U2, method=\"bh\")\n\n        num_rej = (result &lt; self.alpha).sum(0)\n        lowess_res = lowess(theta, num_rej, frac=1 / 5)\n\n        if num_rej.max() &lt;= 10:\n            j = 0\n        else:\n            residual = num_rej[num_rej &gt; 0] - lowess_res[num_rej &gt; 0]\n            thresh = lowess_res.max() - np.sqrt(np.mean(residual**2))\n\n            if np.any(num_rej &gt; thresh):\n                j = np.where(num_rej &gt; thresh)[0][0]\n            else:\n                j = 0\n\n        self.local_adata.varm[\"padj\"] = result.loc[:, j]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.substeps.IndependentFiltering.run_independent_filtering","title":"<code>run_independent_filtering(data_from_opener, shared_state)</code>","text":"<p>Run independent filtering on the p-values trend.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing the results of the wald tests, namely - \"p_values\" : p-values - \"wald_statistics\" : Wald statistics - \"wald_se\" : Wald standard errors</p> required Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef run_independent_filtering(self, data_from_opener, shared_state: Any):\n    \"\"\"Run independent filtering on the p-values trend.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state containing the results of the wald tests, namely\n        - \"p_values\" : p-values\n        - \"wald_statistics\" : Wald statistics\n        - \"wald_se\" : Wald standard errors\n    \"\"\"\n    p_values = shared_state[\"p_values\"]\n    wald_statistics = shared_state[\"wald_statistics\"]\n    wald_se = shared_state[\"wald_se\"]\n\n    self.local_adata.varm[\"p_values\"] = p_values\n    self.local_adata.varm[\"wald_statistics\"] = wald_statistics\n    self.local_adata.varm[\"wald_se\"] = wald_se\n\n    base_mean = self.local_adata.varm[\"_normed_means\"]\n\n    lower_quantile = np.mean(base_mean == 0)\n\n    if lower_quantile &lt; 0.95:\n        upper_quantile = 0.95\n    else:\n        upper_quantile = 1\n\n    theta = np.linspace(lower_quantile, upper_quantile, 50)\n    cutoffs = np.quantile(base_mean, theta)\n\n    result = pd.DataFrame(\n        np.nan, index=self.local_adata.var_names, columns=np.arange(len(theta))\n    )\n\n    for i, cutoff in enumerate(cutoffs):\n        use = (base_mean &gt;= cutoff) &amp; (~np.isnan(p_values))\n        U2 = p_values[use]\n        if not len(U2) == 0:\n            result.loc[use, i] = false_discovery_control(U2, method=\"bh\")\n\n    num_rej = (result &lt; self.alpha).sum(0)\n    lowess_res = lowess(theta, num_rej, frac=1 / 5)\n\n    if num_rej.max() &lt;= 10:\n        j = 0\n    else:\n        residual = num_rej[num_rej &gt; 0] - lowess_res[num_rej &gt; 0]\n        thresh = lowess_res.max() - np.sqrt(np.mean(residual**2))\n\n        if np.any(num_rej &gt; thresh):\n            j = np.where(num_rej &gt; thresh)[0][0]\n        else:\n            j = 0\n\n    self.local_adata.varm[\"padj\"] = result.loc[:, j]\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.substeps.PValueAdjustment","title":"<code>PValueAdjustment</code>","text":"<p>Mixin class implementing p-value adjustment.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object.</p> <p>Methods:</p> Name Description <code>run_p_value_adjustment</code> <p>Run p-value adjustment on the p-values trend using the Benjamini-Hochberg method.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/substeps.py</code> <pre><code>class PValueAdjustment:\n    \"\"\"Mixin class implementing p-value adjustment.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n\n    Methods\n    -------\n    run_p_value_adjustment\n        Run p-value adjustment on the p-values trend using the Benjamini-Hochberg\n        method.\n    \"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def run_p_value_adjustment(self, data_from_opener, shared_state: Any):\n        \"\"\"Run p-value adjustment on the p-values trend using the BH method.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state containing the results of the Wald tests, namely\n            - \"p_values\" : p-values, as a numpy array\n            - \"wald_statistics\" : Wald statistics\n            - \"wald_se\" : Wald standard errors\n        \"\"\"\n        p_values = shared_state[\"p_values\"]\n        wald_statistics = shared_state[\"wald_statistics\"]\n        wald_se = shared_state[\"wald_se\"]\n\n        self.local_adata.varm[\"p_values\"] = p_values\n        self.local_adata.varm[\"wald_statistics\"] = wald_statistics\n        self.local_adata.varm[\"wald_se\"] = wald_se\n\n        padj = pd.Series(np.nan, index=self.local_adata.var_names)\n        padj.loc[~np.isnan(p_values)] = false_discovery_control(\n            p_values[~np.isnan(p_values)], method=\"bh\"\n        )\n\n        self.local_adata.varm[\"padj\"] = padj\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.compute_padj.substeps.PValueAdjustment.run_p_value_adjustment","title":"<code>run_p_value_adjustment(data_from_opener, shared_state)</code>","text":"<p>Run p-value adjustment on the p-values trend using the BH method.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing the results of the Wald tests, namely - \"p_values\" : p-values, as a numpy array - \"wald_statistics\" : Wald statistics - \"wald_se\" : Wald standard errors</p> required Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/compute_padj/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef run_p_value_adjustment(self, data_from_opener, shared_state: Any):\n    \"\"\"Run p-value adjustment on the p-values trend using the BH method.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state containing the results of the Wald tests, namely\n        - \"p_values\" : p-values, as a numpy array\n        - \"wald_statistics\" : Wald statistics\n        - \"wald_se\" : Wald standard errors\n    \"\"\"\n    p_values = shared_state[\"p_values\"]\n    wald_statistics = shared_state[\"wald_statistics\"]\n    wald_se = shared_state[\"wald_se\"]\n\n    self.local_adata.varm[\"p_values\"] = p_values\n    self.local_adata.varm[\"wald_statistics\"] = wald_statistics\n    self.local_adata.varm[\"wald_se\"] = wald_se\n\n    padj = pd.Series(np.nan, index=self.local_adata.var_names)\n    padj.loc[~np.isnan(p_values)] = false_discovery_control(\n        p_values[~np.isnan(p_values)], method=\"bh\"\n    )\n\n    self.local_adata.varm[\"padj\"] = padj\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering","title":"<code>cooks_filtering</code>","text":"<p>Substep to perform cooks filtering.</p>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.cooks_filtering","title":"<code>cooks_filtering</code>","text":"<p>Module to implement the base Mixin class for Cooks filtering.</p>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.cooks_filtering.CooksFiltering","title":"<code>CooksFiltering</code>","text":"<p>               Bases: <code>LocFindCooksOutliers</code>, <code>AggregateCooksOutliers</code>, <code>LocGetMaxCooks</code>, <code>AggMaxCooks</code>, <code>LocGetMaxCooksCounts</code>, <code>AggMaxCooksCounts</code>, <code>LocCountNumberSamplesAbove</code>, <code>AggCooksFiltering</code></p> <p>A class to perform Cooks filtering of p-values.</p> <p>Methods:</p> Name Description <code>cooks_filtering</code> <p>The method to find Cooks outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/cooks_filtering.py</code> <pre><code>class CooksFiltering(\n    LocFindCooksOutliers,\n    AggregateCooksOutliers,\n    LocGetMaxCooks,\n    AggMaxCooks,\n    LocGetMaxCooksCounts,\n    AggMaxCooksCounts,\n    LocCountNumberSamplesAbove,\n    AggCooksFiltering,\n):\n    \"\"\"A class to perform Cooks filtering of p-values.\n\n    Methods\n    -------\n    cooks_filtering\n        The method to find Cooks outliers.\n    \"\"\"\n\n    @log_organisation_method\n    def cooks_filtering(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        wald_test_shared_state,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Perform Cooks filtering.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[dict]\n            Local states. Required to propagate intermediate results.\n\n        wald_test_shared_state : dict\n            A shared state containing the Wald test results.\n            These results are the following fields:\n            - \"p_values\": p-values of the Wald test.\n            - \"wald_statistics\" : Wald statistics.\n            - \"wald_se\" : Wald standard errors.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. The new local state contains Cook's distances.\n\n        shared_state: dict\n            A new shared state containing the following fields:\n            - \"p_values\": p-values of the Wald test, updated to be nan for Cook's\n            outliers.\n            - \"wald_statistics\" : Wald statistics, for compatibility.\n            - \"wald_se\" : Wald standard errors, for compatibility.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.find_local_cooks_outliers,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=wald_test_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Find local Cook's outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        cooks_outliers_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_cooks_outliers,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Find the global Cook's outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.get_max_local_cooks,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=cooks_outliers_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get local max cooks distance\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        max_cooks_distance_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_max_cooks,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Get the max cooks distance for the outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.get_max_local_cooks_gene_counts,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=max_cooks_distance_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get the local max gene counts for the Cook's outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        max_cooks_gene_counts_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_max_cooks_gene_counts,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Get the max gene counts for the Cook's outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.count_local_number_samples_above,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=max_cooks_gene_counts_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Count the number of samples above the max gene counts\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        cooks_filtered_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_cooks_filtering,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Finish Cooks filtering\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        return local_states, cooks_filtered_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.cooks_filtering.CooksFiltering.cooks_filtering","title":"<code>cooks_filtering(train_data_nodes, aggregation_node, local_states, wald_test_shared_state, round_idx, clean_models)</code>","text":"<p>Perform Cooks filtering.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>wald_test_shared_state</code> <code>dict</code> <p>A shared state containing the Wald test results. These results are the following fields: - \"p_values\": p-values of the Wald test. - \"wald_statistics\" : Wald statistics. - \"wald_se\" : Wald standard errors.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. The new local state contains Cook's distances.</p> <code>shared_state</code> <code>dict</code> <p>A new shared state containing the following fields: - \"p_values\": p-values of the Wald test, updated to be nan for Cook's outliers. - \"wald_statistics\" : Wald statistics, for compatibility. - \"wald_se\" : Wald standard errors, for compatibility.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/cooks_filtering.py</code> <pre><code>@log_organisation_method\ndef cooks_filtering(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    wald_test_shared_state,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Perform Cooks filtering.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[dict]\n        Local states. Required to propagate intermediate results.\n\n    wald_test_shared_state : dict\n        A shared state containing the Wald test results.\n        These results are the following fields:\n        - \"p_values\": p-values of the Wald test.\n        - \"wald_statistics\" : Wald statistics.\n        - \"wald_se\" : Wald standard errors.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. The new local state contains Cook's distances.\n\n    shared_state: dict\n        A new shared state containing the following fields:\n        - \"p_values\": p-values of the Wald test, updated to be nan for Cook's\n        outliers.\n        - \"wald_statistics\" : Wald statistics, for compatibility.\n        - \"wald_se\" : Wald standard errors, for compatibility.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.find_local_cooks_outliers,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=wald_test_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Find local Cook's outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    cooks_outliers_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_cooks_outliers,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Find the global Cook's outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.get_max_local_cooks,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=cooks_outliers_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get local max cooks distance\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    max_cooks_distance_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_max_cooks,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Get the max cooks distance for the outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.get_max_local_cooks_gene_counts,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=max_cooks_distance_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get the local max gene counts for the Cook's outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    max_cooks_gene_counts_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_max_cooks_gene_counts,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Get the max gene counts for the Cook's outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.count_local_number_samples_above,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=max_cooks_gene_counts_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Count the number of samples above the max gene counts\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    cooks_filtered_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_cooks_filtering,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Finish Cooks filtering\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    return local_states, cooks_filtered_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggCooksFiltering","title":"<code>AggCooksFiltering</code>","text":"<p>Mixin class to aggregate the cooks filtering.</p> <p>Methods:</p> Name Description <code>agg_cooks_filtering</code> <p>Aggregate the local number of samples above.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class AggCooksFiltering:\n    \"\"\"Mixin class to aggregate the cooks filtering.\n\n    Methods\n    -------\n    agg_cooks_filtering\n        Aggregate the local number of samples above.\n    \"\"\"\n\n    @remote\n    @log_remote\n    def agg_cooks_filtering(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Aggregate the local number of samples above to get cooks filtered genes.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of shared states from the local step with the following keys:\n            - local_num_samples_above: np.ndarray of shape (n_genes,)\n                The local number of samples above the max cooks gene counts.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n            - p_values: np.ndarray of shape (n_genes,)\n                The p-values from the Wald test.\n            - wald_statistics: np.ndarray of shape (n_genes,)\n                The Wald statistics from the Wald test.\n            - wald_se: np.ndarray of shape (n_genes,)\n                The Wald standard errors from the Wald test.\n\n        Returns\n        -------\n        dict\n            A shared state with the following fields:\n            - p_values: np.ndarray of shape (n_genes,)\n                The p-values from the Wald test with nan for the cooks outliers.\n            - wald_statistics: np.ndarray of shape (n_genes,)\n                The Wald statistics.\n            - wald_se: np.ndarray of shape (n_genes,)\n                The Wald standard errors.\n        \"\"\"\n        # Find the number of samples with counts above the max cooks\n        cooks_outliers = shared_states[0][\"cooks_outliers\"]\n\n        num_samples_above_max_cooks = np.sum(\n            [state[\"local_num_samples_above\"] for state in shared_states], axis=0\n        )\n\n        # If that number is greater than 3, set the cooks filter to false\n        cooks_outliers[cooks_outliers] = num_samples_above_max_cooks &lt; 3\n\n        # Set the p-values to nan on cooks outliers\n        p_values = shared_states[0][\"p_values\"]\n        p_values[cooks_outliers] = np.nan\n\n        return {\n            \"p_values\": p_values,\n            \"wald_statistics\": shared_states[0][\"wald_statistics\"],\n            \"wald_se\": shared_states[0][\"wald_se\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggCooksFiltering.agg_cooks_filtering","title":"<code>agg_cooks_filtering(shared_states)</code>","text":"<p>Aggregate the local number of samples above to get cooks filtered genes.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of shared states from the local step with the following keys: - local_num_samples_above: np.ndarray of shape (n_genes,)     The local number of samples above the max cooks gene counts. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier. - p_values: np.ndarray of shape (n_genes,)     The p-values from the Wald test. - wald_statistics: np.ndarray of shape (n_genes,)     The Wald statistics from the Wald test. - wald_se: np.ndarray of shape (n_genes,)     The Wald standard errors from the Wald test.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A shared state with the following fields: - p_values: np.ndarray of shape (n_genes,)     The p-values from the Wald test with nan for the cooks outliers. - wald_statistics: np.ndarray of shape (n_genes,)     The Wald statistics. - wald_se: np.ndarray of shape (n_genes,)     The Wald standard errors.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_cooks_filtering(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Aggregate the local number of samples above to get cooks filtered genes.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of shared states from the local step with the following keys:\n        - local_num_samples_above: np.ndarray of shape (n_genes,)\n            The local number of samples above the max cooks gene counts.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n        - p_values: np.ndarray of shape (n_genes,)\n            The p-values from the Wald test.\n        - wald_statistics: np.ndarray of shape (n_genes,)\n            The Wald statistics from the Wald test.\n        - wald_se: np.ndarray of shape (n_genes,)\n            The Wald standard errors from the Wald test.\n\n    Returns\n    -------\n    dict\n        A shared state with the following fields:\n        - p_values: np.ndarray of shape (n_genes,)\n            The p-values from the Wald test with nan for the cooks outliers.\n        - wald_statistics: np.ndarray of shape (n_genes,)\n            The Wald statistics.\n        - wald_se: np.ndarray of shape (n_genes,)\n            The Wald standard errors.\n    \"\"\"\n    # Find the number of samples with counts above the max cooks\n    cooks_outliers = shared_states[0][\"cooks_outliers\"]\n\n    num_samples_above_max_cooks = np.sum(\n        [state[\"local_num_samples_above\"] for state in shared_states], axis=0\n    )\n\n    # If that number is greater than 3, set the cooks filter to false\n    cooks_outliers[cooks_outliers] = num_samples_above_max_cooks &lt; 3\n\n    # Set the p-values to nan on cooks outliers\n    p_values = shared_states[0][\"p_values\"]\n    p_values[cooks_outliers] = np.nan\n\n    return {\n        \"p_values\": p_values,\n        \"wald_statistics\": shared_states[0][\"wald_statistics\"],\n        \"wald_se\": shared_states[0][\"wald_se\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggMaxCooks","title":"<code>AggMaxCooks</code>","text":"<p>Mixin class to aggregate the max cooks distances.</p> <p>Methods:</p> Name Description <code>agg_max_cooks</code> <p>Aggregate the local max cooks distances.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class AggMaxCooks:\n    \"\"\"Mixin class to aggregate the max cooks distances.\n\n    Methods\n    -------\n    agg_max_cooks\n        Aggregate the local max cooks distances.\n    \"\"\"\n\n    @remote\n    @log_remote\n    def agg_max_cooks(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Aggregate the local max cooks.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of shared states from the local step with the following keys:\n            - local_max_cooks: np.ndarray of shape (n_genes,)\n                The local maximum cooks distance for the outliers.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n\n        Returns\n        -------\n        shared_state : dict\n            Aggregated max cooks.\n            It is a dictionary with the following fields:\n            - max_cooks: np.ndarray of shape (n_cooks_genes,)\n                The maximum cooks distance for the outliers in the aggregated dataset.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n        \"\"\"\n        return {\n            \"max_cooks\": np.max(\n                [state[\"local_max_cooks\"] for state in shared_states], axis=0\n            ),\n            \"cooks_outliers\": shared_states[0][\"cooks_outliers\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggMaxCooks.agg_max_cooks","title":"<code>agg_max_cooks(shared_states)</code>","text":"<p>Aggregate the local max cooks.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of shared states from the local step with the following keys: - local_max_cooks: np.ndarray of shape (n_genes,)     The local maximum cooks distance for the outliers. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>Aggregated max cooks. It is a dictionary with the following fields: - max_cooks: np.ndarray of shape (n_cooks_genes,)     The maximum cooks distance for the outliers in the aggregated dataset. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_max_cooks(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Aggregate the local max cooks.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of shared states from the local step with the following keys:\n        - local_max_cooks: np.ndarray of shape (n_genes,)\n            The local maximum cooks distance for the outliers.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n\n    Returns\n    -------\n    shared_state : dict\n        Aggregated max cooks.\n        It is a dictionary with the following fields:\n        - max_cooks: np.ndarray of shape (n_cooks_genes,)\n            The maximum cooks distance for the outliers in the aggregated dataset.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n    \"\"\"\n    return {\n        \"max_cooks\": np.max(\n            [state[\"local_max_cooks\"] for state in shared_states], axis=0\n        ),\n        \"cooks_outliers\": shared_states[0][\"cooks_outliers\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggMaxCooksCounts","title":"<code>AggMaxCooksCounts</code>","text":"<p>Mixin class to aggregate the max cooks gene counts.</p> <p>Methods:</p> Name Description <code>agg_max_cooks_gene_counts</code> <p>Aggregate the local max cooks gene counts. The goal is to have the gene counts corresponding to the maximum cooks distance for each gene across all datasets.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class AggMaxCooksCounts:\n    \"\"\"Mixin class to aggregate the max cooks gene counts.\n\n    Methods\n    -------\n    agg_max_cooks_gene_counts\n        Aggregate the local max cooks gene counts. The goal is to have the gene\n        counts corresponding to the maximum cooks distance for each gene across\n        all datasets.\n    \"\"\"\n\n    @remote\n    @log_remote\n    def agg_max_cooks_gene_counts(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Aggregate the local max cooks gene counts.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of shared states from the local step with the following keys:\n            - local_max_cooks_gene_counts: np.ndarray of shape (n_genes,)\n                The local maximum cooks gene counts for the outliers.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n\n        Returns\n        -------\n        shared_state : dict\n            A shared state with the following fields:\n            - max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)\n                For each gene, the array contains the gene counts corresponding to the\n                maximum cooks distance for that gene across all datasets.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n        \"\"\"\n        return {\n            \"max_cooks_gene_counts\": np.ma.stack(\n                [state[\"local_max_cooks_gene_counts\"] for state in shared_states],\n                axis=0,\n            ).min(axis=0),\n            \"cooks_outliers\": shared_states[0][\"cooks_outliers\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggMaxCooksCounts.agg_max_cooks_gene_counts","title":"<code>agg_max_cooks_gene_counts(shared_states)</code>","text":"<p>Aggregate the local max cooks gene counts.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of shared states from the local step with the following keys: - local_max_cooks_gene_counts: np.ndarray of shape (n_genes,)     The local maximum cooks gene counts for the outliers. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>A shared state with the following fields: - max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)     For each gene, the array contains the gene counts corresponding to the     maximum cooks distance for that gene across all datasets. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_max_cooks_gene_counts(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Aggregate the local max cooks gene counts.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of shared states from the local step with the following keys:\n        - local_max_cooks_gene_counts: np.ndarray of shape (n_genes,)\n            The local maximum cooks gene counts for the outliers.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n\n    Returns\n    -------\n    shared_state : dict\n        A shared state with the following fields:\n        - max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)\n            For each gene, the array contains the gene counts corresponding to the\n            maximum cooks distance for that gene across all datasets.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n    \"\"\"\n    return {\n        \"max_cooks_gene_counts\": np.ma.stack(\n            [state[\"local_max_cooks_gene_counts\"] for state in shared_states],\n            axis=0,\n        ).min(axis=0),\n        \"cooks_outliers\": shared_states[0][\"cooks_outliers\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggregateCooksOutliers","title":"<code>AggregateCooksOutliers</code>","text":"<p>Mixin class to aggregate the cooks outliers.</p> <p>Methods:</p> Name Description <code>agg_cooks_outliers</code> <p>Aggregate the local cooks outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class AggregateCooksOutliers:\n    \"\"\"Mixin class to aggregate the cooks outliers.\n\n    Methods\n    -------\n    agg_cooks_outliers\n        Aggregate the local cooks outliers.\n    \"\"\"\n\n    @remote\n    @log_remote\n    @prepare_cooks_agg\n    def agg_cooks_outliers(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Aggregate the local cooks outliers.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of shared states from the local step with the following keys:\n            - local_cooks_outliers: np.ndarray of shape (n_genes,)\n            - cooks_cutoff: float\n\n        Returns\n        -------\n        shared_state : dict\n            Aggregated cooks outliers.\n            It is a dictionary with the following fields:\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier in\n                any of the local datasets\n            - cooks_cutoff: float\n                The cutoff used to define the fact that a gene is a cooks outlier.\n        \"\"\"\n        return {\n            \"cooks_outliers\": np.any(\n                [state[\"local_cooks_outliers\"] for state in shared_states], axis=0\n            ),\n            \"cooks_cutoff\": shared_states[0][\"cooks_cutoff\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.AggregateCooksOutliers.agg_cooks_outliers","title":"<code>agg_cooks_outliers(shared_states)</code>","text":"<p>Aggregate the local cooks outliers.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of shared states from the local step with the following keys: - local_cooks_outliers: np.ndarray of shape (n_genes,) - cooks_cutoff: float</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>Aggregated cooks outliers. It is a dictionary with the following fields: - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier in     any of the local datasets - cooks_cutoff: float     The cutoff used to define the fact that a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote\n@log_remote\n@prepare_cooks_agg\ndef agg_cooks_outliers(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Aggregate the local cooks outliers.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of shared states from the local step with the following keys:\n        - local_cooks_outliers: np.ndarray of shape (n_genes,)\n        - cooks_cutoff: float\n\n    Returns\n    -------\n    shared_state : dict\n        Aggregated cooks outliers.\n        It is a dictionary with the following fields:\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier in\n            any of the local datasets\n        - cooks_cutoff: float\n            The cutoff used to define the fact that a gene is a cooks outlier.\n    \"\"\"\n    return {\n        \"cooks_outliers\": np.any(\n            [state[\"local_cooks_outliers\"] for state in shared_states], axis=0\n        ),\n        \"cooks_cutoff\": shared_states[0][\"cooks_cutoff\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocCountNumberSamplesAbove","title":"<code>LocCountNumberSamplesAbove</code>","text":"<p>Mixin class to count the number of samples above the max cooks gene counts.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Methods:</p> Name Description <code>count_local_number_samples_above</code> <p>Count the number of samples above the max cooks gene counts.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class LocCountNumberSamplesAbove:\n    \"\"\"Mixin class to count the number of samples above the max cooks gene counts.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n\n    Methods\n    -------\n    count_local_number_samples_above\n        Count the number of samples above the max cooks gene counts.\n    \"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def count_local_number_samples_above(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Count the number of samples above the max cooks gene counts.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state from the previous step with the following\n            keys:\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n            - max_cooks_gene_counts: np.ndarray of shape (n_genes,)\n                For each gene, the array contains the gene counts corresponding to the\n                maximum cooks distance for that gene across all datasets.\n\n        Returns\n        -------\n        shared_state : dict\n            A shared state with the following fields:\n            - local_num_samples_above: np.ndarray of shape (n_cooks_genes,)\n                For each gene, the array contains the number of samples above the\n                maximum cooks gene counts.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n            - p_values: np.ndarray of shape (n_genes,)\n                The p-values from the Wald test.\n            - wald_statistic: np.ndarray of shape (n_genes,)\n                The Wald statistics from the Wald test.\n            - wald_se: np.ndarray of shape (n_genes,)\n                The Wald standard errors from the Wald test.\n        \"\"\"\n        cooks_outliers = shared_state[\"cooks_outliers\"]\n        max_cooks_gene_counts = shared_state[\"max_cooks_gene_counts\"]\n\n        num_samples_above = np.sum(\n            self.local_adata.X[:, cooks_outliers] &gt; max_cooks_gene_counts, axis=0\n        )\n\n        return {\n            \"local_num_samples_above\": num_samples_above,\n            \"cooks_outliers\": cooks_outliers,\n            \"p_values\": self.local_adata.varm[\"p_values\"],\n            \"wald_statistics\": self.local_adata.varm[\"wald_statistics\"],\n            \"wald_se\": self.local_adata.varm[\"wald_se\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocCountNumberSamplesAbove.count_local_number_samples_above","title":"<code>count_local_number_samples_above(data_from_opener, shared_state)</code>","text":"<p>Count the number of samples above the max cooks gene counts.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state from the previous step with the following keys: - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier. - max_cooks_gene_counts: np.ndarray of shape (n_genes,)     For each gene, the array contains the gene counts corresponding to the     maximum cooks distance for that gene across all datasets.</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>A shared state with the following fields: - local_num_samples_above: np.ndarray of shape (n_cooks_genes,)     For each gene, the array contains the number of samples above the     maximum cooks gene counts. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier. - p_values: np.ndarray of shape (n_genes,)     The p-values from the Wald test. - wald_statistic: np.ndarray of shape (n_genes,)     The Wald statistics from the Wald test. - wald_se: np.ndarray of shape (n_genes,)     The Wald standard errors from the Wald test.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef count_local_number_samples_above(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Count the number of samples above the max cooks gene counts.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state from the previous step with the following\n        keys:\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n        - max_cooks_gene_counts: np.ndarray of shape (n_genes,)\n            For each gene, the array contains the gene counts corresponding to the\n            maximum cooks distance for that gene across all datasets.\n\n    Returns\n    -------\n    shared_state : dict\n        A shared state with the following fields:\n        - local_num_samples_above: np.ndarray of shape (n_cooks_genes,)\n            For each gene, the array contains the number of samples above the\n            maximum cooks gene counts.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n        - p_values: np.ndarray of shape (n_genes,)\n            The p-values from the Wald test.\n        - wald_statistic: np.ndarray of shape (n_genes,)\n            The Wald statistics from the Wald test.\n        - wald_se: np.ndarray of shape (n_genes,)\n            The Wald standard errors from the Wald test.\n    \"\"\"\n    cooks_outliers = shared_state[\"cooks_outliers\"]\n    max_cooks_gene_counts = shared_state[\"max_cooks_gene_counts\"]\n\n    num_samples_above = np.sum(\n        self.local_adata.X[:, cooks_outliers] &gt; max_cooks_gene_counts, axis=0\n    )\n\n    return {\n        \"local_num_samples_above\": num_samples_above,\n        \"cooks_outliers\": cooks_outliers,\n        \"p_values\": self.local_adata.varm[\"p_values\"],\n        \"wald_statistics\": self.local_adata.varm[\"wald_statistics\"],\n        \"wald_se\": self.local_adata.varm[\"wald_se\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocFindCooksOutliers","title":"<code>LocFindCooksOutliers</code>","text":"<p>Mixin class to find the local cooks outliers.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object. Is expected to have a \"tot_num_samples\" key in uns.</p> <code>refit_cooks</code> <code>bool</code> <p>Whether to refit the cooks outliers.</p> <p>Methods:</p> Name Description <code>find_local_cooks_outliers</code> <p>Find the local cooks outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class LocFindCooksOutliers:\n    \"\"\"Mixin class to find the local cooks outliers.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n        Is expected to have a \"tot_num_samples\" key in uns.\n\n    refit_cooks : bool\n        Whether to refit the cooks outliers.\n\n\n    Methods\n    -------\n    find_local_cooks_outliers\n        Find the local cooks outliers.\n    \"\"\"\n\n    local_adata: AnnData\n    refit_cooks: bool\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    @prepare_cooks_local\n    def find_local_cooks_outliers(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Find the local cooks outliers.\n\n        This method is expected to run on the results of the Wald tests.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state from the previous step with the following\n            keys:\n            - p_values: np.ndarray of shape (n_genes,)\n            - wald_statistics: np.ndarray of shape (n_genes,)\n            - wald_se: np.ndarray of shape (n_genes,)\n\n        Returns\n        -------\n        shared_state : dict\n            A shared state with the following fields:\n            - local_cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n            - cooks_cutoff: float\n                The cutoff used to define the fact that a gene is a cooks outlier.\n        \"\"\"\n        # Save these in the local adata\n        self.local_adata.varm[\"p_values\"] = shared_state[\"p_values\"]\n        self.local_adata.varm[\"wald_statistics\"] = shared_state[\"wald_statistics\"]\n        self.local_adata.varm[\"wald_se\"] = shared_state[\"wald_se\"]\n\n        tot_num_samples = self.local_adata.uns[\"tot_num_samples\"]\n        num_vars = self.local_adata.uns[\"n_params\"]\n\n        cooks_cutoff = f.ppf(0.99, num_vars, tot_num_samples - num_vars)\n\n        # Take into account whether we already replaced outliers\n        cooks_layer = (\n            \"replace_cooks\"\n            if self.refit_cooks and self.local_adata.varm[\"refitted\"].sum() &gt; 0\n            else \"cooks\"\n        )\n\n        if cooks_layer == \"replace_cooks\":\n            assert \"replaced\" in self.local_adata.varm.keys()\n            replace_cooks = pd.DataFrame(self.local_adata.layers[\"cooks\"].copy())\n            replace_cooks.loc[\n                self.local_adata.obsm[\"replaceable\"], self.local_adata.varm[\"refitted\"]\n            ] = 0.0\n            self.local_adata.layers[\"replace_cooks\"] = replace_cooks\n\n        use_for_max = self.local_adata.obs[\"cells\"].apply(\n            lambda x: (self.local_adata.uns[\"num_replicates\"] &gt;= 3).loc[x]\n        )\n\n        cooks_outliers = (\n            (self.local_adata[use_for_max, :].layers[cooks_layer] &gt; cooks_cutoff)\n            .any(axis=0)\n            .copy()\n        )\n\n        return {\"local_cooks_outliers\": cooks_outliers, \"cooks_cutoff\": cooks_cutoff}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocFindCooksOutliers.find_local_cooks_outliers","title":"<code>find_local_cooks_outliers(data_from_opener, shared_state)</code>","text":"<p>Find the local cooks outliers.</p> <p>This method is expected to run on the results of the Wald tests.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state from the previous step with the following keys: - p_values: np.ndarray of shape (n_genes,) - wald_statistics: np.ndarray of shape (n_genes,) - wald_se: np.ndarray of shape (n_genes,)</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>A shared state with the following fields: - local_cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier. - cooks_cutoff: float     The cutoff used to define the fact that a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\n@prepare_cooks_local\ndef find_local_cooks_outliers(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Find the local cooks outliers.\n\n    This method is expected to run on the results of the Wald tests.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state from the previous step with the following\n        keys:\n        - p_values: np.ndarray of shape (n_genes,)\n        - wald_statistics: np.ndarray of shape (n_genes,)\n        - wald_se: np.ndarray of shape (n_genes,)\n\n    Returns\n    -------\n    shared_state : dict\n        A shared state with the following fields:\n        - local_cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n        - cooks_cutoff: float\n            The cutoff used to define the fact that a gene is a cooks outlier.\n    \"\"\"\n    # Save these in the local adata\n    self.local_adata.varm[\"p_values\"] = shared_state[\"p_values\"]\n    self.local_adata.varm[\"wald_statistics\"] = shared_state[\"wald_statistics\"]\n    self.local_adata.varm[\"wald_se\"] = shared_state[\"wald_se\"]\n\n    tot_num_samples = self.local_adata.uns[\"tot_num_samples\"]\n    num_vars = self.local_adata.uns[\"n_params\"]\n\n    cooks_cutoff = f.ppf(0.99, num_vars, tot_num_samples - num_vars)\n\n    # Take into account whether we already replaced outliers\n    cooks_layer = (\n        \"replace_cooks\"\n        if self.refit_cooks and self.local_adata.varm[\"refitted\"].sum() &gt; 0\n        else \"cooks\"\n    )\n\n    if cooks_layer == \"replace_cooks\":\n        assert \"replaced\" in self.local_adata.varm.keys()\n        replace_cooks = pd.DataFrame(self.local_adata.layers[\"cooks\"].copy())\n        replace_cooks.loc[\n            self.local_adata.obsm[\"replaceable\"], self.local_adata.varm[\"refitted\"]\n        ] = 0.0\n        self.local_adata.layers[\"replace_cooks\"] = replace_cooks\n\n    use_for_max = self.local_adata.obs[\"cells\"].apply(\n        lambda x: (self.local_adata.uns[\"num_replicates\"] &gt;= 3).loc[x]\n    )\n\n    cooks_outliers = (\n        (self.local_adata[use_for_max, :].layers[cooks_layer] &gt; cooks_cutoff)\n        .any(axis=0)\n        .copy()\n    )\n\n    return {\"local_cooks_outliers\": cooks_outliers, \"cooks_cutoff\": cooks_cutoff}\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocGetMaxCooks","title":"<code>LocGetMaxCooks</code>","text":"<p>Mixin class to get the maximum cooks distance for the outliers.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object.</p> <p>Methods:</p> Name Description <code>get_max_local_cooks</code> <p>Get the maximum cooks distance for the outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class LocGetMaxCooks:\n    \"\"\"Mixin class to get the maximum cooks distance for the outliers.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n\n    Methods\n    -------\n    get_max_local_cooks\n        Get the maximum cooks distance for the outliers.\n    \"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_max_local_cooks(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Get the maximum cooks distance for the outliers.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state from the previous step with the following\n            keys:\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n            - cooks_cutoff: float\n\n        Returns\n        -------\n        shared_state : dict\n            A shared state with the following fields:\n            - local_max_cooks: np.ndarray of shape (n_cooks_genes,)\n                The maximum cooks distance for the outliers in the local dataset.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n        \"\"\"\n        cooks_outliers = shared_state[\"cooks_outliers\"]\n        cooks_cutoff = shared_state[\"cooks_cutoff\"]\n\n        max_cooks = np.max(self.local_adata.layers[\"cooks\"][:, cooks_outliers], axis=0)\n\n        max_cooks[max_cooks &lt;= cooks_cutoff] = 0.0\n\n        max_cooks_idx = self.local_adata.layers[\"cooks\"][:, cooks_outliers].argmax(\n            axis=0\n        )\n\n        max_cooks_value = self.local_adata.layers[\"cooks\"][:, cooks_outliers][\n            max_cooks_idx, np.arange(len(max_cooks))\n        ]\n\n        max_cooks_gene_counts = self.local_adata.X[:, cooks_outliers][\n            max_cooks_idx, np.arange(len(max_cooks))\n        ]\n\n        # Save the max cooks gene counts and max cooks value\n        self.local_adata.uns[\"max_cooks_gene_counts\"] = max_cooks_gene_counts\n        self.local_adata.uns[\"max_cooks_value\"] = max_cooks_value\n\n        return {\n            \"local_max_cooks\": max_cooks,\n            \"cooks_outliers\": cooks_outliers,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocGetMaxCooks.get_max_local_cooks","title":"<code>get_max_local_cooks(data_from_opener, shared_state)</code>","text":"<p>Get the maximum cooks distance for the outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state from the previous step with the following keys: - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier. - cooks_cutoff: float</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>A shared state with the following fields: - local_max_cooks: np.ndarray of shape (n_cooks_genes,)     The maximum cooks distance for the outliers in the local dataset. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_max_local_cooks(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Get the maximum cooks distance for the outliers.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state from the previous step with the following\n        keys:\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n        - cooks_cutoff: float\n\n    Returns\n    -------\n    shared_state : dict\n        A shared state with the following fields:\n        - local_max_cooks: np.ndarray of shape (n_cooks_genes,)\n            The maximum cooks distance for the outliers in the local dataset.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n    \"\"\"\n    cooks_outliers = shared_state[\"cooks_outliers\"]\n    cooks_cutoff = shared_state[\"cooks_cutoff\"]\n\n    max_cooks = np.max(self.local_adata.layers[\"cooks\"][:, cooks_outliers], axis=0)\n\n    max_cooks[max_cooks &lt;= cooks_cutoff] = 0.0\n\n    max_cooks_idx = self.local_adata.layers[\"cooks\"][:, cooks_outliers].argmax(\n        axis=0\n    )\n\n    max_cooks_value = self.local_adata.layers[\"cooks\"][:, cooks_outliers][\n        max_cooks_idx, np.arange(len(max_cooks))\n    ]\n\n    max_cooks_gene_counts = self.local_adata.X[:, cooks_outliers][\n        max_cooks_idx, np.arange(len(max_cooks))\n    ]\n\n    # Save the max cooks gene counts and max cooks value\n    self.local_adata.uns[\"max_cooks_gene_counts\"] = max_cooks_gene_counts\n    self.local_adata.uns[\"max_cooks_value\"] = max_cooks_value\n\n    return {\n        \"local_max_cooks\": max_cooks,\n        \"cooks_outliers\": cooks_outliers,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocGetMaxCooksCounts","title":"<code>LocGetMaxCooksCounts</code>","text":"<p>Mixin class to get the maximum cooks counts for the outliers.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object.</p> <p>Methods:</p> Name Description <code>get_max_local_cooks_gene_counts</code> <p>Get the maximum cooks counts for the outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>class LocGetMaxCooksCounts:\n    \"\"\"Mixin class to get the maximum cooks counts for the outliers.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n\n    Methods\n    -------\n    get_max_local_cooks_gene_counts\n        Get the maximum cooks counts for the outliers.\n    \"\"\"\n\n    local_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_max_local_cooks_gene_counts(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Get the maximum cooks counts for the outliers.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            Shared state from the previous step with the following\n            keys:\n            - max_cooks: np.ndarray of shape (n_cooks_genes,)\n                The maximum cooks distance for the outliers.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n\n        Returns\n        -------\n        shared_state : dict\n            A shared state with the following fields:\n            - local_max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)\n                For each gene, the array contains the gene counts corresponding to the\n                maximum cooks distance for that gene if the maximum cooks distance\n                in the local dataset is equal to the maximum cooks distance in the\n                aggregated dataset, and nan otherwise.\n            - cooks_outliers: np.ndarray of shape (n_genes,)\n                It is a boolean array indicating whether a gene is a cooks outlier.\n        \"\"\"\n        max_cooks = shared_state[\"max_cooks\"]\n        cooks_outliers = shared_state[\"cooks_outliers\"]\n\n        max_cooks_gene_counts = self.local_adata.uns[\"max_cooks_gene_counts\"].copy()\n        max_cooks_value = self.local_adata.uns[\"max_cooks_value\"].copy()\n\n        # Remove them from the uns field as they are no longer needed\n        del self.local_adata.uns[\"max_cooks_gene_counts\"]\n        del self.local_adata.uns[\"max_cooks_value\"]\n\n        max_cooks_gene_counts[max_cooks_value &lt; max_cooks] = (\n            -1\n        )  # We can use a &lt; because the count value are non negative integers.\n\n        max_cooks_gene_counts_ma = np.ma.masked_array(\n            max_cooks_gene_counts, max_cooks_gene_counts == -1\n        )\n\n        return {\n            \"local_max_cooks_gene_counts\": max_cooks_gene_counts_ma,\n            \"cooks_outliers\": cooks_outliers,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.cooks_filtering.substeps.LocGetMaxCooksCounts.get_max_local_cooks_gene_counts","title":"<code>get_max_local_cooks_gene_counts(data_from_opener, shared_state)</code>","text":"<p>Get the maximum cooks counts for the outliers.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state from the previous step with the following keys: - max_cooks: np.ndarray of shape (n_cooks_genes,)     The maximum cooks distance for the outliers. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> required <p>Returns:</p> Name Type Description <code>shared_state</code> <code>dict</code> <p>A shared state with the following fields: - local_max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)     For each gene, the array contains the gene counts corresponding to the     maximum cooks distance for that gene if the maximum cooks distance     in the local dataset is equal to the maximum cooks distance in the     aggregated dataset, and nan otherwise. - cooks_outliers: np.ndarray of shape (n_genes,)     It is a boolean array indicating whether a gene is a cooks outlier.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/cooks_filtering/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_max_local_cooks_gene_counts(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Get the maximum cooks counts for the outliers.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        Shared state from the previous step with the following\n        keys:\n        - max_cooks: np.ndarray of shape (n_cooks_genes,)\n            The maximum cooks distance for the outliers.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n\n    Returns\n    -------\n    shared_state : dict\n        A shared state with the following fields:\n        - local_max_cooks_gene_counts: np.ndarray of shape (n_cooks_genes,)\n            For each gene, the array contains the gene counts corresponding to the\n            maximum cooks distance for that gene if the maximum cooks distance\n            in the local dataset is equal to the maximum cooks distance in the\n            aggregated dataset, and nan otherwise.\n        - cooks_outliers: np.ndarray of shape (n_genes,)\n            It is a boolean array indicating whether a gene is a cooks outlier.\n    \"\"\"\n    max_cooks = shared_state[\"max_cooks\"]\n    cooks_outliers = shared_state[\"cooks_outliers\"]\n\n    max_cooks_gene_counts = self.local_adata.uns[\"max_cooks_gene_counts\"].copy()\n    max_cooks_value = self.local_adata.uns[\"max_cooks_value\"].copy()\n\n    # Remove them from the uns field as they are no longer needed\n    del self.local_adata.uns[\"max_cooks_gene_counts\"]\n    del self.local_adata.uns[\"max_cooks_value\"]\n\n    max_cooks_gene_counts[max_cooks_value &lt; max_cooks] = (\n        -1\n    )  # We can use a &lt; because the count value are non negative integers.\n\n    max_cooks_gene_counts_ma = np.ma.masked_array(\n        max_cooks_gene_counts, max_cooks_gene_counts == -1\n    )\n\n    return {\n        \"local_max_cooks_gene_counts\": max_cooks_gene_counts_ma,\n        \"cooks_outliers\": cooks_outliers,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats","title":"<code>deseq2_stats</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats.DESeq2Stats","title":"<code>DESeq2Stats</code>","text":"<p>               Bases: <code>RunWaldTests</code>, <code>CooksFiltering</code>, <code>ComputeAdjustedPValues</code></p> <p>Mixin class to compute statistics with DESeq2.</p> <p>This class encapsulates the Wald tests, the Cooks filtering and the computation of adjusted p-values.</p> <p>Methods:</p> Name Description <code>run_deseq2_stats</code> <p>Run the DESeq2 statistics pipeline. Performs Wald tests, Cook's filtering and computes adjusted p-values.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/deseq2_stats.py</code> <pre><code>class DESeq2Stats(RunWaldTests, CooksFiltering, ComputeAdjustedPValues):\n    \"\"\"Mixin class to compute statistics with DESeq2.\n\n    This class encapsulates the Wald tests, the Cooks filtering and the computation\n    of adjusted p-values.\n\n    Methods\n    -------\n    run_deseq2_stats\n        Run the DESeq2 statistics pipeline.\n        Performs Wald tests, Cook's filtering and computes adjusted p-values.\n    \"\"\"\n\n    cooks_filter: bool\n\n    @log_organisation_method\n    def run_deseq2_stats(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Run the DESeq2 statistics pipeline.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[dict]\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n\n        Returns\n        -------\n        local_states: dict\n            Local states.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        #### Perform Wald tests ####\n        logger.info(\"Running Wald tests.\")\n\n        local_states, wald_shared_state, round_idx = self.run_wald_tests(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n        )\n\n        logger.info(\"Finished running Wald tests.\")\n\n        if self.cooks_filter:\n            logger.info(\"Running Cook's filtering...\")\n            local_states, wald_shared_state, round_idx = self.cooks_filtering(\n                train_data_nodes,\n                aggregation_node,\n                local_states,\n                wald_shared_state,\n                round_idx,\n                clean_models=clean_models,\n            )\n            logger.info(\"Finished running Cook's filtering.\")\n            logger.info(\"Computing adjusted p-values...\")\n        (\n            local_states,\n            round_idx,\n        ) = self.compute_adjusted_p_values(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            wald_shared_state,\n            round_idx,\n            clean_models=clean_models,\n        )\n        logger.info(\"Finished computing adjusted p-values.\")\n\n        return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats.DESeq2Stats.run_deseq2_stats","title":"<code>run_deseq2_stats(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Run the DESeq2 statistics pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/deseq2_stats.py</code> <pre><code>@log_organisation_method\ndef run_deseq2_stats(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Run the DESeq2 statistics pipeline.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[dict]\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n\n    Returns\n    -------\n    local_states: dict\n        Local states.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    #### Perform Wald tests ####\n    logger.info(\"Running Wald tests.\")\n\n    local_states, wald_shared_state, round_idx = self.run_wald_tests(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n    )\n\n    logger.info(\"Finished running Wald tests.\")\n\n    if self.cooks_filter:\n        logger.info(\"Running Cook's filtering...\")\n        local_states, wald_shared_state, round_idx = self.cooks_filtering(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            wald_shared_state,\n            round_idx,\n            clean_models=clean_models,\n        )\n        logger.info(\"Finished running Cook's filtering.\")\n        logger.info(\"Computing adjusted p-values...\")\n    (\n        local_states,\n        round_idx,\n    ) = self.compute_adjusted_p_values(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        wald_shared_state,\n        round_idx,\n        clean_models=clean_models,\n    )\n    logger.info(\"Finished computing adjusted p-values.\")\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests","title":"<code>wald_tests</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.substeps.AggRunWaldTests","title":"<code>AggRunWaldTests</code>","text":"<p>Mixin to run Wald tests.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/substeps.py</code> <pre><code>class AggRunWaldTests:\n    \"\"\"Mixin to run Wald tests.\"\"\"\n\n    lfc_null: float\n    alt_hypothesis: Literal[\"greaterAbs\", \"lessAbs\", \"greater\", \"less\"] | None\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n\n    @remote\n    @log_remote\n    @prepare_cooks_agg\n    def agg_run_wald_tests(self, shared_states: list) -&gt; dict:\n        \"\"\"Run the Wald tests.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of shared states containing:\n            - local_H_matrix: np.ndarray\n                The local H matrix.\n            - LFC: np.ndarray\n                The log fold changes, in natural log scale.\n            - contrast_vector: np.ndarray\n                The contrast vector.\n\n        Returns\n        -------\n        dict\n            Contains:\n            - p_values: np.ndarray\n                The (unadjusted) p-values (n_genes,).\n            - wald_statistics: np.ndarray\n                The Wald statistics (n_genes,).\n            - wald_se: np.ndarray\n                The standard errors of the Wald statistics (n_genes,).\n        \"\"\"\n        # First step: aggregate the local H matrices\n\n        H = sum([state[\"local_H_matrix\"] for state in shared_states])\n\n        # Second step: compute the Wald tests in parallel\n        with parallel_backend(self.joblib_backend):\n            wald_test_results = Parallel(\n                n_jobs=self.num_jobs, verbose=self.joblib_verbosity\n            )(\n                delayed(wald_test)(\n                    H[i],\n                    shared_states[0][\"LFC\"].values[i],\n                    None,\n                    shared_states[0][\"contrast_vector\"],\n                    np.log(2) * self.lfc_null,\n                    self.alt_hypothesis,\n                )\n                for i in range(len(H))\n            )\n\n        # Finally, unpack the results\n        p_values = np.array([r[0] for r in wald_test_results])\n        wald_statistics = np.array([r[1] for r in wald_test_results])\n        wald_se = np.array([r[2] for r in wald_test_results])\n\n        return {\n            \"p_values\": p_values,\n            \"wald_statistics\": wald_statistics,\n            \"wald_se\": wald_se,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.substeps.AggRunWaldTests.agg_run_wald_tests","title":"<code>agg_run_wald_tests(shared_states)</code>","text":"<p>Run the Wald tests.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of shared states containing: - local_H_matrix: np.ndarray     The local H matrix. - LFC: np.ndarray     The log fold changes, in natural log scale. - contrast_vector: np.ndarray     The contrast vector.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Contains: - p_values: np.ndarray     The (unadjusted) p-values (n_genes,). - wald_statistics: np.ndarray     The Wald statistics (n_genes,). - wald_se: np.ndarray     The standard errors of the Wald statistics (n_genes,).</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/substeps.py</code> <pre><code>@remote\n@log_remote\n@prepare_cooks_agg\ndef agg_run_wald_tests(self, shared_states: list) -&gt; dict:\n    \"\"\"Run the Wald tests.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of shared states containing:\n        - local_H_matrix: np.ndarray\n            The local H matrix.\n        - LFC: np.ndarray\n            The log fold changes, in natural log scale.\n        - contrast_vector: np.ndarray\n            The contrast vector.\n\n    Returns\n    -------\n    dict\n        Contains:\n        - p_values: np.ndarray\n            The (unadjusted) p-values (n_genes,).\n        - wald_statistics: np.ndarray\n            The Wald statistics (n_genes,).\n        - wald_se: np.ndarray\n            The standard errors of the Wald statistics (n_genes,).\n    \"\"\"\n    # First step: aggregate the local H matrices\n\n    H = sum([state[\"local_H_matrix\"] for state in shared_states])\n\n    # Second step: compute the Wald tests in parallel\n    with parallel_backend(self.joblib_backend):\n        wald_test_results = Parallel(\n            n_jobs=self.num_jobs, verbose=self.joblib_verbosity\n        )(\n            delayed(wald_test)(\n                H[i],\n                shared_states[0][\"LFC\"].values[i],\n                None,\n                shared_states[0][\"contrast_vector\"],\n                np.log(2) * self.lfc_null,\n                self.alt_hypothesis,\n            )\n            for i in range(len(H))\n        )\n\n    # Finally, unpack the results\n    p_values = np.array([r[0] for r in wald_test_results])\n    wald_statistics = np.array([r[1] for r in wald_test_results])\n    wald_se = np.array([r[2] for r in wald_test_results])\n\n    return {\n        \"p_values\": p_values,\n        \"wald_statistics\": wald_statistics,\n        \"wald_se\": wald_se,\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.substeps.LocBuildContrastVectorHMatrix","title":"<code>LocBuildContrastVectorHMatrix</code>","text":"<p>Mixin to get compute contrast vectors and local H matrices.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/substeps.py</code> <pre><code>class LocBuildContrastVectorHMatrix:\n    \"\"\"Mixin to get compute contrast vectors and local H matrices.\"\"\"\n\n    local_adata: ad.AnnData\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    @prepare_cooks_local\n    def compute_contrast_vector_and_H_matrix(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Build the contrast vector and the local H matrices.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Not used.\n\n        Returns\n        -------\n        dict\n            Contains:\n            - local_H_matrix: np.ndarray\n                The local H matrix.\n            - LFC: np.ndarray\n                The log fold changes, in natural log scale.\n            - contrast_vector: np.ndarray\n                The contrast vector.\n        \"\"\"\n        # Build contrast vector and index\n        (\n            self.local_adata.uns[\"contrast_vector\"],\n            self.local_adata.uns[\"contrast_idx\"],\n        ) = build_contrast_vector(\n            self.local_adata.uns[\"contrast\"],\n            self.local_adata.varm[\"LFC\"].columns,\n        )\n\n        # ---- Compute the summands for the covariance matrix ---- #\n\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n                delayed(make_irls_update_summands_and_nll_batch)(\n                    self.local_adata.obsm[\"design_matrix\"].values,\n                    self.local_adata.obsm[\"size_factors\"],\n                    self.local_adata.varm[\"LFC\"][i : i + self.irls_batch_size].values,\n                    self.local_adata.varm[\"dispersions\"][i : i + self.irls_batch_size],\n                    self.local_adata.X[:, i : i + self.irls_batch_size],\n                    0,\n                )\n                for i in range(0, self.local_adata.n_vars, self.irls_batch_size)\n            )\n\n        H = np.concatenate([r[0] for r in res])\n\n        return {\n            \"local_H_matrix\": H,\n            \"LFC\": self.local_adata.varm[\"LFC\"],\n            \"contrast_vector\": self.local_adata.uns[\"contrast_vector\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.substeps.LocBuildContrastVectorHMatrix.compute_contrast_vector_and_H_matrix","title":"<code>compute_contrast_vector_and_H_matrix(data_from_opener, shared_state)</code>","text":"<p>Build the contrast vector and the local H matrices.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Contains: - local_H_matrix: np.ndarray     The local H matrix. - LFC: np.ndarray     The log fold changes, in natural log scale. - contrast_vector: np.ndarray     The contrast vector.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\n@prepare_cooks_local\ndef compute_contrast_vector_and_H_matrix(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Build the contrast vector and the local H matrices.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Not used.\n\n    Returns\n    -------\n    dict\n        Contains:\n        - local_H_matrix: np.ndarray\n            The local H matrix.\n        - LFC: np.ndarray\n            The log fold changes, in natural log scale.\n        - contrast_vector: np.ndarray\n            The contrast vector.\n    \"\"\"\n    # Build contrast vector and index\n    (\n        self.local_adata.uns[\"contrast_vector\"],\n        self.local_adata.uns[\"contrast_idx\"],\n    ) = build_contrast_vector(\n        self.local_adata.uns[\"contrast\"],\n        self.local_adata.varm[\"LFC\"].columns,\n    )\n\n    # ---- Compute the summands for the covariance matrix ---- #\n\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n            delayed(make_irls_update_summands_and_nll_batch)(\n                self.local_adata.obsm[\"design_matrix\"].values,\n                self.local_adata.obsm[\"size_factors\"],\n                self.local_adata.varm[\"LFC\"][i : i + self.irls_batch_size].values,\n                self.local_adata.varm[\"dispersions\"][i : i + self.irls_batch_size],\n                self.local_adata.X[:, i : i + self.irls_batch_size],\n                0,\n            )\n            for i in range(0, self.local_adata.n_vars, self.irls_batch_size)\n        )\n\n    H = np.concatenate([r[0] for r in res])\n\n    return {\n        \"local_H_matrix\": H,\n        \"LFC\": self.local_adata.varm[\"LFC\"],\n        \"contrast_vector\": self.local_adata.uns[\"contrast_vector\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.wald_tests","title":"<code>wald_tests</code>","text":""},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.wald_tests.RunWaldTests","title":"<code>RunWaldTests</code>","text":"<p>               Bases: <code>LocBuildContrastVectorHMatrix</code>, <code>AggRunWaldTests</code></p> <p>Mixin class to implement the computation of the Wald tests.</p> <p>Methods:</p> Name Description <code>run_wald_tests</code> <p>The method to compute the Wald tests.</p> Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/wald_tests.py</code> <pre><code>class RunWaldTests(LocBuildContrastVectorHMatrix, AggRunWaldTests):\n    \"\"\"Mixin class to implement the computation of the Wald tests.\n\n    Methods\n    -------\n    run_wald_tests\n        The method to compute the Wald tests.\n    \"\"\"\n\n    @log_organisation_method\n    def run_wald_tests(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Compute the Wald tests.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n        \"\"\"\n        # --- Build contrast vectors and compute local H matrices --- #\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.compute_contrast_vector_and_H_matrix,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=None,  # TODO plug in previous step\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Build contrast vectors and compute local H matrices\",\n            clean_models=clean_models,\n        )\n\n        # --- Aggregate the H matrices and run the Wald tests --- #\n        wald_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_run_wald_tests,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            round_idx=round_idx,\n            description=\"Run Wald tests.\",\n            clean_models=clean_models,\n        )\n\n        return local_states, wald_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#fedpydeseq2.core.deseq2_core.deseq2_stats.wald_tests.wald_tests.RunWaldTests.run_wald_tests","title":"<code>run_wald_tests(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Compute the Wald tests.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required Source code in <code>fedpydeseq2/core/deseq2_core/deseq2_stats/wald_tests/wald_tests.py</code> <pre><code>@log_organisation_method\ndef run_wald_tests(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Compute the Wald tests.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n    \"\"\"\n    # --- Build contrast vectors and compute local H matrices --- #\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.compute_contrast_vector_and_H_matrix,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        round_idx=round_idx,\n        input_local_states=local_states,\n        input_shared_state=None,  # TODO plug in previous step\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Build contrast vectors and compute local H matrices\",\n        clean_models=clean_models,\n    )\n\n    # --- Aggregate the H matrices and run the Wald tests --- #\n    wald_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_run_wald_tests,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        round_idx=round_idx,\n        description=\"Run Wald tests.\",\n        clean_models=clean_models,\n    )\n\n    return local_states, wald_shared_state, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/deseq2_stats/deseq2_stats/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 local_H_matrix nparray \\((G, p, p)\\) The hat matrix with \\(\\texttt{nan}\\) entries for zero genes, used to compute Wald statistics. Should be merged with hat matrix. Each center Server 1 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 1 contrast_vector nparray \\((p,)\\) The contrast vector to apply to the LFC to get the desired log fold change corresponding to the input contrast of the form Of the form \\(\\texttt{[factor, level1, level2]}\\). Each center Server 1 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 1 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 1 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is retrieved from the current local state. Each center Server 1 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 2 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Server Center 2 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. This statistics depends on the \\(\\texttt{lfc\\_null}\\) parameter which sets the null hypothesis on the log fold change (set to \\(0\\) by default), and the \\(\\texttt{alt\\_hypothesis}\\) parameter, which defines the alternative hypothesis on the log fold change (set to \\(\\texttt{None}\\) by default, can be \\(\\texttt{greater}, ~\\texttt{greaterAbs}, ~\\texttt{less},~ \\texttt{lessAbs}\\)). If the alternative hypothesis is \\(\\texttt{None}\\), then the Wald statistic is computed as the centered normalized log fold change. Server Center 2 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server Center 2 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 2 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 3 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is retrieved from the current local state. Each center Server 3 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 3 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 3 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 3 cooks_cutoff float \\(()\\) The cutoff value for the Cook's distance, set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Each center Server 3 local_cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the Cook's distance is above the cutoff value for any sample in the center which i) has not been replaced by an imputation value if Cook's outliers have been refitted and ii) has at least three replicates across all centers. Each center Server 3 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 4 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a local Cook's outlier in any center. Computed from the local cooks outliers across all centers. Server Center 4 cooks_cutoff float \\(()\\) The cutoff value for the Cook's distance, set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Server Center 4 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 4 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 5 local_max_cooks nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the maximum Cook's distance across all samples in the center, if there is one above the cutoff value. Otherwise, \\(0\\). Each center Server 5 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 6 max_cooks nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the maximum Cook's distance across all samples in all centers, if there is one above the cutoff value. Otherwise, \\(0\\). Computed from the local max cooks across all centers. Server Center 6 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Server Center 7 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 7 local_max_cooks_gene_counts MaskedArray \\((G_{\\texttt{act}},)\\) A masked array where for each gene \\(g\\) with a Cook's outliers, the gene is masked if the sample maximizing the Cook's distance is not in the center, and the count value of the sample maximizing the Cook's distance otherwise. Each center Server 8 max_cooks_gene_counts MaskedArray \\((G_{\\texttt{act}},)\\) A masked array where for each gene \\(g\\) with a Cook's outliers, we have the minimum of the counts maximizing the Cook's distance across all centers. Should not be masked. Server Center 8 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Server Center 9 local_num_samples_above MaskedArray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the number of samples in the center whose gene count for gene \\(g\\) is above the gene count of the sample maximizing the Cook's distance across all centers for that gene. Each center Server 9 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 9 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Stored in the local state. Each center Server 9 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Stored in the local state. Each center Server 9 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Stored in the local state. Each center Server 10 p_values nparray \\((G,)\\) If Cook's filtering is enabled (which is the case by default with the $\\texttt{cooks_filter} $ parameter), then the p-values of genes which have \\(\\leq 2\\) samples above the gene count of the sample maximizing the Cook's distance across all centers are set to \\(\\texttt{nan}\\). Otherwise, passed on without modification. Server Center 10 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Passed on without modification. Server Center 10 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Passed on without modification. Server Center"},{"location":"api/core/deseq2_core/deseq2_stats/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 local_H_matrix nparray \\((G, p, p)\\) The hat matrix with \\(\\texttt{nan}\\) entries for zero genes, used to compute Wald statistics. Should be merged with hat matrix. Each center Server 1 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 1 contrast_vector nparray \\((p,)\\) The contrast vector to apply to the LFC to get the desired log fold change corresponding to the input contrast of the form Of the form \\(\\texttt{[factor, level1, level2]}\\). Each center Server 1 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 1 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 1 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is retrieved from the current local state. Each center Server 1 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 2 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Server Center 2 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. This statistics depends on the \\(\\texttt{lfc\\_null}\\) parameter which sets the null hypothesis on the log fold change (set to \\(0\\) by default), and the \\(\\texttt{alt\\_hypothesis}\\) parameter, which defines the alternative hypothesis on the log fold change (set to \\(\\texttt{None}\\) by default, can be \\(\\texttt{greater}, ~\\texttt{greaterAbs}, ~\\texttt{less},~ \\texttt{lessAbs}\\)). If the alternative hypothesis is \\(\\texttt{None}\\), then the Wald statistic is computed as the centered normalized log fold change. Server Center 2 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server Center 2 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 2 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 3 varEst nparray \\((G,)\\) For each gene \\(g\\), the trimmed variance estimate of the normed counts, denoted with \\(V^{\\texttt{trim}}_g\\). This quantitiy is retrieved from the current local state. Each center Server 3 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 3 mean_normed_counts nparray \\((G,)\\) For each gene, the mean of the local normed counts, i.e., \\(\\overline{Z}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{Z^{(k)}_{ig}}\\). Each center Server 3 _skip_cooks bool A boolean indicating whether to skip the computation of the intermediate quantities to compute the  of the Cook's distance. This is set to \\(\\texttt{True}\\) if the Cook's distance is stored in the local state (which is not the case by default due to memory issues). Otherwise, it is set to \\(\\texttt{False}\\) (default behaviour). Each center Server 3 cooks_cutoff float \\(()\\) The cutoff value for the Cook's distance, set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Each center Server 3 local_cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the Cook's distance is above the cutoff value for any sample in the center which i) has not been replaced by an imputation value if Cook's outliers have been refitted and ii) has at least three replicates across all centers. Each center Server 3 local_hat_matrix nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 4 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a local Cook's outlier in any center. Computed from the local cooks outliers across all centers. Server Center 4 cooks_cutoff float \\(()\\) The cutoff value for the Cook's distance, set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Server Center 4 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 4 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 5 local_max_cooks nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the maximum Cook's distance across all samples in the center, if there is one above the cutoff value. Otherwise, \\(0\\). Each center Server 5 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 6 max_cooks nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the maximum Cook's distance across all samples in all centers, if there is one above the cutoff value. Otherwise, \\(0\\). Computed from the local max cooks across all centers. Server Center 6 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Server Center 7 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 7 local_max_cooks_gene_counts MaskedArray \\((G_{\\texttt{act}},)\\) A masked array where for each gene \\(g\\) with a Cook's outliers, the gene is masked if the sample maximizing the Cook's distance is not in the center, and the count value of the sample maximizing the Cook's distance otherwise. Each center Server 8 max_cooks_gene_counts MaskedArray \\((G_{\\texttt{act}},)\\) A masked array where for each gene \\(g\\) with a Cook's outliers, we have the minimum of the counts maximizing the Cook's distance across all centers. Should not be masked. Server Center 8 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Server Center 9 local_num_samples_above MaskedArray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\) which has a Cook's outlier, the number of samples in the center whose gene count for gene \\(g\\) is above the gene count of the sample maximizing the Cook's distance across all centers for that gene. Each center Server 9 cooks_outliers nparray \\((G,)\\) A boolean array which for each gene \\(g\\) indicates if the gene is a Cook's outlier in any center. Passed on without modification. Each center Server 9 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic. Stored in the local state. Each center Server 9 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Stored in the local state. Each center Server 9 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Stored in the local state. Each center Server 10 p_values nparray \\((G,)\\) If Cook's filtering is enabled (which is the case by default with the $\\texttt{cooks_filter} $ parameter), then the p-values of genes which have \\(\\leq 2\\) samples above the gene count of the sample maximizing the Cook's distance across all centers are set to \\(\\texttt{nan}\\). Otherwise, passed on without modification. Server Center 10 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Passed on without modification. Server Center 10 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Passed on without modification. Server Center"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>replace_outliers</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#api","title":"API","text":""},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.replace_outliers","title":"<code>replace_outliers</code>","text":""},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.replace_outliers.ReplaceCooksOutliers","title":"<code>ReplaceCooksOutliers</code>","text":"<p>               Bases: <code>ComputeTrimmedMean</code>, <code>LocFindCooksOutliers</code>, <code>AggMergeOutlierGenes</code>, <code>LocReplaceCooksOutliers</code>, <code>LocSetRefitAdata</code>, <code>AggNewAllZeros</code>, <code>LocSetNewAllZerosAndGetFeatures</code></p> <p>Mixin class to replace Cook's outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/replace_outliers.py</code> <pre><code>class ReplaceCooksOutliers(\n    ComputeTrimmedMean,\n    LocFindCooksOutliers,\n    AggMergeOutlierGenes,\n    LocReplaceCooksOutliers,\n    LocSetRefitAdata,\n    AggNewAllZeros,\n    LocSetNewAllZerosAndGetFeatures,\n):\n    \"\"\"Mixin class to replace Cook's outliers.\"\"\"\n\n    trimmed_mean_num_iter: int\n\n    @log_organisation_method\n    def replace_outliers(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        cooks_shared_state,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Replace outlier counts.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[dict]\n            Local states. Required to propagate intermediate results.\n\n        cooks_shared_state: dict\n            Shared state with the dispersion values for Cook's distances, in a\n            \"cooks_dispersions\" key.\n\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states. The new local state contains Cook's distances.\n\n        shared_states: list[dict]\n            List of shared states with the features vector to input to\n            compute_genewise_dispersion in a \"local_features\" key.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        # Store trimmed means and find local Cooks outliers\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_find_cooks_outliers,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=cooks_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Find local Cooks outliers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Build the global list of genes for which to replace outliers\n        genes_to_replace_share_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_merge_outlier_genes,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Merge the lists of local outlier genes\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Store trimmed means and find local Cooks outliers\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_set_refit_adata,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=genes_to_replace_share_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Set the refit adata with the genes to replace\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Compute imputation values, on genes to refit only.\n        local_states, trimmed_means_shared_state, round_idx = self.compute_trim_mean(\n            train_data_nodes,\n            aggregation_node,\n            local_states,\n            round_idx,\n            clean_models=clean_models,\n            layer_used=\"normed_counts\",\n            trim_ratio=0.2,\n            mode=\"normal\",\n            n_iter=self.trimmed_mean_num_iter,\n            refit=True,\n        )\n\n        # Replace outliers in replaceable samples locally\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_replace_cooks_outliers,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=trimmed_means_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Replace Cooks outliers locally\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Find genes who have only have zero counts due to imputation\n\n        new_all_zeros_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.aggregate_new_all_zeros,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Find new all zero genes\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Set new all zeros genes and get features vector\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_set_new_all_zeros_get_features,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=new_all_zeros_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Set new all zero genes and get features vector\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.replace_outliers.ReplaceCooksOutliers.replace_outliers","title":"<code>replace_outliers(train_data_nodes, aggregation_node, local_states, cooks_shared_state, round_idx, clean_models)</code>","text":"<p>Replace outlier counts.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>cooks_shared_state</code> <p>Shared state with the dispersion values for Cook's distances, in a \"cooks_dispersions\" key.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. The new local state contains Cook's distances.</p> <code>shared_states</code> <code>list[dict]</code> <p>List of shared states with the features vector to input to compute_genewise_dispersion in a \"local_features\" key.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/replace_outliers.py</code> <pre><code>@log_organisation_method\ndef replace_outliers(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    cooks_shared_state,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Replace outlier counts.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[dict]\n        Local states. Required to propagate intermediate results.\n\n    cooks_shared_state: dict\n        Shared state with the dispersion values for Cook's distances, in a\n        \"cooks_dispersions\" key.\n\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states. The new local state contains Cook's distances.\n\n    shared_states: list[dict]\n        List of shared states with the features vector to input to\n        compute_genewise_dispersion in a \"local_features\" key.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    # Store trimmed means and find local Cooks outliers\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_find_cooks_outliers,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=cooks_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Find local Cooks outliers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Build the global list of genes for which to replace outliers\n    genes_to_replace_share_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_merge_outlier_genes,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Merge the lists of local outlier genes\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Store trimmed means and find local Cooks outliers\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_set_refit_adata,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=genes_to_replace_share_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Set the refit adata with the genes to replace\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Compute imputation values, on genes to refit only.\n    local_states, trimmed_means_shared_state, round_idx = self.compute_trim_mean(\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models=clean_models,\n        layer_used=\"normed_counts\",\n        trim_ratio=0.2,\n        mode=\"normal\",\n        n_iter=self.trimmed_mean_num_iter,\n        refit=True,\n    )\n\n    # Replace outliers in replaceable samples locally\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_replace_cooks_outliers,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=trimmed_means_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Replace Cooks outliers locally\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Find genes who have only have zero counts due to imputation\n\n    new_all_zeros_shared_state, round_idx = aggregation_step(\n        aggregation_method=self.aggregate_new_all_zeros,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Find new all zero genes\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Set new all zeros genes and get features vector\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.local_set_new_all_zeros_get_features,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=new_all_zeros_shared_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Set new all zero genes and get features vector\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    return local_states, shared_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.AggMergeOutlierGenes","title":"<code>AggMergeOutlierGenes</code>","text":"<p>Build the global list of genes to replace.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class AggMergeOutlierGenes:\n    \"\"\"Build the global list of genes to replace.\"\"\"\n\n    @remote\n    @log_remote\n    def agg_merge_outlier_genes(\n        self,\n        shared_states: list[dict],\n    ) -&gt; dict:\n        \"\"\"Merge the lists of genes to replace.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of dictionaries containing:\n            - \"local_genes_to_replace\": genes with Cook's distance above the threshold,\n            - \"replaceable_samples\": a boolean indicating whether there is at least\n               one sample with enough replicates to replace it.\n\n        Returns\n        -------\n        dict\n            A dictionary with a unique key: \"genes_to_replace\" containing the list\n            of genes for which to replace outlier values.\n        \"\"\"\n        # If no sample is replaceable, we can skip\n        any_replaceable = any(state[\"replaceable_samples\"] for state in shared_states)\n\n        if not any_replaceable:\n            return {\"genes_to_replace\": set()}\n\n        else:\n            # Take the union of all local list of genes to replace\n            genes_to_replace = set.union(\n                *[state[\"local_genes_to_replace\"] for state in shared_states]\n            )\n\n            return {\n                \"genes_to_replace\": genes_to_replace,\n            }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.AggMergeOutlierGenes.agg_merge_outlier_genes","title":"<code>agg_merge_outlier_genes(shared_states)</code>","text":"<p>Merge the lists of genes to replace.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of dictionaries containing: - \"local_genes_to_replace\": genes with Cook's distance above the threshold, - \"replaceable_samples\": a boolean indicating whether there is at least    one sample with enough replicates to replace it.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with a unique key: \"genes_to_replace\" containing the list of genes for which to replace outlier values.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_merge_outlier_genes(\n    self,\n    shared_states: list[dict],\n) -&gt; dict:\n    \"\"\"Merge the lists of genes to replace.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of dictionaries containing:\n        - \"local_genes_to_replace\": genes with Cook's distance above the threshold,\n        - \"replaceable_samples\": a boolean indicating whether there is at least\n           one sample with enough replicates to replace it.\n\n    Returns\n    -------\n    dict\n        A dictionary with a unique key: \"genes_to_replace\" containing the list\n        of genes for which to replace outlier values.\n    \"\"\"\n    # If no sample is replaceable, we can skip\n    any_replaceable = any(state[\"replaceable_samples\"] for state in shared_states)\n\n    if not any_replaceable:\n        return {\"genes_to_replace\": set()}\n\n    else:\n        # Take the union of all local list of genes to replace\n        genes_to_replace = set.union(\n            *[state[\"local_genes_to_replace\"] for state in shared_states]\n        )\n\n        return {\n            \"genes_to_replace\": genes_to_replace,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.AggNewAllZeros","title":"<code>AggNewAllZeros</code>","text":"<p>Mixin to compute the new all zeros and share to the centers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class AggNewAllZeros:\n    \"\"\"Mixin to compute the new all zeros and share to the centers.\"\"\"\n\n    @remote\n    @log_remote\n    def aggregate_new_all_zeros(self, shared_states: list) -&gt; dict:\n        \"\"\"Compute the global mean given the local results.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of results (local_mean, n_samples) from training nodes.\n            In refit mode, also contains \"loc_new_all_zero\".\n\n        Returns\n        -------\n        dict\n            New all-zero genes.\n        \"\"\"\n        # Find genes that are all zero due to imputation of counts\n        new_all_zeroes = np.all(\n            [state[\"loc_new_all_zeroes\"] for state in shared_states], axis=0\n        )\n\n        return {\"new_all_zeroes\": new_all_zeroes}\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.AggNewAllZeros.aggregate_new_all_zeros","title":"<code>aggregate_new_all_zeros(shared_states)</code>","text":"<p>Compute the global mean given the local results.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of results (local_mean, n_samples) from training nodes. In refit mode, also contains \"loc_new_all_zero\".</p> required <p>Returns:</p> Type Description <code>dict</code> <p>New all-zero genes.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote\n@log_remote\ndef aggregate_new_all_zeros(self, shared_states: list) -&gt; dict:\n    \"\"\"Compute the global mean given the local results.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of results (local_mean, n_samples) from training nodes.\n        In refit mode, also contains \"loc_new_all_zero\".\n\n    Returns\n    -------\n    dict\n        New all-zero genes.\n    \"\"\"\n    # Find genes that are all zero due to imputation of counts\n    new_all_zeroes = np.all(\n        [state[\"loc_new_all_zeroes\"] for state in shared_states], axis=0\n    )\n\n    return {\"new_all_zeroes\": new_all_zeroes}\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocFindCooksOutliers","title":"<code>LocFindCooksOutliers</code>","text":"<p>Find local Cooks outliers.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class LocFindCooksOutliers:\n    \"\"\"Find local Cooks outliers.\"\"\"\n\n    local_adata: AnnData\n    min_replicates: int\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_find_cooks_outliers(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Find local Cooks outliers by comparing the cooks distance to a threshold.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict, optional\n            Not used.\n\n        Returns\n        -------\n        dict\n            Shared state containing:\n            - \"local_genes_to_replace\": genes with Cook's distance above the threshold,\n            - \"replaceable_samples\": a boolean indicating whether there is at least one\n               sample with enough replicates to replace it.\n        \"\"\"\n        # Find replaceable samples\n        n_or_more = self.local_adata.uns[\"num_replicates\"] &gt;= self.min_replicates\n\n        self.local_adata.obsm[\"replaceable\"] = n_or_more[\n            self.local_adata.obs[\"cells\"]\n        ].values\n\n        # Find genes with Cook's distance above the threshold\n        n_params = self.local_adata.uns[\"n_params\"]\n        cooks_cutoff = f.ppf(\n            0.99, n_params, self.local_adata.uns[\"tot_num_samples\"] - n_params\n        )\n\n        self.local_adata.uns[\"_where_cooks_g_cutoff\"] = np.where(\n            self.local_adata.layers[\"cooks\"] &gt; cooks_cutoff\n        )\n\n        local_idx_to_replace = (self.local_adata.layers[\"cooks\"] &gt; cooks_cutoff).any(\n            axis=0\n        )\n        local_genes_to_replace = self.local_adata.var_names[local_idx_to_replace]\n\n        return {\n            \"local_genes_to_replace\": set(local_genes_to_replace),\n            \"replaceable_samples\": self.local_adata.obsm[\"replaceable\"].any(),\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocFindCooksOutliers.loc_find_cooks_outliers","title":"<code>loc_find_cooks_outliers(data_from_opener, shared_state)</code>","text":"<p>Find local Cooks outliers by comparing the cooks distance to a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Shared state containing: - \"local_genes_to_replace\": genes with Cook's distance above the threshold, - \"replaceable_samples\": a boolean indicating whether there is at least one    sample with enough replicates to replace it.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_find_cooks_outliers(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Find local Cooks outliers by comparing the cooks distance to a threshold.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict, optional\n        Not used.\n\n    Returns\n    -------\n    dict\n        Shared state containing:\n        - \"local_genes_to_replace\": genes with Cook's distance above the threshold,\n        - \"replaceable_samples\": a boolean indicating whether there is at least one\n           sample with enough replicates to replace it.\n    \"\"\"\n    # Find replaceable samples\n    n_or_more = self.local_adata.uns[\"num_replicates\"] &gt;= self.min_replicates\n\n    self.local_adata.obsm[\"replaceable\"] = n_or_more[\n        self.local_adata.obs[\"cells\"]\n    ].values\n\n    # Find genes with Cook's distance above the threshold\n    n_params = self.local_adata.uns[\"n_params\"]\n    cooks_cutoff = f.ppf(\n        0.99, n_params, self.local_adata.uns[\"tot_num_samples\"] - n_params\n    )\n\n    self.local_adata.uns[\"_where_cooks_g_cutoff\"] = np.where(\n        self.local_adata.layers[\"cooks\"] &gt; cooks_cutoff\n    )\n\n    local_idx_to_replace = (self.local_adata.layers[\"cooks\"] &gt; cooks_cutoff).any(\n        axis=0\n    )\n    local_genes_to_replace = self.local_adata.var_names[local_idx_to_replace]\n\n    return {\n        \"local_genes_to_replace\": set(local_genes_to_replace),\n        \"replaceable_samples\": self.local_adata.obsm[\"replaceable\"].any(),\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocReplaceCooksOutliers","title":"<code>LocReplaceCooksOutliers</code>","text":"<p>Mixin to replace cooks outliers locally.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class LocReplaceCooksOutliers:\n    \"\"\"Mixin to replace cooks outliers locally.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_replace_cooks_outliers(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; dict:\n        \"\"\"Replace outlier counts with imputed values.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            A dictionary with a \"trimmed_mean_normed_counts\" key, containing the\n            trimmed means to use to compute the imputed values.\n\n        Returns\n        -------\n        dict\n            A dictionary containing:\n            - \"loc_new_all_zero\": a boolean array indicating which genes are now\n              all-zero.\n        \"\"\"\n        # Set the trimmed mean normed counts in the varm\n        self.refit_adata.varm[\"_trimmed_mean_normed_counts\"] = shared_state[\n            \"trimmed_mean_normed_counts\"\n        ]\n\n        set_imputed_counts_refit_adata(self)\n\n        # Find new all-zero columns\n        new_all_zeroes = self.refit_adata.X.sum(axis=0) == 0\n\n        # Return the new local logmeans\n        with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n            return {\n                \"loc_new_all_zeroes\": new_all_zeroes,\n            }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocReplaceCooksOutliers.loc_replace_cooks_outliers","title":"<code>loc_replace_cooks_outliers(data_from_opener, shared_state)</code>","text":"<p>Replace outlier counts with imputed values.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>A dictionary with a \"trimmed_mean_normed_counts\" key, containing the trimmed means to use to compute the imputed values.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing: - \"loc_new_all_zero\": a boolean array indicating which genes are now   all-zero.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_replace_cooks_outliers(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; dict:\n    \"\"\"Replace outlier counts with imputed values.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        A dictionary with a \"trimmed_mean_normed_counts\" key, containing the\n        trimmed means to use to compute the imputed values.\n\n    Returns\n    -------\n    dict\n        A dictionary containing:\n        - \"loc_new_all_zero\": a boolean array indicating which genes are now\n          all-zero.\n    \"\"\"\n    # Set the trimmed mean normed counts in the varm\n    self.refit_adata.varm[\"_trimmed_mean_normed_counts\"] = shared_state[\n        \"trimmed_mean_normed_counts\"\n    ]\n\n    set_imputed_counts_refit_adata(self)\n\n    # Find new all-zero columns\n    new_all_zeroes = self.refit_adata.X.sum(axis=0) == 0\n\n    # Return the new local logmeans\n    with np.errstate(divide=\"ignore\"):  # ignore division by zero warnings\n        return {\n            \"loc_new_all_zeroes\": new_all_zeroes,\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocSetNewAllZerosAndGetFeatures","title":"<code>LocSetNewAllZerosAndGetFeatures</code>","text":"<p>Mixin to set the new all zeros and return local features.</p> <p>This Mixin implements the method to perform the transition towards the compute_rough_dispersions steps after refitting. It sets the new all zeros genes in the local AnnData and computes the local features to be shared to the aggregation node.</p> <p>Methods:</p> Name Description <code>local_set_new_all_zeros_get_features</code> <p>The method to set the new all zeros genes and compute the local features.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class LocSetNewAllZerosAndGetFeatures:\n    \"\"\"Mixin to set the new all zeros and return local features.\n\n    This Mixin implements the method to perform the transition towards the\n    compute_rough_dispersions steps after refitting. It sets the new all zeros\n    genes in the local AnnData and computes the local features to be shared\n    to the aggregation node.\n\n    Methods\n    -------\n    local_set_new_all_zeros_get_features\n        The method to set the new all zeros genes and compute the local features.\n    \"\"\"\n\n    local_adata: ad.AnnData\n    refit_adata: ad.AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_set_new_all_zeros_get_features(\n        self,\n        data_from_opener,\n        shared_state,\n    ) -&gt; dict:\n        \"\"\"Set the new_all_zeros field and get the features.\n\n        This method is used to set the new_all_zeros field in the local_adata uns\n        field. This is the set of genes that are all zero after outlier replacement.\n\n        It then restricts the refit_adata to the genes which are not all_zero.\n\n        Finally, it computes the local features to be shared via shared_state to the\n        aggregation node.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Shared state containing the \"new_all_zeroes\" key.\n\n        Returns\n        -------\n        dict\n            Local feature vector to be shared via shared_state to\n            the aggregation node.\n        \"\"\"\n        # Take all-zero genes into account\n        new_all_zeroes = shared_state[\"new_all_zeroes\"]\n\n        self.local_adata.uns[\"new_all_zeroes_genes\"] = self.refit_adata.var_names[\n            new_all_zeroes\n        ]\n\n        self.local_adata.varm[\"refitted\"] = self.local_adata.varm[\"replaced\"].copy()\n        # Only replace if genes are not all zeroes after outlier replacement\n        self.local_adata.varm[\"refitted\"][\n            self.local_adata.varm[\"refitted\"]\n        ] = ~new_all_zeroes\n\n        # RESTRICT REFIT ADATA TO NOT NEW ALL ZEROES\n        self.refit_adata = self.refit_adata[:, ~new_all_zeroes].copy()\n\n        # Update normed counts\n        set_normed_counts(self.refit_adata)\n\n        #### ---- Compute Gram matrix and feature vector ---- ####\n\n        design = self.refit_adata.obsm[\"design_matrix\"].values\n\n        return {\n            \"local_features\": design.T @ self.refit_adata.layers[\"normed_counts\"],\n        }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocSetNewAllZerosAndGetFeatures.local_set_new_all_zeros_get_features","title":"<code>local_set_new_all_zeros_get_features(data_from_opener, shared_state)</code>","text":"<p>Set the new_all_zeros field and get the features.</p> <p>This method is used to set the new_all_zeros field in the local_adata uns field. This is the set of genes that are all zero after outlier replacement.</p> <p>It then restricts the refit_adata to the genes which are not all_zero.</p> <p>Finally, it computes the local features to be shared via shared_state to the aggregation node.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared state containing the \"new_all_zeroes\" key.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Local feature vector to be shared via shared_state to the aggregation node.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_set_new_all_zeros_get_features(\n    self,\n    data_from_opener,\n    shared_state,\n) -&gt; dict:\n    \"\"\"Set the new_all_zeros field and get the features.\n\n    This method is used to set the new_all_zeros field in the local_adata uns\n    field. This is the set of genes that are all zero after outlier replacement.\n\n    It then restricts the refit_adata to the genes which are not all_zero.\n\n    Finally, it computes the local features to be shared via shared_state to the\n    aggregation node.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Shared state containing the \"new_all_zeroes\" key.\n\n    Returns\n    -------\n    dict\n        Local feature vector to be shared via shared_state to\n        the aggregation node.\n    \"\"\"\n    # Take all-zero genes into account\n    new_all_zeroes = shared_state[\"new_all_zeroes\"]\n\n    self.local_adata.uns[\"new_all_zeroes_genes\"] = self.refit_adata.var_names[\n        new_all_zeroes\n    ]\n\n    self.local_adata.varm[\"refitted\"] = self.local_adata.varm[\"replaced\"].copy()\n    # Only replace if genes are not all zeroes after outlier replacement\n    self.local_adata.varm[\"refitted\"][\n        self.local_adata.varm[\"refitted\"]\n    ] = ~new_all_zeroes\n\n    # RESTRICT REFIT ADATA TO NOT NEW ALL ZEROES\n    self.refit_adata = self.refit_adata[:, ~new_all_zeroes].copy()\n\n    # Update normed counts\n    set_normed_counts(self.refit_adata)\n\n    #### ---- Compute Gram matrix and feature vector ---- ####\n\n    design = self.refit_adata.obsm[\"design_matrix\"].values\n\n    return {\n        \"local_features\": design.T @ self.refit_adata.layers[\"normed_counts\"],\n    }\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocSetRefitAdata","title":"<code>LocSetRefitAdata</code>","text":"<p>Mixin to replace cooks outliers locally.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>class LocSetRefitAdata:\n    \"\"\"Mixin to replace cooks outliers locally.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_set_refit_adata(\n        self,\n        data_from_opener,\n        shared_state: dict,\n    ) -&gt; None:\n        \"\"\"Set a refit adata containing the counts of the genes to replace.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            A dictionary with a \"genes_to_replace\" key, containing the list of genes\n            for which to replace outlier values.\n        \"\"\"\n        # Save the information on which genes will be replaced\n        genes_to_replace = pd.Series(False, index=self.local_adata.var_names)\n        genes_to_replace[list(shared_state[\"genes_to_replace\"])] = True\n        self.local_adata.varm[\"replaced\"] = genes_to_replace.values\n\n        # Copy the values corresponding to the genes to refit in the refit_adata\n        set_basic_refit_adata(self)\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#fedpydeseq2.core.deseq2_core.replace_outliers.substeps.LocSetRefitAdata.loc_set_refit_adata","title":"<code>loc_set_refit_adata(data_from_opener, shared_state)</code>","text":"<p>Set a refit adata containing the counts of the genes to replace.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>A dictionary with a \"genes_to_replace\" key, containing the list of genes for which to replace outlier values.</p> required Source code in <code>fedpydeseq2/core/deseq2_core/replace_outliers/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_set_refit_adata(\n    self,\n    data_from_opener,\n    shared_state: dict,\n) -&gt; None:\n    \"\"\"Set a refit adata containing the counts of the genes to replace.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        A dictionary with a \"genes_to_replace\" key, containing the list of genes\n        for which to replace outlier values.\n    \"\"\"\n    # Save the information on which genes will be replaced\n    genes_to_replace = pd.Series(False, index=self.local_adata.var_names)\n    genes_to_replace[list(shared_state[\"genes_to_replace\"])] = True\n    self.local_adata.varm[\"replaced\"] = genes_to_replace.values\n\n    # Copy the values corresponding to the genes to refit in the refit_adata\n    set_basic_refit_adata(self)\n</code></pre>"},{"location":"api/core/deseq2_core/replace_outliers/replace_outliers/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 0 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 0 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 1 local_genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in the center. For a given gene \\(g\\) and given sample \\(i\\), the Cook's distance is computed as \\(\\tfrac{h^{(k)}_{ig}}{p (1-h^{(k)}_{ig})^2}~(R^2)^{(k)}_{ig}\\) where \\((R^2)^{(k)}_{ig}\\) is the squared Pearson residual of the negative binomial GLM with log fold changes \\(\\beta_g\\) and dispersions \\(\\alpha^{\\texttt{cooks}}_g\\), computed as \\((Y^{(k)}_{ig} - \\mu^{(k)}_{ig})^2/(V^{\\texttt{NB}})^{(k)}_{ig}\\), and \\(h^{(k)}_{ig}\\) is the \\(i\\)-th diagonal element of \\(X^{(k)} H^{-1}_{g} (X^{(k)})^{\\top}\\), where \\(H^{-1}_{g}\\) is the inverse of the global hat matrix. The cutoff value is set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Here, \\(\\mu^{(k)}_{ig} = \\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) and \\((V^{\\texttt{NB}})^{(k)}_{ig} = \\mu^{(k)}_{ig} (1 + \\mu^{(k)}_{ig} \\alpha^{\\texttt{cooks}}_g)\\). Each center Server 1 replaceable_samples bool \\(()\\) A boolean indicating if there are any replaceable samples in the center. A sample \\(i\\) of center \\(k\\) is said to be replaceable if there are at least \\(\\texttt{min\\_replicates}\\) samples across all centers which share the same design factor levels as \\(i\\). \\(\\texttt{min\\_replicates}\\) is a user defined parameter, set to \\(7\\) by default. Each center Server 2 genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server Center 4 use_lvl bool Whether or not to use the levels of the design matrix to compute the trimmed mean. Set to \\(\\texttt{False}\\) in this case, so that the trimmed mean is computed across all samples. Each center Server 4 max_values nparray \\((G_{\\texttt{r}},)\\) Contains the maximum value of the center normed counts for each gene to replace \\(g\\) across all samples. Each center Server 4 min_values nparray \\((G_{\\texttt{r}},)\\) Contains the minimum value of the center normed counts for each gene to replace \\(g\\)  across all samples. Each center Server 5 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Server Center 5 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Contains an upper bound of the \\(r_{\\texttt{trim}}\\)-quantile value of the normed counts for each gene to replace \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the upper bound of the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the upper bound of the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Server Center 5 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Contains a lower bound of the \\(r_{\\texttt{trim}}\\)-quantile value of the normed counts for each gene to replace \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the lower bound of the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the lower bound of the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Server Center 6 trim_ratio float The trim ratio, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.2\\). Each center Server 6 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 6 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 6 num_strictly_above nparray \\((G_{\\texttt{r}}, p)\\) Contains the number of samples whose normed counts value for each gene to replace \\(g\\) is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value of gene \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Each center Server 6 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Each center Server 6 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 7 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Server Center 7 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) for each gene to replace \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across all samples. For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across all samples, depending on the number of samples whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 7 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) for each gene to replace \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across all samples. For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across all samples, depending on the number of samples whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 8 trim_ratio float the trim ratio, set to \\(0.2\\) Each center Server 8 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 8 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 8 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 8 trimmed_local_sum nparray \\((G_{\\texttt{r}},)\\) A numpy array of shape \\((G,)\\) containing the sum of the normed counts across all samples, whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile. For each gene to replace \\(g\\), the entry is the trimmed local sum. Each center Server 8 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Each center Server 8 num_strictly_above nparray \\((G_{\\texttt{r}}, p)\\) Contains the number of samples whose normed counts value for each gene to replace \\(g\\) is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value of gene \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Each center Server 9 trimmed_mean_normed_counts nparray \\((G_{\\texttt{r}},)\\) For each gene to replace \\(g\\), the trimmed mean of the normed counts across all samples, denoted with \\(\\overline{Z}^{\\texttt{trim}}_{g}\\), with trim ratio set to \\(0.2\\). Server Center 10 loc_new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if the new count matrix of the center is all zeroes (across samples) for this gene (the new count matrix is computed by imputing the Cook's outliers with the trimmed mean of the normed counts times the size factor). Each center Server 11 new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if all counts across all centers are zero for this gene. Server Center 12 local_features nparray \\((p, G_{\\texttt{act}})\\) \\(\\Phi^{(k)}  := (X^{(k)})^{\\top} Z^{(k)}\\), where \\(Z^{(k)}\\) is the normalized counts in the center \\(k\\) on the set of genes to replace, where the value of Cook's outliers have been replaced using a trimmed mean. Each center Server"},{"location":"api/core/deseq2_core/replace_outliers/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 0 cooks_dispersions nparray \\((G,)\\) For each gene \\(g\\), a robust estimate of the dispersion parameter \\(\\alpha^{\\texttt{cooks}}_g\\) computed from the trimmed variance estimate and the global mean of the normed counts. We compute this estimate as \\(\\max((V^{\\texttt{trim}}_g- \\overline{Y}_g)/\\overline{Y}_g^2,0.04)\\). Server Center 0 global_hat_matrix_inv nparray \\((G_{\\texttt{nz}}, p, p)\\) For each gene \\(g\\), we compute the global hat matrix as the sum of the local hat matrices, and its inverse. Server Center 1 local_genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in the center. For a given gene \\(g\\) and given sample \\(i\\), the Cook's distance is computed as \\(\\tfrac{h^{(k)}_{ig}}{p (1-h^{(k)}_{ig})^2}~(R^2)^{(k)}_{ig}\\) where \\((R^2)^{(k)}_{ig}\\) is the squared Pearson residual of the negative binomial GLM with log fold changes \\(\\beta_g\\) and dispersions \\(\\alpha^{\\texttt{cooks}}_g\\), computed as \\((Y^{(k)}_{ig} - \\mu^{(k)}_{ig})^2/(V^{\\texttt{NB}})^{(k)}_{ig}\\), and \\(h^{(k)}_{ig}\\) is the \\(i\\)-th diagonal element of \\(X^{(k)} H^{-1}_{g} (X^{(k)})^{\\top}\\), where \\(H^{-1}_{g}\\) is the inverse of the global hat matrix. The cutoff value is set to the \\(0.99\\)-th quantile of the F-distribution with \\(p\\) and \\(n-p\\) degrees of freedom. Here, \\(\\mu^{(k)}_{ig} = \\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) and \\((V^{\\texttt{NB}})^{(k)}_{ig} = \\mu^{(k)}_{ig} (1 + \\mu^{(k)}_{ig} \\alpha^{\\texttt{cooks}}_g)\\). Each center Server 1 replaceable_samples bool \\(()\\) A boolean indicating if there are any replaceable samples in the center. A sample \\(i\\) of center \\(k\\) is said to be replaceable if there are at least \\(\\texttt{min\\_replicates}\\) samples across all centers which share the same design factor levels as \\(i\\). \\(\\texttt{min\\_replicates}\\) is a user defined parameter, set to \\(7\\) by default. Each center Server 2 genes_to_replace set The set of genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server Center 4 use_lvl bool Whether or not to use the levels of the design matrix to compute the trimmed mean. Set to \\(\\texttt{False}\\) in this case, so that the trimmed mean is computed across all samples. Each center Server 4 max_values nparray \\((G_{\\texttt{r}},)\\) Contains the maximum value of the center normed counts for each gene to replace \\(g\\) across all samples. Each center Server 4 min_values nparray \\((G_{\\texttt{r}},)\\) Contains the minimum value of the center normed counts for each gene to replace \\(g\\)  across all samples. Each center Server 5 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Server Center 5 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Contains an upper bound of the \\(r_{\\texttt{trim}}\\)-quantile value of the normed counts for each gene to replace \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the upper bound of the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the upper bound of the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Server Center 5 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Contains a lower bound of the \\(r_{\\texttt{trim}}\\)-quantile value of the normed counts for each gene to replace \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the lower bound of the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the lower bound of the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Server Center 6 trim_ratio float The trim ratio, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.2\\). Each center Server 6 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 6 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 6 num_strictly_above nparray \\((G_{\\texttt{r}}, p)\\) Contains the number of samples whose normed counts value for each gene to replace \\(g\\) is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value of gene \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Each center Server 6 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Each center Server 6 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 7 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Server Center 7 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) for each gene to replace \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across all samples. For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across all samples, depending on the number of samples whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 7 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) for each gene to replace \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across all samples. For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across all samples, depending on the number of samples whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 8 trim_ratio float the trim ratio, set to \\(0.2\\) Each center Server 8 lower_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 8 upper_bounds_thresholds nparray \\((G_{\\texttt{r}}, p)\\) Simply passed on from the previous state. Each center Server 8 n_samples int The number of samples in a center \\(n_k\\) for each center \\(k\\). Each center Server 8 trimmed_local_sum nparray \\((G_{\\texttt{r}},)\\) A numpy array of shape \\((G,)\\) containing the sum of the normed counts across all samples, whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile. For each gene to replace \\(g\\), the entry is the trimmed local sum. Each center Server 8 use_lvl bool Set to \\(\\texttt{False}\\) in this case, simply passed on from the previous state. Each center Server 8 num_strictly_above nparray \\((G_{\\texttt{r}}, p)\\) Contains the number of samples whose normed counts value for each gene to replace \\(g\\) is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value of gene \\(g\\) across all samples. for each gene to replace \\(g\\), the first entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(r_{\\texttt{trim}}\\)-quantile value, and the second entry is the number of samples whose normed counts value is strictly above the average of the upper and lower bounds on the \\(1-r_{\\texttt{trim}}\\)-quantile value, where \\(r_{\\texttt{trim}}=0.2\\). Each center Server 9 trimmed_mean_normed_counts nparray \\((G_{\\texttt{r}},)\\) For each gene to replace \\(g\\), the trimmed mean of the normed counts across all samples, denoted with \\(\\overline{Z}^{\\texttt{trim}}_{g}\\), with trim ratio set to \\(0.2\\). Server Center 10 loc_new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if the new count matrix of the center is all zeroes (across samples) for this gene (the new count matrix is computed by imputing the Cook's outliers with the trimmed mean of the normed counts times the size factor). Each center Server 11 new_all_zeroes nparray \\((G_{\\texttt{r}},)\\) A boolean array which for each gene to replace \\(g\\) indicates if all counts across all centers are zero for this gene. Server Center 12 local_features nparray \\((p, G_{\\texttt{act}})\\) \\(\\Phi^{(k)}  := (X^{(k)})^{\\top} Z^{(k)}\\), where \\(Z^{(k)}\\) is the normalized counts in the center \\(k\\) on the set of genes to replace, where the value of Cook's outliers have been replaced using a trimmed mean. Each center Server"},{"location":"api/core/deseq2_core/replace_refitted_values/replace_refitted_values/","title":"API","text":""},{"location":"api/core/deseq2_core/replace_refitted_values/replace_refitted_values/#fedpydeseq2.core.deseq2_core.replace_refitted_values.replace_refitted_values","title":"<code>replace_refitted_values</code>","text":""},{"location":"api/core/deseq2_core/replace_refitted_values/replace_refitted_values/#fedpydeseq2.core.deseq2_core.replace_refitted_values.replace_refitted_values.ReplaceRefittedValues","title":"<code>ReplaceRefittedValues</code>","text":"<p>Mixin class to replace refitted values.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_refitted_values/replace_refitted_values.py</code> <pre><code>class ReplaceRefittedValues:\n    \"\"\"Mixin class to replace refitted values.\"\"\"\n\n    local_adata: ad.AnnData | None\n    refit_adata: ad.AnnData | None\n\n    @log_organisation_method\n    def replace_refitted_values(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Replace the values that were refitted in `local_adata`s.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: list[Dict]\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        Returns\n        -------\n        local_states: dict\n            Local states, with refitted values\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_replace_refitted_values,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Replace refitted values in local adatas\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        return local_states, round_idx\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_replace_refitted_values(self, data_from_opener, shared_state):\n        \"\"\"Replace refitted values in local_adata from refit_adata.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict\n            Not used.\n        \"\"\"\n        # Replace values in main object\n        list_varm_keys = [\n            \"_normed_means\",\n            \"LFC\",\n            \"genewise_dispersions\",\n            \"fitted_dispersions\",\n            \"MAP_dispersions\",\n            \"dispersions\",\n        ]\n        for key in list_varm_keys:\n            self.local_adata.varm[key][self.local_adata.varm[\"refitted\"]] = (\n                self.refit_adata.varm[key]\n            )\n\n        # Take into account new all-zero genes\n        new_all_zeroes_genes = self.local_adata.uns[\"new_all_zeroes_genes\"]\n        if len(new_all_zeroes_genes) &gt; 0:\n            self.local_adata.varm[\"_normed_means\"][\n                self.local_adata.var_names.get_indexer(new_all_zeroes_genes)\n            ] = 0\n            self.local_adata.varm[\"LFC\"].loc[new_all_zeroes_genes, :] = 0\n</code></pre>"},{"location":"api/core/deseq2_core/replace_refitted_values/replace_refitted_values/#fedpydeseq2.core.deseq2_core.replace_refitted_values.replace_refitted_values.ReplaceRefittedValues.loc_replace_refitted_values","title":"<code>loc_replace_refitted_values(data_from_opener, shared_state)</code>","text":"<p>Replace refitted values in local_adata from refit_adata.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> required Source code in <code>fedpydeseq2/core/deseq2_core/replace_refitted_values/replace_refitted_values.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_replace_refitted_values(self, data_from_opener, shared_state):\n    \"\"\"Replace refitted values in local_adata from refit_adata.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict\n        Not used.\n    \"\"\"\n    # Replace values in main object\n    list_varm_keys = [\n        \"_normed_means\",\n        \"LFC\",\n        \"genewise_dispersions\",\n        \"fitted_dispersions\",\n        \"MAP_dispersions\",\n        \"dispersions\",\n    ]\n    for key in list_varm_keys:\n        self.local_adata.varm[key][self.local_adata.varm[\"refitted\"]] = (\n            self.refit_adata.varm[key]\n        )\n\n    # Take into account new all-zero genes\n    new_all_zeroes_genes = self.local_adata.uns[\"new_all_zeroes_genes\"]\n    if len(new_all_zeroes_genes) &gt; 0:\n        self.local_adata.varm[\"_normed_means\"][\n            self.local_adata.var_names.get_indexer(new_all_zeroes_genes)\n        ] = 0\n        self.local_adata.varm[\"LFC\"].loc[new_all_zeroes_genes, :] = 0\n</code></pre>"},{"location":"api/core/deseq2_core/replace_refitted_values/replace_refitted_values/#fedpydeseq2.core.deseq2_core.replace_refitted_values.replace_refitted_values.ReplaceRefittedValues.replace_refitted_values","title":"<code>replace_refitted_values(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Replace the values that were refitted in <code>local_adata</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states, with refitted values</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/deseq2_core/replace_refitted_values/replace_refitted_values.py</code> <pre><code>@log_organisation_method\ndef replace_refitted_values(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Replace the values that were refitted in `local_adata`s.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: list[Dict]\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    Returns\n    -------\n    local_states: dict\n        Local states, with refitted values\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_replace_refitted_values,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Replace refitted values in local adatas\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    return local_states, round_idx\n</code></pre>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>save_pipeline_results</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/#api","title":"API","text":"<p>Module to implement Mixin to get results as a shared state.</p>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/#fedpydeseq2.core.deseq2_core.save_pipeline_results.SavePipelineResults","title":"<code>SavePipelineResults</code>","text":"<p>               Bases: <code>AggPassOnResults</code></p> <p>Mixin class to save pipeline results.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>Local AnnData object.</p> <code>results</code> <code>dict</code> <p>Results to share.</p> <code>VARM_KEYS</code> <code>list</code> <p>List of keys to extract from the varm attribute.</p> <code>UNS_KEYS</code> <code>list</code> <p>List of keys to extract from the uns attribute.</p> <p>Methods:</p> Name Description <code>save_pipeline_results</code> <p>Save the pipeline results. These results will be downloaded at the end of the pipeline. They are defined using the VARM_KEYS and UNS_KEYS attributes.</p> <code>get_results_from_local_states</code> <p>Get the results to share from the local states.</p> Source code in <code>fedpydeseq2/core/deseq2_core/save_pipeline_results.py</code> <pre><code>class SavePipelineResults(AggPassOnResults):\n    \"\"\"Mixin class to save pipeline results.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        Local AnnData object.\n\n    results : dict\n        Results to share.\n\n    VARM_KEYS : list\n        List of keys to extract from the varm attribute.\n\n    UNS_KEYS : list\n        List of keys to extract from the uns attribute.\n\n    Methods\n    -------\n    save_pipeline_results\n        Save the pipeline results.\n        These results will be downloaded at the end of the pipeline.\n        They are defined using the VARM_KEYS and UNS_KEYS attributes.\n\n    get_results_from_local_states\n        Get the results to share from the local states.\n    \"\"\"\n\n    local_adata: ad.AnnData\n    results: dict | None\n\n    VARM_KEYS = [\n        \"MAP_dispersions\",\n        \"dispersions\",\n        \"genewise_dispersions\",\n        \"non_zero\",\n        \"fitted_dispersions\",\n        \"LFC\",\n        \"padj\",\n        \"p_values\",\n        \"wald_statistics\",\n        \"wald_se\",\n        \"replaced\",\n        \"refitted\",\n    ]\n\n    UNS_KEYS = [\n        \"prior_disp_var\",\n        \"_squared_logres\",\n        \"contrast\",\n    ]\n\n    @log_organisation_method\n    def save_pipeline_results(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx,\n        clean_models,\n    ):\n        \"\"\"Build the results that will be downloaded at the end of the pipeline.\n\n        Parameters\n        ----------\n        train_data_nodes: list[TrainDataNode]\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            Index of the current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n        \"\"\"\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.get_results_from_local_states,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Get results to share from the local centers\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        # Build the global list of genes for which to replace outliers\n        aggregation_step(\n            aggregation_method=self.pass_on_results,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Merge the lists of results and return output\",\n            round_idx=round_idx,\n            clean_models=False,\n        )\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def get_results_from_local_states(\n        self,\n        data_from_opener,\n        shared_state: dict | None,\n    ) -&gt; dict:\n        \"\"\"Get the results to share from the local states.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            AnnData returned by the opener. Not used.\n\n        shared_state : dict, optional\n            Not used.\n\n        Returns\n        -------\n        dict\n            Shared state containing the gene names, as well\n            as selected fields from the varm and uns attributes.\n        \"\"\"\n        shared_state = {\n            \"gene_names\": self.local_adata.var_names,\n        }\n        for varm_key in self.VARM_KEYS:\n            if varm_key in self.local_adata.varm.keys():\n                shared_state[varm_key] = self.local_adata.varm[varm_key]\n            else:\n                shared_state[varm_key] = None\n\n        for uns_key in self.UNS_KEYS:\n            shared_state[uns_key] = self.local_adata.uns[uns_key]\n\n        return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/#fedpydeseq2.core.deseq2_core.save_pipeline_results.SavePipelineResults.get_results_from_local_states","title":"<code>get_results_from_local_states(data_from_opener, shared_state)</code>","text":"<p>Get the results to share from the local states.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>AnnData returned by the opener. Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Not used.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Shared state containing the gene names, as well as selected fields from the varm and uns attributes.</p> Source code in <code>fedpydeseq2/core/deseq2_core/save_pipeline_results.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef get_results_from_local_states(\n    self,\n    data_from_opener,\n    shared_state: dict | None,\n) -&gt; dict:\n    \"\"\"Get the results to share from the local states.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        AnnData returned by the opener. Not used.\n\n    shared_state : dict, optional\n        Not used.\n\n    Returns\n    -------\n    dict\n        Shared state containing the gene names, as well\n        as selected fields from the varm and uns attributes.\n    \"\"\"\n    shared_state = {\n        \"gene_names\": self.local_adata.var_names,\n    }\n    for varm_key in self.VARM_KEYS:\n        if varm_key in self.local_adata.varm.keys():\n            shared_state[varm_key] = self.local_adata.varm[varm_key]\n        else:\n            shared_state[varm_key] = None\n\n    for uns_key in self.UNS_KEYS:\n        shared_state[uns_key] = self.local_adata.uns[uns_key]\n\n    return shared_state\n</code></pre>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/#fedpydeseq2.core.deseq2_core.save_pipeline_results.SavePipelineResults.save_pipeline_results","title":"<code>save_pipeline_results(train_data_nodes, aggregation_node, local_states, round_idx, clean_models)</code>","text":"<p>Build the results that will be downloaded at the end of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <p>Index of the current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required Source code in <code>fedpydeseq2/core/deseq2_core/save_pipeline_results.py</code> <pre><code>@log_organisation_method\ndef save_pipeline_results(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx,\n    clean_models,\n):\n    \"\"\"Build the results that will be downloaded at the end of the pipeline.\n\n    Parameters\n    ----------\n    train_data_nodes: list[TrainDataNode]\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        Index of the current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n    \"\"\"\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.get_results_from_local_states,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Get results to share from the local centers\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    # Build the global list of genes for which to replace outliers\n    aggregation_step(\n        aggregation_method=self.pass_on_results,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Merge the lists of results and return output\",\n        round_idx=round_idx,\n        clean_models=False,\n    )\n</code></pre>"},{"location":"api/core/deseq2_core/save_pipeline_results/save_pipeline_results/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 gene_names Index \\((G,)\\) The gene names. Each center Server 1 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Each center Server 1 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Each center Server 1 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Each center Server 1 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Each center Server 1 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Each center Server 1 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Each center Server 1 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Each center Server 1 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Each center Server 1 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 1 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Each center Server 1 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 1 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 1 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Each center Server 1 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 1 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Each center Server 2 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server All 2 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Server All 2 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server All 2 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server All 2 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Server All 2 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Server All 2 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Server All 2 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server All 2 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server All 2 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 2 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Server All 2 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 2 gene_names Index \\((G,)\\) The gene names. Server All 2 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server All 2 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Server All 2 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server All"},{"location":"api/core/deseq2_core/save_pipeline_results/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 gene_names Index \\((G,)\\) The gene names. Each center Server 1 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Each center Server 1 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Each center Server 1 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Each center Server 1 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Each center Server 1 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Each center Server 1 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Each center Server 1 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Each center Server 1 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Each center Server 1 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Each center Server 1 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Each center Server 1 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 1 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 1 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Each center Server 1 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Each center Server 1 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Each center Server 2 prior_disp_var float \\(()\\) A prior on the variance of the log dispersions around the log trend curve \\(\\sigma_{\\texttt{trend}}^2\\), estimated as the maximum between \\(0.25\\) and \\(\\texttt{\\_squared\\_log\\_res} - \\psi_1((n-p)/2)\\), where \\(\\psi_1(f/2)\\) is the variance of the log of a \\(\\chi^2_f\\) distribution. For more details, see \\citep{love2014deseq2}. Server All 2 refitted nparray \\((G,)\\) A boolean array marking genes which can be replaced and which, after replacing the count value by the imputation value, are non-zero. Server All 2 replaced nparray \\((G,)\\) A boolean array marking genes \\(g\\) for which the Cook's distance is above the cutoff value for any sample in any center (the union of the local genes to replace across all centers). Server All 2 wald_se nparray \\((G,)\\) For each gene, the standard error on the log fold change value for the given contrast given by the GLM. Server All 2 wald_statistics nparray \\((G,)\\) For each gene, the Wald statistic of the gene expression. Server All 2 p_values nparray \\((G,)\\) For each gene, the p-value of the Wald statistic, computed from the survival function of the normal distribution applied to the wald statistic, and set to \\(\\texttt{nan}\\) if the gene is a Cook's outlier if \\(\\texttt{cooks\\_filter}\\) is enabled. Server All 2 padj Series \\((G,)\\) The adjusted p-values for each gene, computed from the p-values using independent filtering if the \\(\\texttt{independent\\_filter}\\) parameter is \\(\\texttt{True}\\) (default), and the Benjamini-Hochberg procedure otherwise. Server All 2 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Server All 2 fitted_dispersions nparray \\((G,)\\) For each gene \\(g\\) with zero counts across all centers, \\(\\texttt{nan}\\). For each non-zero gene \\(g\\), either \\(\\alpha_{\\texttt{trend}}(\\overline{Z}_g)\\) if the disp_function_type is \"parametric\" (i.e., the fitting of the parameters has converged), or \\(\\texttt{mean\\_disp}\\) otherwise (i.e., if the disp_function_type is \"mean\"). Denoted with \\(\\alpha^{\\texttt{trend}}_g\\). Server All 2 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 2 dispersions nparray \\((G,)\\) The estimated dispersions for each gene, which are the MAP dispersions if the gene is not an outlier w.r.t. the trend curve, and the gene-wise dispersions otherwise. Server All 2 MAP_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the MAP dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices, and the regularization coming from the prior on the log dispersions around the trend curve. The Cox-Reid, prior regularized nll per gene and per dispersion in the grid is obtained by summing the regularization terms and the nll. Finally, for every gene, the MAP dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server All 2 gene_names Index \\((G,)\\) The gene names. Server All 2 _squared_logres float \\(()\\) The squared mean absolute deviation of the difference between the log of the genewise dispersions and the log of the fitted dispersions, restricted to the non-zero genes whose gene-wise dispersions are above \\(100\\times \\texttt{min\\_disp}\\). The mean absolute deviation estimate is defined as the median of the absolute difference between the log residual and its median, scaled by the percent point function of the normal distribution at \\(0.75\\). Server All 2 LFC DataFrame \\((G, p)\\) The log fold changes of the gene expression. This dataframe is indexed by genes on one hand, and by design column names on the other. Server All 2 contrast list A list of three strings representing the contrast of interest, in case it is not specified by the user. Of the form \\(\\texttt{[factor, level1, level2]}\\). For example, \\(\\texttt{[stage, Advanced, Non-advanced]}\\). Server All"},{"location":"api/core/federated_algorithms/federated_algorithms/","title":"Federated algorithms","text":"<p>In this sub-module of the <code>fedpydeseq2</code> package, we implement federated algorithms that are used multiple times.</p>"},{"location":"api/core/federated_algorithms/federated_algorithms/#federated-trimmed-mean-compute_trimmed_mean","title":"Federated trimmed mean: compute_trimmed_mean","text":""},{"location":"api/core/federated_algorithms/federated_algorithms/#dispersions-grid-search-dispersions_grid_search","title":"Dispersions grid search: dispersions_grid_search","text":""},{"location":"api/core/federated_algorithms/federated_algorithms/#federated-irls-fed_irls","title":"Federated IRLS: fed_irls","text":""},{"location":"api/core/federated_algorithms/federated_algorithms/#federated-prox-quasi-newton-fed_pqn","title":"Federated Prox Quasi Newton fed_PQN","text":""},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>compute_trim_mean</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#api","title":"API","text":""},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.compute_trimmed_mean","title":"<code>compute_trimmed_mean</code>","text":"<p>Module containing the steps to compute trimmed mean.</p>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.compute_trimmed_mean.ComputeTrimmedMean","title":"<code>ComputeTrimmedMean</code>","text":"<p>               Bases: <code>LocInitTrimmedMean</code>, <code>AggInitTrimmedMean</code>, <code>LocalIterationTrimmedMean</code>, <code>AggIterationTrimmedMean</code>, <code>LocFinalTrimmedMean</code>, <code>AggFinalTrimmedMean</code></p> <p>Strategy to compute the trimmed mean.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/compute_trimmed_mean.py</code> <pre><code>class ComputeTrimmedMean(\n    LocInitTrimmedMean,\n    AggInitTrimmedMean,\n    LocalIterationTrimmedMean,\n    AggIterationTrimmedMean,\n    LocFinalTrimmedMean,\n    AggFinalTrimmedMean,\n):\n    \"\"\"Strategy to compute the trimmed mean.\"\"\"\n\n    @log_organisation_method\n    def compute_trim_mean(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        round_idx: int,\n        clean_models: bool,\n        layer_used: str,\n        mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n        trim_ratio: float | None = None,\n        n_iter: int = 50,\n        refit: bool = False,\n        min_replicates_trimmed_mean: int = 3,\n    ):\n        \"\"\"Run the trimmed mean computation on the layer specified.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        layer_used : str\n            The layer used to compute the trimmed mean.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            If True, the models are cleaned.\n\n        mode : Literal[\"normal\", \"cooks\"]\n            The mode to use. If \"cooks\", the local trimmed mean is actually computed\n            per level, and predefined trim ratios are applied, as well as certain\n            scaling factors on the outputed means.\n            If \"normal\", the local trimmed mean is computed on the whole dataset, as\n            expected, using the trim_ratio parameter.\n\n        trim_ratio : float, optional\n            The ratio to trim. Should be between 0 and 0.5.\n            Is only used in \"normal\" mode, and should be None in \"cooks\" mode.\n\n        n_iter : int\n            The number of iterations.\n\n        refit : bool\n            If True, the function will compute the trimmed mean on the refit adata only.\n\n        min_replicates_trimmed_mean : int\n            The minimum number of replicates to compute the trimmed mean.\n\n        Returns\n        -------\n        local_states: list[dict]\n            Local states dictionaries.\n\n        final_trimmed_mean_agg_share_state: dict\n            Dictionary containing the final trimmed mean aggregation share\n            state in a field \"trimmed_mean_&lt;layer_used&gt;\".\n\n        round_idx: int\n        \"\"\"\n        if mode == \"cooks\":\n            assert trim_ratio is None, \"trim_ratio should be None in cooks mode\"\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.loc_init_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=None,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Initialize trim mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\n                \"layer_used\": layer_used,\n                \"mode\": mode,\n                \"refit\": refit,\n                \"min_replicates_trimmed_mean\": min_replicates_trimmed_mean,\n            },\n        )\n\n        aggregation_share_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_init_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Aggregation init of trimmed mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n\n        start_loop()\n        for iteration in range(n_iter):\n            start_iteration(iteration)\n            local_states, shared_states, round_idx = local_step(\n                local_method=self.local_iteration_trimmed_mean,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                input_local_states=local_states,\n                input_shared_state=aggregation_share_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Local iteration of trimmed mean\",\n                round_idx=round_idx,\n                clean_models=clean_models,\n                method_params={\n                    \"layer_used\": layer_used,\n                    \"mode\": mode,\n                    \"trim_ratio\": trim_ratio,\n                    \"refit\": refit,\n                },\n            )\n\n            aggregation_share_state, round_idx = aggregation_step(\n                aggregation_method=self.agg_iteration_trimmed_mean,\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                input_shared_states=shared_states,\n                description=\"Aggregation iteration of trimmed mean\",\n                round_idx=round_idx,\n                clean_models=clean_models,\n            )\n            end_iteration()\n        end_loop()\n\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.final_local_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=aggregation_share_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Final local step of trimmed mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\n                \"layer_used\": layer_used,\n                \"trim_ratio\": trim_ratio,\n                \"mode\": mode,\n                \"refit\": refit,\n            },\n        )\n\n        final_trimmed_mean_agg_share_state, round_idx = aggregation_step(\n            aggregation_method=self.final_agg_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"final aggregation of trimmed mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\"layer_used\": layer_used, \"mode\": mode},\n        )\n\n        return local_states, final_trimmed_mean_agg_share_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.compute_trimmed_mean.ComputeTrimmedMean.compute_trim_mean","title":"<code>compute_trim_mean(train_data_nodes, aggregation_node, local_states, round_idx, clean_models, layer_used, mode='normal', trim_ratio=None, n_iter=50, refit=False, min_replicates_trimmed_mean=3)</code>","text":"<p>Run the trimmed mean computation on the layer specified.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>layer_used</code> <code>str</code> <p>The layer used to compute the trimmed mean.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>round_idx</code> <code>int</code> <p>The current round.</p> required <code>clean_models</code> <code>bool</code> <p>If True, the models are cleaned.</p> required <code>mode</code> <code>Literal['normal', 'cooks']</code> <p>The mode to use. If \"cooks\", the local trimmed mean is actually computed per level, and predefined trim ratios are applied, as well as certain scaling factors on the outputed means. If \"normal\", the local trimmed mean is computed on the whole dataset, as expected, using the trim_ratio parameter.</p> <code>'normal'</code> <code>trim_ratio</code> <code>float</code> <p>The ratio to trim. Should be between 0 and 0.5. Is only used in \"normal\" mode, and should be None in \"cooks\" mode.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>The number of iterations.</p> <code>50</code> <code>refit</code> <code>bool</code> <p>If True, the function will compute the trimmed mean on the refit adata only.</p> <code>False</code> <code>min_replicates_trimmed_mean</code> <code>int</code> <p>The minimum number of replicates to compute the trimmed mean.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>list[dict]</code> <p>Local states dictionaries.</p> <code>final_trimmed_mean_agg_share_state</code> <code>dict</code> <p>Dictionary containing the final trimmed mean aggregation share state in a field \"trimmed_mean_\". <code>round_idx</code> <code>int</code> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/compute_trimmed_mean.py</code> <pre><code>@log_organisation_method\ndef compute_trim_mean(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    round_idx: int,\n    clean_models: bool,\n    layer_used: str,\n    mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n    trim_ratio: float | None = None,\n    n_iter: int = 50,\n    refit: bool = False,\n    min_replicates_trimmed_mean: int = 3,\n):\n    \"\"\"Run the trimmed mean computation on the layer specified.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    layer_used : str\n        The layer used to compute the trimmed mean.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        If True, the models are cleaned.\n\n    mode : Literal[\"normal\", \"cooks\"]\n        The mode to use. If \"cooks\", the local trimmed mean is actually computed\n        per level, and predefined trim ratios are applied, as well as certain\n        scaling factors on the outputed means.\n        If \"normal\", the local trimmed mean is computed on the whole dataset, as\n        expected, using the trim_ratio parameter.\n\n    trim_ratio : float, optional\n        The ratio to trim. Should be between 0 and 0.5.\n        Is only used in \"normal\" mode, and should be None in \"cooks\" mode.\n\n    n_iter : int\n        The number of iterations.\n\n    refit : bool\n        If True, the function will compute the trimmed mean on the refit adata only.\n\n    min_replicates_trimmed_mean : int\n        The minimum number of replicates to compute the trimmed mean.\n\n    Returns\n    -------\n    local_states: list[dict]\n        Local states dictionaries.\n\n    final_trimmed_mean_agg_share_state: dict\n        Dictionary containing the final trimmed mean aggregation share\n        state in a field \"trimmed_mean_&lt;layer_used&gt;\".\n\n    round_idx: int\n    \"\"\"\n    if mode == \"cooks\":\n        assert trim_ratio is None, \"trim_ratio should be None in cooks mode\"\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.loc_init_trimmed_mean,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=None,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Initialize trim mean\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\n            \"layer_used\": layer_used,\n            \"mode\": mode,\n            \"refit\": refit,\n            \"min_replicates_trimmed_mean\": min_replicates_trimmed_mean,\n        },\n    )\n\n    aggregation_share_state, round_idx = aggregation_step(\n        aggregation_method=self.agg_init_trimmed_mean,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"Aggregation init of trimmed mean\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n    )\n\n    start_loop()\n    for iteration in range(n_iter):\n        start_iteration(iteration)\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_iteration_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=aggregation_share_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Local iteration of trimmed mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\n                \"layer_used\": layer_used,\n                \"mode\": mode,\n                \"trim_ratio\": trim_ratio,\n                \"refit\": refit,\n            },\n        )\n\n        aggregation_share_state, round_idx = aggregation_step(\n            aggregation_method=self.agg_iteration_trimmed_mean,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Aggregation iteration of trimmed mean\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n        )\n        end_iteration()\n    end_loop()\n\n    local_states, shared_states, round_idx = local_step(\n        local_method=self.final_local_trimmed_mean,\n        train_data_nodes=train_data_nodes,\n        output_local_states=local_states,\n        input_local_states=local_states,\n        input_shared_state=aggregation_share_state,\n        aggregation_id=aggregation_node.organization_id,\n        description=\"Final local step of trimmed mean\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\n            \"layer_used\": layer_used,\n            \"trim_ratio\": trim_ratio,\n            \"mode\": mode,\n            \"refit\": refit,\n        },\n    )\n\n    final_trimmed_mean_agg_share_state, round_idx = aggregation_step(\n        aggregation_method=self.final_agg_trimmed_mean,\n        train_data_nodes=train_data_nodes,\n        aggregation_node=aggregation_node,\n        input_shared_states=shared_states,\n        description=\"final aggregation of trimmed mean\",\n        round_idx=round_idx,\n        clean_models=clean_models,\n        method_params={\"layer_used\": layer_used, \"mode\": mode},\n    )\n\n    return local_states, final_trimmed_mean_agg_share_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps","title":"<code>substeps</code>","text":"<p>Module to implement the substeps for comuting the trimmed mean.</p> <p>This module contains all these substeps as mixin classes.</p>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggFinalTrimmedMean","title":"<code>AggFinalTrimmedMean</code>","text":"<p>Mixin class of the aggregation of the finalisation of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class AggFinalTrimmedMean:\n    \"\"\"Mixin class of the aggregation of the finalisation of the trimmed mean algo.\"\"\"\n\n    @remote\n    @log_remote\n    def final_agg_trimmed_mean(\n        self,\n        shared_states: list[dict],\n        layer_used: str,\n        mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n    ) -&gt; dict:\n        \"\"\"Compute the initial global upper and lower bounds.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of dictionnaries with the following keys:\n            - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n            - n_samples : np.ndarray(int) of size (n_genes,2)\n            - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n            - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mode : Literal[\"normal\", \"cooks\"]\n            Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n            either on the normalized counts or the squared error.\n            It will be applied per level, except if there are not enough samples.\n            Moreover, trim ratios will be computed based on the number of replicates.\n            If \"normal\", the function will be applied on the whole dataset, using the\n            trim_ratio parameter.\n\n\n        Returns\n        -------\n        dict\n            If mode is \"cooks\" and if the layer is \"sqerror\", a dictionary with the\n            \"varEst\" key containing\n                - The maximum of the trimmed means per level if use_level is true,\n                rescaled by a scale factor depending on the number of replicates\n                - The trimmed mean of the whole dataset otherwise\n                scaled by 1.51.\n            else, if mode is cooks and use_lvl is true, a dictionary with a\n            trimmed_mean_normed_counts key containing a dataframe\n            with the trimmed means per level, levels being columns\n            else, a dictionary with the following keys:\n                - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)\n        \"\"\"\n        use_lvl = shared_states[0][\"use_lvl\"]\n        if mode == \"cooks\" and use_lvl:\n            result = {}\n            for lvl in shared_states[0].keys():\n                if lvl == \"use_lvl\":\n                    continue\n                result[lvl] = self.final_agg_trimmed_mean_per_lvl(\n                    [state[lvl] for state in shared_states], layer_used\n                )[f\"trimmed_mean_{layer_used}\"]\n            if layer_used == \"sqerror\":\n                return {\"varEst\": pd.DataFrame.from_dict(result).max(axis=1).to_numpy()}\n            else:\n                return {f\"trimmed_mean_{layer_used}\": pd.DataFrame.from_dict(result)}\n        elif mode == \"cooks\" and layer_used == \"sqerror\":\n            return {\n                \"varEst\": self.final_agg_trimmed_mean_per_lvl(\n                    shared_states, layer_used\n                )[\"trimmed_mean_sqerror\"]\n            }\n        return self.final_agg_trimmed_mean_per_lvl(shared_states, layer_used)\n\n    def final_agg_trimmed_mean_per_lvl(\n        self,\n        shared_states: list[dict],\n        layer_used: str,\n    ) -&gt; dict:\n        \"\"\"Aggregate step of the finalisation of the trimmed mean algo.\n\n        Parameters\n        ----------\n        shared_states :  list[dict]\n            List of dictionary containing the following keys:\n                - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n                - n_samples : np.ndarray(int) of size (n_genes,2)\n                - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n                - upper_bounds : np.ndarray of size (n_genes,2)\n                - lower_bounds : np.ndarray of size (n_genes,2)\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys:\n                - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)\n        \"\"\"\n        trim_ratio = shared_states[0][\"trim_ratio\"]\n        n_samples = np.sum([state[\"n_samples\"] for state in shared_states])\n        agg_n_samples_strictly_above_quantiles = np.sum(\n            [state[\"num_strictly_above\"] for state in shared_states],\n            axis=0,\n        )\n        n_trim = np.floor(n_samples * trim_ratio)\n        targets = np.array([n_trim, n_samples - n_trim])\n        effective_n_samples = n_samples - 2 * n_trim\n        trimmed_sum = np.sum(\n            [state[\"trimmed_local_sum\"] for state in shared_states], axis=0\n        )\n        current_thresholds = (\n            shared_states[0][\"upper_bounds_thresholds\"]\n            + shared_states[0][\"lower_bounds_thresholds\"]\n        ) / 2.0\n\n        # The following lines deal with the \"tie\" cases, i.e. where duplicate values\n        # fall on both part of the \"n_trimmed\" position. In that case,\n        # agg_n_samples_strictly_above_quantiles is different from target.\n        # \"delta_sample_above_quantile\" encode how many elements were wrongly\n        # trimmed or not trimmed. We know that these elements were close to the\n        # values of the threshold up to ~2^{-n_iter} precision. We can then correct the\n        # trimmed sum easily using the threshold values.\n\n        delta_sample_above_quantile = (\n            agg_n_samples_strictly_above_quantiles - targets[None, :]\n        )\n        trimmed_sum = (\n            trimmed_sum\n            + delta_sample_above_quantile[..., 0] * current_thresholds[..., 0]\n        )\n        trimmed_sum = (\n            trimmed_sum\n            - delta_sample_above_quantile[..., 1] * current_thresholds[..., 1]\n        )\n        trimmed_mean = trimmed_sum / effective_n_samples\n        if \"scale\" in shared_states[0].keys():\n            scale = shared_states[0][\"scale\"]\n            trimmed_mean = trimmed_mean * scale\n        return {f\"trimmed_mean_{layer_used}\": trimmed_mean}\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggFinalTrimmedMean.final_agg_trimmed_mean","title":"<code>final_agg_trimmed_mean(shared_states, layer_used, mode='normal')</code>","text":"<p>Compute the initial global upper and lower bounds.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of dictionnaries with the following keys: - trimmed_local_sum : np.ndarray(float) of size (n_genes,2) - n_samples : np.ndarray(int) of size (n_genes,2) - num_strictly_above : np.ndarray(int) of size (n_genes,2) - upper_bounds_thresholds : np.ndarray of size (n_genes,2) - lower_bounds_thresholds : np.ndarray of size (n_genes,2) If use_lvl is true, the dictionary is nested with the levels as keys.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mode</code> <code>Literal['normal', 'cooks']</code> <p>Mode of the trimmed mean algo. If \"cooks\", the function will be applied either on the normalized counts or the squared error. It will be applied per level, except if there are not enough samples. Moreover, trim ratios will be computed based on the number of replicates. If \"normal\", the function will be applied on the whole dataset, using the trim_ratio parameter.</p> <code>'normal'</code> <p>Returns:</p> Type Description <code>dict</code> <p>If mode is \"cooks\" and if the layer is \"sqerror\", a dictionary with the \"varEst\" key containing     - The maximum of the trimmed means per level if use_level is true,     rescaled by a scale factor depending on the number of replicates     - The trimmed mean of the whole dataset otherwise     scaled by 1.51. else, if mode is cooks and use_lvl is true, a dictionary with a trimmed_mean_normed_counts key containing a dataframe with the trimmed means per level, levels being columns else, a dictionary with the following keys:     - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote\n@log_remote\ndef final_agg_trimmed_mean(\n    self,\n    shared_states: list[dict],\n    layer_used: str,\n    mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n) -&gt; dict:\n    \"\"\"Compute the initial global upper and lower bounds.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of dictionnaries with the following keys:\n        - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n        - n_samples : np.ndarray(int) of size (n_genes,2)\n        - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n        - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n        - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mode : Literal[\"normal\", \"cooks\"]\n        Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n        either on the normalized counts or the squared error.\n        It will be applied per level, except if there are not enough samples.\n        Moreover, trim ratios will be computed based on the number of replicates.\n        If \"normal\", the function will be applied on the whole dataset, using the\n        trim_ratio parameter.\n\n\n    Returns\n    -------\n    dict\n        If mode is \"cooks\" and if the layer is \"sqerror\", a dictionary with the\n        \"varEst\" key containing\n            - The maximum of the trimmed means per level if use_level is true,\n            rescaled by a scale factor depending on the number of replicates\n            - The trimmed mean of the whole dataset otherwise\n            scaled by 1.51.\n        else, if mode is cooks and use_lvl is true, a dictionary with a\n        trimmed_mean_normed_counts key containing a dataframe\n        with the trimmed means per level, levels being columns\n        else, a dictionary with the following keys:\n            - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)\n    \"\"\"\n    use_lvl = shared_states[0][\"use_lvl\"]\n    if mode == \"cooks\" and use_lvl:\n        result = {}\n        for lvl in shared_states[0].keys():\n            if lvl == \"use_lvl\":\n                continue\n            result[lvl] = self.final_agg_trimmed_mean_per_lvl(\n                [state[lvl] for state in shared_states], layer_used\n            )[f\"trimmed_mean_{layer_used}\"]\n        if layer_used == \"sqerror\":\n            return {\"varEst\": pd.DataFrame.from_dict(result).max(axis=1).to_numpy()}\n        else:\n            return {f\"trimmed_mean_{layer_used}\": pd.DataFrame.from_dict(result)}\n    elif mode == \"cooks\" and layer_used == \"sqerror\":\n        return {\n            \"varEst\": self.final_agg_trimmed_mean_per_lvl(\n                shared_states, layer_used\n            )[\"trimmed_mean_sqerror\"]\n        }\n    return self.final_agg_trimmed_mean_per_lvl(shared_states, layer_used)\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggFinalTrimmedMean.final_agg_trimmed_mean_per_lvl","title":"<code>final_agg_trimmed_mean_per_lvl(shared_states, layer_used)</code>","text":"<p>Aggregate step of the finalisation of the trimmed mean algo.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code> list[dict]</code> <p>List of dictionary containing the following keys:     - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)     - n_samples : np.ndarray(int) of size (n_genes,2)     - num_strictly_above : np.ndarray(int) of size (n_genes,2)     - upper_bounds : np.ndarray of size (n_genes,2)     - lower_bounds : np.ndarray of size (n_genes,2)</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys:     - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def final_agg_trimmed_mean_per_lvl(\n    self,\n    shared_states: list[dict],\n    layer_used: str,\n) -&gt; dict:\n    \"\"\"Aggregate step of the finalisation of the trimmed mean algo.\n\n    Parameters\n    ----------\n    shared_states :  list[dict]\n        List of dictionary containing the following keys:\n            - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n            - n_samples : np.ndarray(int) of size (n_genes,2)\n            - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n            - upper_bounds : np.ndarray of size (n_genes,2)\n            - lower_bounds : np.ndarray of size (n_genes,2)\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys:\n            - trimmed_mean_layer_used : np.ndarray(float) of size (n_genes)\n    \"\"\"\n    trim_ratio = shared_states[0][\"trim_ratio\"]\n    n_samples = np.sum([state[\"n_samples\"] for state in shared_states])\n    agg_n_samples_strictly_above_quantiles = np.sum(\n        [state[\"num_strictly_above\"] for state in shared_states],\n        axis=0,\n    )\n    n_trim = np.floor(n_samples * trim_ratio)\n    targets = np.array([n_trim, n_samples - n_trim])\n    effective_n_samples = n_samples - 2 * n_trim\n    trimmed_sum = np.sum(\n        [state[\"trimmed_local_sum\"] for state in shared_states], axis=0\n    )\n    current_thresholds = (\n        shared_states[0][\"upper_bounds_thresholds\"]\n        + shared_states[0][\"lower_bounds_thresholds\"]\n    ) / 2.0\n\n    # The following lines deal with the \"tie\" cases, i.e. where duplicate values\n    # fall on both part of the \"n_trimmed\" position. In that case,\n    # agg_n_samples_strictly_above_quantiles is different from target.\n    # \"delta_sample_above_quantile\" encode how many elements were wrongly\n    # trimmed or not trimmed. We know that these elements were close to the\n    # values of the threshold up to ~2^{-n_iter} precision. We can then correct the\n    # trimmed sum easily using the threshold values.\n\n    delta_sample_above_quantile = (\n        agg_n_samples_strictly_above_quantiles - targets[None, :]\n    )\n    trimmed_sum = (\n        trimmed_sum\n        + delta_sample_above_quantile[..., 0] * current_thresholds[..., 0]\n    )\n    trimmed_sum = (\n        trimmed_sum\n        - delta_sample_above_quantile[..., 1] * current_thresholds[..., 1]\n    )\n    trimmed_mean = trimmed_sum / effective_n_samples\n    if \"scale\" in shared_states[0].keys():\n        scale = shared_states[0][\"scale\"]\n        trimmed_mean = trimmed_mean * scale\n    return {f\"trimmed_mean_{layer_used}\": trimmed_mean}\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggInitTrimmedMean","title":"<code>AggInitTrimmedMean</code>","text":"<p>Mixin class for the aggregation of the init of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class AggInitTrimmedMean:\n    \"\"\"Mixin class for the aggregation of the init of the trimmed mean algo.\"\"\"\n\n    @remote\n    @log_remote\n    def agg_init_trimmed_mean(\n        self,\n        shared_states: list[dict],\n    ) -&gt; dict:\n        \"\"\"Compute the initial global upper and lower bounds.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            If use_lvl is False (in any shared state),\n            list of dictionaries with the following keys:\n                - max_values: np.ndarray of size (n_genes,)\n                - min_values: np.ndarray of size (n_genes,)\n            If use_lvl is True, list of dictionaries with the same keys as above\n            nested inside a dictionary with the levels as keys.\n\n        Returns\n        -------\n        dict\n            use_level is a key present in all input shared states, and will be passed\n            on to the output shared state\n            If use_lvl is False, dict with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n               - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n            otherwise, a dictionary with the same keys for nested inside a dictionary\n            with the levels as keys.\n        \"\"\"\n        use_lvl = shared_states[0][\"use_lvl\"]\n        result = {\"use_lvl\": use_lvl}\n        if use_lvl:\n            for lvl in shared_states[0].keys():\n                if lvl == \"use_lvl\":\n                    continue\n                result[lvl] = self.agg_init_trimmed_mean_per_lvl(\n                    [state[lvl] for state in shared_states]\n                )\n            return result\n        else:\n            result.update(self.agg_init_trimmed_mean_per_lvl(shared_states))\n            return result\n\n    def agg_init_trimmed_mean_per_lvl(self, shared_states: list[dict]) -&gt; dict:\n        \"\"\"Compute the initial global upper and lower bounds.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of dictionaries with the following keys:\n                - max_values: np.ndarray of size (n_genes,)\n                - min_values: np.ndarray of size (n_genes,)\n\n        Returns\n        -------\n        dict\n            dict with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n               - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n        \"\"\"\n        # To initialize the dichotomic search of the quantile thresholds, we need to\n        # set the upper and lower bounds of the thresholds.\n        upper_bounds_thresholds = np.nanmax(\n            np.array([state[\"max_values\"] for state in shared_states]), axis=0\n        )\n        lower_bounds_thresholds = np.nanmin(\n            np.array([state[\"min_values\"] for state in shared_states]), axis=0\n        )\n\n        # We are looking for two thresholds, one for the upper quantile and one for the\n        # lower quantile. We initialize the search with the same value for both.\n        upper_bounds_thresholds = np.vstack([upper_bounds_thresholds] * 2).T\n        lower_bounds_thresholds = np.vstack([lower_bounds_thresholds] * 2).T\n\n        upper_bounds_thresholds = upper_bounds_thresholds.astype(np.float32)\n        lower_bounds_thresholds = lower_bounds_thresholds.astype(np.float32)\n\n        return {\n            \"upper_bounds_thresholds\": upper_bounds_thresholds,\n            \"lower_bounds_thresholds\": lower_bounds_thresholds,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggInitTrimmedMean.agg_init_trimmed_mean","title":"<code>agg_init_trimmed_mean(shared_states)</code>","text":"<p>Compute the initial global upper and lower bounds.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>If use_lvl is False (in any shared state), list of dictionaries with the following keys:     - max_values: np.ndarray of size (n_genes,)     - min_values: np.ndarray of size (n_genes,) If use_lvl is True, list of dictionaries with the same keys as above nested inside a dictionary with the levels as keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>use_level is a key present in all input shared states, and will be passed on to the output shared state If use_lvl is False, dict with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)    - lower_bounds_thresholds : np.ndarray of size (n_genes, 2) otherwise, a dictionary with the same keys for nested inside a dictionary with the levels as keys.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_init_trimmed_mean(\n    self,\n    shared_states: list[dict],\n) -&gt; dict:\n    \"\"\"Compute the initial global upper and lower bounds.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        If use_lvl is False (in any shared state),\n        list of dictionaries with the following keys:\n            - max_values: np.ndarray of size (n_genes,)\n            - min_values: np.ndarray of size (n_genes,)\n        If use_lvl is True, list of dictionaries with the same keys as above\n        nested inside a dictionary with the levels as keys.\n\n    Returns\n    -------\n    dict\n        use_level is a key present in all input shared states, and will be passed\n        on to the output shared state\n        If use_lvl is False, dict with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n           - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n        otherwise, a dictionary with the same keys for nested inside a dictionary\n        with the levels as keys.\n    \"\"\"\n    use_lvl = shared_states[0][\"use_lvl\"]\n    result = {\"use_lvl\": use_lvl}\n    if use_lvl:\n        for lvl in shared_states[0].keys():\n            if lvl == \"use_lvl\":\n                continue\n            result[lvl] = self.agg_init_trimmed_mean_per_lvl(\n                [state[lvl] for state in shared_states]\n            )\n        return result\n    else:\n        result.update(self.agg_init_trimmed_mean_per_lvl(shared_states))\n        return result\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggInitTrimmedMean.agg_init_trimmed_mean_per_lvl","title":"<code>agg_init_trimmed_mean_per_lvl(shared_states)</code>","text":"<p>Compute the initial global upper and lower bounds.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of dictionaries with the following keys:     - max_values: np.ndarray of size (n_genes,)     - min_values: np.ndarray of size (n_genes,)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)    - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def agg_init_trimmed_mean_per_lvl(self, shared_states: list[dict]) -&gt; dict:\n    \"\"\"Compute the initial global upper and lower bounds.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of dictionaries with the following keys:\n            - max_values: np.ndarray of size (n_genes,)\n            - min_values: np.ndarray of size (n_genes,)\n\n    Returns\n    -------\n    dict\n        dict with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n           - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n    \"\"\"\n    # To initialize the dichotomic search of the quantile thresholds, we need to\n    # set the upper and lower bounds of the thresholds.\n    upper_bounds_thresholds = np.nanmax(\n        np.array([state[\"max_values\"] for state in shared_states]), axis=0\n    )\n    lower_bounds_thresholds = np.nanmin(\n        np.array([state[\"min_values\"] for state in shared_states]), axis=0\n    )\n\n    # We are looking for two thresholds, one for the upper quantile and one for the\n    # lower quantile. We initialize the search with the same value for both.\n    upper_bounds_thresholds = np.vstack([upper_bounds_thresholds] * 2).T\n    lower_bounds_thresholds = np.vstack([lower_bounds_thresholds] * 2).T\n\n    upper_bounds_thresholds = upper_bounds_thresholds.astype(np.float32)\n    lower_bounds_thresholds = lower_bounds_thresholds.astype(np.float32)\n\n    return {\n        \"upper_bounds_thresholds\": upper_bounds_thresholds,\n        \"lower_bounds_thresholds\": lower_bounds_thresholds,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggIterationTrimmedMean","title":"<code>AggIterationTrimmedMean</code>","text":"<p>Mixin class of the aggregation of the iteration of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class AggIterationTrimmedMean:\n    \"\"\"Mixin class of the aggregation of the iteration of the trimmed mean algo.\"\"\"\n\n    @remote\n    @log_remote\n    def agg_iteration_trimmed_mean(\n        self,\n        shared_states: list[dict],\n    ) -&gt; dict:\n        \"\"\"Compute the initial global upper and lower bounds.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of dictionnaries with the following keys:\n                - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n                - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - n_samples: int\n                - trim_ratio: float\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n               - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n        \"\"\"\n        use_lvl = shared_states[0][\"use_lvl\"]\n        result = {\"use_lvl\": use_lvl}\n        if use_lvl:\n            for lvl in shared_states[0].keys():\n                if lvl == \"use_lvl\":\n                    continue\n                result[lvl] = self.agg_iteration_trimmed_mean_per_lvl(\n                    [state[lvl] for state in shared_states]\n                )\n            return result\n        else:\n            result.update(self.agg_iteration_trimmed_mean_per_lvl(shared_states))\n            return result\n\n    def agg_iteration_trimmed_mean_per_lvl(\n        self,\n        shared_states: list[dict],\n    ) -&gt; dict:\n        \"\"\"Aggregate step of the iteration of the trimmed mean algo.\n\n        Parameters\n        ----------\n        shared_states : list[dict]\n            List of dictionary containing the following keys:\n                - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n                - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - n_samples: int\n                - trim_ratio: float\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n               - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n        \"\"\"\n        trim_ratio = shared_states[0][\"trim_ratio\"]\n        upper_bounds_thresholds = shared_states[0][\"upper_bounds_thresholds\"]\n        lower_bounds_thresholds = shared_states[0][\"lower_bounds_thresholds\"]\n\n        n_samples = np.sum([state[\"n_samples\"] for state in shared_states])\n\n        n_trim = np.floor(n_samples * trim_ratio)\n        # Targets contain the number of samples we want to have above the two\n        # thresholds.\n\n        targets = np.array([n_trim, n_samples - n_trim])\n\n        # We sum the number of samples above the thresholds for each gene.\n        agg_n_samples_strictly_above_quantiles = np.sum(\n            [state[\"num_strictly_above\"] for state in shared_states],\n            axis=0,\n        )\n\n        # Mask of size (n_genes,2) indicating for each gene and each of the two\n        # thresholds if the number of samples above the threshold is too high.\n        mask_threshold_too_high = (\n            agg_n_samples_strictly_above_quantiles &lt; targets[None, :]\n        )\n\n        # Similarly, we create a mask for the case where the number of samples above the\n        # thresholds is too low.\n        mask_threshold_too_low = (\n            agg_n_samples_strictly_above_quantiles &gt; targets[None, :]\n        )\n\n        ## Update the thresholds and bounds when the thresholds are two high or too low.\n        upper_bounds_thresholds[mask_threshold_too_high] = (\n            upper_bounds_thresholds[mask_threshold_too_high]\n            + lower_bounds_thresholds[mask_threshold_too_high]\n        ) / 2.0\n\n        lower_bounds_thresholds[mask_threshold_too_low] = (\n            upper_bounds_thresholds[mask_threshold_too_low]\n            + lower_bounds_thresholds[mask_threshold_too_low]\n        ) / 2.0\n\n        return {\n            \"upper_bounds_thresholds\": upper_bounds_thresholds,\n            \"lower_bounds_thresholds\": lower_bounds_thresholds,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggIterationTrimmedMean.agg_iteration_trimmed_mean","title":"<code>agg_iteration_trimmed_mean(shared_states)</code>","text":"<p>Compute the initial global upper and lower bounds.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of dictionnaries with the following keys:     - num_strictly_above: np.ndarray[int] of size (n_genes,2)     - upper_bounds_thresholds: np.ndarray of size (n_genes,2)     - lower_bounds_thresholds: np.ndarray of size (n_genes,2)     - n_samples: int     - trim_ratio: float If use_lvl is true, the dictionary is nested with the levels as keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)    - lower_bounds_thresholds : np.ndarray of size (n_genes, 2) If use_lvl is true, the dictionary is nested with the levels as keys.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote\n@log_remote\ndef agg_iteration_trimmed_mean(\n    self,\n    shared_states: list[dict],\n) -&gt; dict:\n    \"\"\"Compute the initial global upper and lower bounds.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of dictionnaries with the following keys:\n            - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n            - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - n_samples: int\n            - trim_ratio: float\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes, 2)\n           - lower_bounds_thresholds : np.ndarray of size (n_genes, 2)\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n    \"\"\"\n    use_lvl = shared_states[0][\"use_lvl\"]\n    result = {\"use_lvl\": use_lvl}\n    if use_lvl:\n        for lvl in shared_states[0].keys():\n            if lvl == \"use_lvl\":\n                continue\n            result[lvl] = self.agg_iteration_trimmed_mean_per_lvl(\n                [state[lvl] for state in shared_states]\n            )\n        return result\n    else:\n        result.update(self.agg_iteration_trimmed_mean_per_lvl(shared_states))\n        return result\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.AggIterationTrimmedMean.agg_iteration_trimmed_mean_per_lvl","title":"<code>agg_iteration_trimmed_mean_per_lvl(shared_states)</code>","text":"<p>Aggregate step of the iteration of the trimmed mean algo.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>List of dictionary containing the following keys:     - num_strictly_above: np.ndarray[int] of size (n_genes,2)     - upper_bounds_thresholds: np.ndarray of size (n_genes,2)     - lower_bounds_thresholds: np.ndarray of size (n_genes,2)     - n_samples: int     - trim_ratio: float</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes,2)    - lower_bounds_thresholds : np.ndarray of size (n_genes,2) If use_lvl is true, the dictionary is nested with the levels as keys.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def agg_iteration_trimmed_mean_per_lvl(\n    self,\n    shared_states: list[dict],\n) -&gt; dict:\n    \"\"\"Aggregate step of the iteration of the trimmed mean algo.\n\n    Parameters\n    ----------\n    shared_states : list[dict]\n        List of dictionary containing the following keys:\n            - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n            - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - n_samples: int\n            - trim_ratio: float\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n           - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n    \"\"\"\n    trim_ratio = shared_states[0][\"trim_ratio\"]\n    upper_bounds_thresholds = shared_states[0][\"upper_bounds_thresholds\"]\n    lower_bounds_thresholds = shared_states[0][\"lower_bounds_thresholds\"]\n\n    n_samples = np.sum([state[\"n_samples\"] for state in shared_states])\n\n    n_trim = np.floor(n_samples * trim_ratio)\n    # Targets contain the number of samples we want to have above the two\n    # thresholds.\n\n    targets = np.array([n_trim, n_samples - n_trim])\n\n    # We sum the number of samples above the thresholds for each gene.\n    agg_n_samples_strictly_above_quantiles = np.sum(\n        [state[\"num_strictly_above\"] for state in shared_states],\n        axis=0,\n    )\n\n    # Mask of size (n_genes,2) indicating for each gene and each of the two\n    # thresholds if the number of samples above the threshold is too high.\n    mask_threshold_too_high = (\n        agg_n_samples_strictly_above_quantiles &lt; targets[None, :]\n    )\n\n    # Similarly, we create a mask for the case where the number of samples above the\n    # thresholds is too low.\n    mask_threshold_too_low = (\n        agg_n_samples_strictly_above_quantiles &gt; targets[None, :]\n    )\n\n    ## Update the thresholds and bounds when the thresholds are two high or too low.\n    upper_bounds_thresholds[mask_threshold_too_high] = (\n        upper_bounds_thresholds[mask_threshold_too_high]\n        + lower_bounds_thresholds[mask_threshold_too_high]\n    ) / 2.0\n\n    lower_bounds_thresholds[mask_threshold_too_low] = (\n        upper_bounds_thresholds[mask_threshold_too_low]\n        + lower_bounds_thresholds[mask_threshold_too_low]\n    ) / 2.0\n\n    return {\n        \"upper_bounds_thresholds\": upper_bounds_thresholds,\n        \"lower_bounds_thresholds\": lower_bounds_thresholds,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocFinalTrimmedMean","title":"<code>LocFinalTrimmedMean</code>","text":"<p>Mixin class to implement the local finalisation of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class LocFinalTrimmedMean:\n    \"\"\"Mixin class to implement the local finalisation of the trimmed mean algo.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def final_local_trimmed_mean(\n        self,\n        data_from_opener,\n        shared_state,\n        layer_used: str,\n        mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n        trim_ratio: float | None = None,\n        refit: bool = False,\n    ) -&gt; dict:\n        \"\"\"Finalise the trimmed mean algo by computing the trimmed mean.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Unused, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used\n               - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mode : Literal[\"normal\", \"cooks\"]\n            Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n            either on the normalized counts or the squared error.\n            It will be applied per level, except if there are not enough samples.\n            Moreover, trim ratios will be computed based on the number of replicates.\n            If \"normal\", the function will be applied on the whole dataset, using the\n            trim_ratio parameter.\n\n        trim_ratio : float, optional\n            Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be\n            None if mode is \"cooks\", and float if mode is \"normal\".\n\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys:\n            - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n            - n_samples : np.ndarray(int) of size (n_genes,2)\n            - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n            - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        use_lvl = shared_state[\"use_lvl\"]\n        result = {\"use_lvl\": use_lvl}\n        if mode == \"cooks\" and use_lvl:\n            for lvl in shared_state.keys():\n                if lvl == \"use_lvl\":\n                    continue\n                mask = adata.obs[\"cells\"] == lvl\n                result[lvl] = self.final_local_trimmed_mean_per_lvl(\n                    data_from_opener, shared_state[lvl], layer_used, mask, refit\n                )\n                trim_ratio = get_trim_ratio(adata.uns[\"num_replicates\"][lvl])\n                if layer_used == \"sqerror\":\n                    scale = get_scale(adata.uns[\"num_replicates\"][lvl])\n                    result[lvl][\"scale\"] = scale\n                result[lvl][\"trim_ratio\"] = trim_ratio\n            return result\n        result.update(\n            self.final_local_trimmed_mean_per_lvl(\n                data_from_opener,\n                shared_state,\n                layer_used,\n                mask=np.ones(adata.n_obs, dtype=bool),\n                refit=refit,\n            )\n        )\n        if mode == \"cooks\":\n            assert trim_ratio is None\n            result[\"trim_ratio\"] = 0.125\n            if layer_used == \"sqerror\":\n                result[\"scale\"] = 1.51\n        else:\n            assert trim_ratio is not None\n            result[\"trim_ratio\"] = trim_ratio\n        return result\n\n    def final_local_trimmed_mean_per_lvl(\n        self,\n        data_from_opener,\n        shared_state,\n        layer_used,\n        mask,\n        refit: bool = False,\n    ) -&gt; dict:\n        \"\"\"Finalise the trimmed mean algo by computing the trimmed mean.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Unused, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used\n               - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mask : np.ndarray\n            Mask to filter values used in the quantile computation.\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys:\n            - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n            - n_samples : np.ndarray(int) of size (n_genes,2)\n            - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n            - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # we create an explicit copy to avoid ImplicitModificationWarning\n        local_adata_filtered = adata[mask].copy()\n        current_thresholds = (\n            shared_state[\"upper_bounds_thresholds\"]\n            + shared_state[\"lower_bounds_thresholds\"]\n        ) / 2.0\n\n        num_strictly_above = (\n            local_adata_filtered.layers[layer_used][..., None]\n            &gt; current_thresholds[None, ...]\n        ).sum(axis=0)\n\n        mask_upper_threshold = (\n            local_adata_filtered.layers[layer_used]\n            &gt; current_thresholds[..., 0][None, :]\n        )\n        mask_lower_threshold = (\n            local_adata_filtered.layers[layer_used]\n            &lt;= current_thresholds[..., 1][None, :]\n        )\n        local_adata_filtered.layers[f\"trimmed_{layer_used}\"] = (\n            local_adata_filtered.layers[layer_used].copy()\n        )\n        local_adata_filtered.layers[f\"trimmed_{layer_used}\"][\n            mask_upper_threshold | mask_lower_threshold\n        ] = 0\n\n        return {\n            \"trimmed_local_sum\": local_adata_filtered.layers[\n                f\"trimmed_{layer_used}\"\n            ].sum(axis=0),\n            \"n_samples\": local_adata_filtered.n_obs,\n            \"num_strictly_above\": num_strictly_above,\n            \"upper_bounds_thresholds\": shared_state[\"upper_bounds_thresholds\"],\n            \"lower_bounds_thresholds\": shared_state[\"lower_bounds_thresholds\"],\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocFinalTrimmedMean.final_local_trimmed_mean","title":"<code>final_local_trimmed_mean(data_from_opener, shared_state, layer_used, mode='normal', trim_ratio=None, refit=False)</code>","text":"<p>Finalise the trimmed mean algo by computing the trimmed mean.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Unused, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used    - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used If use_lvl is true, the dictionary is nested with the levels as keys.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mode</code> <code>Literal['normal', 'cooks']</code> <p>Mode of the trimmed mean algo. If \"cooks\", the function will be applied either on the normalized counts or the squared error. It will be applied per level, except if there are not enough samples. Moreover, trim ratios will be computed based on the number of replicates. If \"normal\", the function will be applied on the whole dataset, using the trim_ratio parameter.</p> <code>'normal'</code> <code>trim_ratio</code> <code>float</code> <p>Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be None if mode is \"cooks\", and float if mode is \"normal\".</p> <code>None</code> <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys: - trimmed_local_sum : np.ndarray(float) of size (n_genes,2) - n_samples : np.ndarray(int) of size (n_genes,2) - num_strictly_above : np.ndarray(int) of size (n_genes,2) - upper_bounds_thresholds : np.ndarray of size (n_genes,2) - lower_bounds_thresholds : np.ndarray of size (n_genes,2) If use_lvl is true, the dictionary is nested with the levels as keys.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef final_local_trimmed_mean(\n    self,\n    data_from_opener,\n    shared_state,\n    layer_used: str,\n    mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n    trim_ratio: float | None = None,\n    refit: bool = False,\n) -&gt; dict:\n    \"\"\"Finalise the trimmed mean algo by computing the trimmed mean.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Unused, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used\n           - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mode : Literal[\"normal\", \"cooks\"]\n        Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n        either on the normalized counts or the squared error.\n        It will be applied per level, except if there are not enough samples.\n        Moreover, trim ratios will be computed based on the number of replicates.\n        If \"normal\", the function will be applied on the whole dataset, using the\n        trim_ratio parameter.\n\n    trim_ratio : float, optional\n        Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be\n        None if mode is \"cooks\", and float if mode is \"normal\".\n\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys:\n        - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n        - n_samples : np.ndarray(int) of size (n_genes,2)\n        - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n        - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n        - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    use_lvl = shared_state[\"use_lvl\"]\n    result = {\"use_lvl\": use_lvl}\n    if mode == \"cooks\" and use_lvl:\n        for lvl in shared_state.keys():\n            if lvl == \"use_lvl\":\n                continue\n            mask = adata.obs[\"cells\"] == lvl\n            result[lvl] = self.final_local_trimmed_mean_per_lvl(\n                data_from_opener, shared_state[lvl], layer_used, mask, refit\n            )\n            trim_ratio = get_trim_ratio(adata.uns[\"num_replicates\"][lvl])\n            if layer_used == \"sqerror\":\n                scale = get_scale(adata.uns[\"num_replicates\"][lvl])\n                result[lvl][\"scale\"] = scale\n            result[lvl][\"trim_ratio\"] = trim_ratio\n        return result\n    result.update(\n        self.final_local_trimmed_mean_per_lvl(\n            data_from_opener,\n            shared_state,\n            layer_used,\n            mask=np.ones(adata.n_obs, dtype=bool),\n            refit=refit,\n        )\n    )\n    if mode == \"cooks\":\n        assert trim_ratio is None\n        result[\"trim_ratio\"] = 0.125\n        if layer_used == \"sqerror\":\n            result[\"scale\"] = 1.51\n    else:\n        assert trim_ratio is not None\n        result[\"trim_ratio\"] = trim_ratio\n    return result\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocFinalTrimmedMean.final_local_trimmed_mean_per_lvl","title":"<code>final_local_trimmed_mean_per_lvl(data_from_opener, shared_state, layer_used, mask, refit=False)</code>","text":"<p>Finalise the trimmed mean algo by computing the trimmed mean.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Unused, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used    - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mask</code> <code>ndarray</code> <p>Mask to filter values used in the quantile computation.</p> required <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys: - trimmed_local_sum : np.ndarray(float) of size (n_genes,2) - n_samples : np.ndarray(int) of size (n_genes,2) - num_strictly_above : np.ndarray(int) of size (n_genes,2) - upper_bounds_thresholds : np.ndarray of size (n_genes,2) - lower_bounds_thresholds : np.ndarray of size (n_genes,2)</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def final_local_trimmed_mean_per_lvl(\n    self,\n    data_from_opener,\n    shared_state,\n    layer_used,\n    mask,\n    refit: bool = False,\n) -&gt; dict:\n    \"\"\"Finalise the trimmed mean algo by computing the trimmed mean.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Unused, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not  used\n           - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mask : np.ndarray\n        Mask to filter values used in the quantile computation.\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys:\n        - trimmed_local_sum : np.ndarray(float) of size (n_genes,2)\n        - n_samples : np.ndarray(int) of size (n_genes,2)\n        - num_strictly_above : np.ndarray(int) of size (n_genes,2)\n        - upper_bounds_thresholds : np.ndarray of size (n_genes,2)\n        - lower_bounds_thresholds : np.ndarray of size (n_genes,2)\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # we create an explicit copy to avoid ImplicitModificationWarning\n    local_adata_filtered = adata[mask].copy()\n    current_thresholds = (\n        shared_state[\"upper_bounds_thresholds\"]\n        + shared_state[\"lower_bounds_thresholds\"]\n    ) / 2.0\n\n    num_strictly_above = (\n        local_adata_filtered.layers[layer_used][..., None]\n        &gt; current_thresholds[None, ...]\n    ).sum(axis=0)\n\n    mask_upper_threshold = (\n        local_adata_filtered.layers[layer_used]\n        &gt; current_thresholds[..., 0][None, :]\n    )\n    mask_lower_threshold = (\n        local_adata_filtered.layers[layer_used]\n        &lt;= current_thresholds[..., 1][None, :]\n    )\n    local_adata_filtered.layers[f\"trimmed_{layer_used}\"] = (\n        local_adata_filtered.layers[layer_used].copy()\n    )\n    local_adata_filtered.layers[f\"trimmed_{layer_used}\"][\n        mask_upper_threshold | mask_lower_threshold\n    ] = 0\n\n    return {\n        \"trimmed_local_sum\": local_adata_filtered.layers[\n            f\"trimmed_{layer_used}\"\n        ].sum(axis=0),\n        \"n_samples\": local_adata_filtered.n_obs,\n        \"num_strictly_above\": num_strictly_above,\n        \"upper_bounds_thresholds\": shared_state[\"upper_bounds_thresholds\"],\n        \"lower_bounds_thresholds\": shared_state[\"lower_bounds_thresholds\"],\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocInitTrimmedMean","title":"<code>LocInitTrimmedMean</code>","text":"<p>Mixin class to implement the local initialisation of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class LocInitTrimmedMean:\n    \"\"\"Mixin class to implement the local initialisation of the trimmed mean algo.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def loc_init_trimmed_mean(\n        self,\n        data_from_opener,\n        shared_state,\n        layer_used: str,\n        mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n        refit: bool = False,\n        min_replicates_trimmed_mean: int = 3,\n    ) -&gt; dict:\n        \"\"\"Initialise the trimmed mean algo, by providing the lower and max bounds.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Unused, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Not used, all the necessary info is stored in the local adata.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mode : Literal[\"normal\", \"cooks\"]\n            Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n            either on the normalized counts or the squared error.\n            It will be applied per level, except if there are not enough samples.\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        min_replicates_trimmed_mean : int\n            Minimum number of replicates to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            If mode is \"normal\" or if mode is \"cooks\" and there are not enough samples,\n            to compute the trimmed mean per level, a dictionary with the following keys\n                - max_values: np.ndarray of size (n_genes,)\n                - min_values: np.ndarray of size (n_genes,)\n                - use_lvl: False\n            otherwise, a dictionary with the max_values and min_values keys, nested\n            inside a dictionary with the levels as keys, plus a use_lvl with value True\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        if mode == \"cooks\":\n            # Check that the layer is either cooks or normed counts\n            assert layer_used in [\"sqerror\", \"normed_counts\"]\n            # Check that num replicates is in the uns\n            assert \"num_replicates\" in adata.uns, \"No num_replicates in the adata\"\n            use_lvl = adata.uns[\"num_replicates\"].max() &gt;= min_replicates_trimmed_mean\n            assert \"cells\" in adata.obs, \"No cells column in the adata\"\n\n        else:\n            use_lvl = False\n        result = {\"use_lvl\": use_lvl}\n        if use_lvl:\n            # In that case, we know we are in cooks mode\n            admissible_levels = adata.uns[\"num_replicates\"][\n                adata.uns[\"num_replicates\"] &gt;= min_replicates_trimmed_mean\n            ].index\n\n            shared_state = dict.fromkeys(admissible_levels, shared_state)\n            for lvl in admissible_levels:\n                mask = adata.obs[\"cells\"] == lvl\n                result[lvl] = self.loc_init_trimmed_mean_per_lvl(  # type: ignore\n                    data_from_opener, shared_state[lvl], layer_used, mask, refit\n                )\n            return result\n        else:\n            result.update(\n                self.loc_init_trimmed_mean_per_lvl(\n                    data_from_opener,\n                    shared_state,\n                    layer_used,\n                    mask=np.ones(adata.n_obs, dtype=bool),\n                    refit=refit,\n                )\n            )\n            return result\n\n    def loc_init_trimmed_mean_per_lvl(\n        self,\n        data_from_opener,\n        shared_state,\n        layer_used: str,\n        mask,\n        refit: bool = False,\n    ) -&gt; dict:\n        \"\"\"Initialise the trimmed mean algo, by providing the lower and max bounds.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Unused, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Not used, all the necessary info is stored in the local adata.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mask : np.ndarray\n            Mask to filter values used in the min and max computation.\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            Dictionary with the following keys\n                - max_values: np.ndarray of size (n_genes,)\n                - min_values: np.ndarray of size (n_genes,)\n                - n_samples: int, number of samples\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        assert layer_used in adata.layers\n        local_adata_filtered = adata[mask]\n        if local_adata_filtered.n_obs &gt; 0:\n            max_values = local_adata_filtered.layers[layer_used].max(axis=0)\n            min_values = local_adata_filtered.layers[layer_used].min(axis=0)\n        else:\n            max_values = np.zeros(adata.n_vars) * np.nan\n            min_values = np.zeros(adata.n_vars) * np.nan\n        return {\n            \"max_values\": max_values,\n            \"min_values\": min_values,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocInitTrimmedMean.loc_init_trimmed_mean","title":"<code>loc_init_trimmed_mean(data_from_opener, shared_state, layer_used, mode='normal', refit=False, min_replicates_trimmed_mean=3)</code>","text":"<p>Initialise the trimmed mean algo, by providing the lower and max bounds.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Unused, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Not used, all the necessary info is stored in the local adata.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mode</code> <code>Literal['normal', 'cooks']</code> <p>Mode of the trimmed mean algo. If \"cooks\", the function will be applied either on the normalized counts or the squared error. It will be applied per level, except if there are not enough samples.</p> <code>'normal'</code> <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <code>min_replicates_trimmed_mean</code> <code>int</code> <p>Minimum number of replicates to compute the trimmed mean.</p> <code>3</code> <p>Returns:</p> Type Description <code>dict</code> <p>If mode is \"normal\" or if mode is \"cooks\" and there are not enough samples, to compute the trimmed mean per level, a dictionary with the following keys     - max_values: np.ndarray of size (n_genes,)     - min_values: np.ndarray of size (n_genes,)     - use_lvl: False otherwise, a dictionary with the max_values and min_values keys, nested inside a dictionary with the levels as keys, plus a use_lvl with value True</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef loc_init_trimmed_mean(\n    self,\n    data_from_opener,\n    shared_state,\n    layer_used: str,\n    mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n    refit: bool = False,\n    min_replicates_trimmed_mean: int = 3,\n) -&gt; dict:\n    \"\"\"Initialise the trimmed mean algo, by providing the lower and max bounds.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Unused, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Not used, all the necessary info is stored in the local adata.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mode : Literal[\"normal\", \"cooks\"]\n        Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n        either on the normalized counts or the squared error.\n        It will be applied per level, except if there are not enough samples.\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    min_replicates_trimmed_mean : int\n        Minimum number of replicates to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        If mode is \"normal\" or if mode is \"cooks\" and there are not enough samples,\n        to compute the trimmed mean per level, a dictionary with the following keys\n            - max_values: np.ndarray of size (n_genes,)\n            - min_values: np.ndarray of size (n_genes,)\n            - use_lvl: False\n        otherwise, a dictionary with the max_values and min_values keys, nested\n        inside a dictionary with the levels as keys, plus a use_lvl with value True\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    if mode == \"cooks\":\n        # Check that the layer is either cooks or normed counts\n        assert layer_used in [\"sqerror\", \"normed_counts\"]\n        # Check that num replicates is in the uns\n        assert \"num_replicates\" in adata.uns, \"No num_replicates in the adata\"\n        use_lvl = adata.uns[\"num_replicates\"].max() &gt;= min_replicates_trimmed_mean\n        assert \"cells\" in adata.obs, \"No cells column in the adata\"\n\n    else:\n        use_lvl = False\n    result = {\"use_lvl\": use_lvl}\n    if use_lvl:\n        # In that case, we know we are in cooks mode\n        admissible_levels = adata.uns[\"num_replicates\"][\n            adata.uns[\"num_replicates\"] &gt;= min_replicates_trimmed_mean\n        ].index\n\n        shared_state = dict.fromkeys(admissible_levels, shared_state)\n        for lvl in admissible_levels:\n            mask = adata.obs[\"cells\"] == lvl\n            result[lvl] = self.loc_init_trimmed_mean_per_lvl(  # type: ignore\n                data_from_opener, shared_state[lvl], layer_used, mask, refit\n            )\n        return result\n    else:\n        result.update(\n            self.loc_init_trimmed_mean_per_lvl(\n                data_from_opener,\n                shared_state,\n                layer_used,\n                mask=np.ones(adata.n_obs, dtype=bool),\n                refit=refit,\n            )\n        )\n        return result\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocInitTrimmedMean.loc_init_trimmed_mean_per_lvl","title":"<code>loc_init_trimmed_mean_per_lvl(data_from_opener, shared_state, layer_used, mask, refit=False)</code>","text":"<p>Initialise the trimmed mean algo, by providing the lower and max bounds.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Unused, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Not used, all the necessary info is stored in the local adata.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mask</code> <code>ndarray</code> <p>Mask to filter values used in the min and max computation.</p> required <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with the following keys     - max_values: np.ndarray of size (n_genes,)     - min_values: np.ndarray of size (n_genes,)     - n_samples: int, number of samples</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def loc_init_trimmed_mean_per_lvl(\n    self,\n    data_from_opener,\n    shared_state,\n    layer_used: str,\n    mask,\n    refit: bool = False,\n) -&gt; dict:\n    \"\"\"Initialise the trimmed mean algo, by providing the lower and max bounds.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Unused, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Not used, all the necessary info is stored in the local adata.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mask : np.ndarray\n        Mask to filter values used in the min and max computation.\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        Dictionary with the following keys\n            - max_values: np.ndarray of size (n_genes,)\n            - min_values: np.ndarray of size (n_genes,)\n            - n_samples: int, number of samples\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    assert layer_used in adata.layers\n    local_adata_filtered = adata[mask]\n    if local_adata_filtered.n_obs &gt; 0:\n        max_values = local_adata_filtered.layers[layer_used].max(axis=0)\n        min_values = local_adata_filtered.layers[layer_used].min(axis=0)\n    else:\n        max_values = np.zeros(adata.n_vars) * np.nan\n        min_values = np.zeros(adata.n_vars) * np.nan\n    return {\n        \"max_values\": max_values,\n        \"min_values\": min_values,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocalIterationTrimmedMean","title":"<code>LocalIterationTrimmedMean</code>","text":"<p>Mixin class to implement the local iteration of the trimmed mean algo.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>class LocalIterationTrimmedMean:\n    \"\"\"Mixin class to implement the local iteration of the trimmed mean algo.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_iteration_trimmed_mean(\n        self,\n        data_from_opener,\n        shared_state,\n        layer_used: str,\n        mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n        trim_ratio: float | None = None,\n        refit: bool = False,\n    ) -&gt; dict:\n        \"\"\"Local iteration of the trimmed mean algo.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n               - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mode : Literal[\"normal\", \"cooks\"]\n            Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n            either on the normalized counts or the squared error.\n            It will be applied per level, except if there are not enough samples.\n            Moreover, trim ratios will be computed based on the number of replicates.\n            If \"normal\", the function will be applied on the whole dataset, using the\n            trim_ratio parameter.\n\n        trim_ratio : float, optional\n            Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be\n            None if mode is \"cooks\", and float if mode is \"normal\".\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the following keys:\n                - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n                - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - n_samples: int\n                - trim_ratio: float\n            If use_lvl is true, the dictionary is nested with the levels as keys.\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n        use_lvl = shared_state[\"use_lvl\"]\n        result = {\"use_lvl\": use_lvl}\n\n        if mode == \"cooks\":\n            assert trim_ratio is None\n        else:\n            assert trim_ratio is not None\n\n        if mode == \"cooks\" and use_lvl:\n            for lvl in shared_state.keys():\n                if lvl == \"use_lvl\":\n                    continue\n                mask = adata.obs[\"cells\"] == lvl\n                result[lvl] = self.local_iteration_trimmed_mean_per_lvl(\n                    data_from_opener, shared_state[lvl], layer_used, mask, refit\n                )\n                trim_ratio = get_trim_ratio(adata.uns[\"num_replicates\"][lvl])\n                result[lvl][\"trim_ratio\"] = trim_ratio\n            return result\n\n        result.update(\n            self.local_iteration_trimmed_mean_per_lvl(\n                data_from_opener,\n                shared_state,\n                layer_used,\n                mask=np.ones(adata.n_obs, dtype=bool),\n                refit=refit,\n            )\n        )\n        if mode == \"cooks\":\n            result[\"trim_ratio\"] = 0.125\n        else:\n            result[\"trim_ratio\"] = trim_ratio\n\n        return result\n\n    def local_iteration_trimmed_mean_per_lvl(\n        self, data_from_opener, shared_state, layer_used, mask, refit: bool = False\n    ) -&gt; dict:\n        \"\"\"Local iteration of the trimmed mean algo.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used, all the necessary info is stored in the local adata.\n\n        shared_state : dict\n            Dictionary with the following keys:\n               - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n               - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n\n        layer_used : str\n            Name of the layer used to compute the trimmed mean.\n\n        mask : np.ndarray\n            Mask to filter values used in the quantile computation.\n\n        refit : bool\n            If true, the function will use the refit adata to compute the trimmed mean.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the following keys:\n                - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n                - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n                - n_samples: int\n        \"\"\"\n        if refit:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # We don't need to pass the thresholds in the share states as it's always the\n        # mean of the upper and lower bounds.\n        threshold = (\n            shared_state[\"upper_bounds_thresholds\"]\n            + shared_state[\"lower_bounds_thresholds\"]\n        ) / 2\n        local_adata_filtered = adata[mask]\n        # Array of size (n_genes, 2) containing the number of samples above the\n        # thresholds.\n        num_strictly_above = (\n            local_adata_filtered.layers[layer_used][..., None] &gt; threshold[None, ...]\n        ).sum(axis=0)\n\n        return {\n            \"num_strictly_above\": num_strictly_above,\n            \"upper_bounds_thresholds\": shared_state[\"upper_bounds_thresholds\"],\n            \"lower_bounds_thresholds\": shared_state[\"lower_bounds_thresholds\"],\n            \"n_samples\": local_adata_filtered.n_obs,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocalIterationTrimmedMean.local_iteration_trimmed_mean","title":"<code>local_iteration_trimmed_mean(data_from_opener, shared_state, layer_used, mode='normal', trim_ratio=None, refit=False)</code>","text":"<p>Local iteration of the trimmed mean algo.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.    - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used. If use_lvl is true, the dictionary is nested with the levels as keys.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mode</code> <code>Literal['normal', 'cooks']</code> <p>Mode of the trimmed mean algo. If \"cooks\", the function will be applied either on the normalized counts or the squared error. It will be applied per level, except if there are not enough samples. Moreover, trim ratios will be computed based on the number of replicates. If \"normal\", the function will be applied on the whole dataset, using the trim_ratio parameter.</p> <code>'normal'</code> <code>trim_ratio</code> <code>float</code> <p>Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be None if mode is \"cooks\", and float if mode is \"normal\".</p> <code>None</code> <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the following keys:     - num_strictly_above: np.ndarray[int] of size (n_genes,2)     - upper_bounds_thresholds: np.ndarray of size (n_genes,2)     - lower_bounds_thresholds: np.ndarray of size (n_genes,2)     - n_samples: int     - trim_ratio: float If use_lvl is true, the dictionary is nested with the levels as keys.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_iteration_trimmed_mean(\n    self,\n    data_from_opener,\n    shared_state,\n    layer_used: str,\n    mode: Literal[\"normal\", \"cooks\"] = \"normal\",\n    trim_ratio: float | None = None,\n    refit: bool = False,\n) -&gt; dict:\n    \"\"\"Local iteration of the trimmed mean algo.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n           - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mode : Literal[\"normal\", \"cooks\"]\n        Mode of the trimmed mean algo. If \"cooks\", the function will be applied\n        either on the normalized counts or the squared error.\n        It will be applied per level, except if there are not enough samples.\n        Moreover, trim ratios will be computed based on the number of replicates.\n        If \"normal\", the function will be applied on the whole dataset, using the\n        trim_ratio parameter.\n\n    trim_ratio : float, optional\n        Ratio of the samples to be trimmed. Must be between 0 and 0.5. Must be\n        None if mode is \"cooks\", and float if mode is \"normal\".\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the following keys:\n            - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n            - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - n_samples: int\n            - trim_ratio: float\n        If use_lvl is true, the dictionary is nested with the levels as keys.\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n    use_lvl = shared_state[\"use_lvl\"]\n    result = {\"use_lvl\": use_lvl}\n\n    if mode == \"cooks\":\n        assert trim_ratio is None\n    else:\n        assert trim_ratio is not None\n\n    if mode == \"cooks\" and use_lvl:\n        for lvl in shared_state.keys():\n            if lvl == \"use_lvl\":\n                continue\n            mask = adata.obs[\"cells\"] == lvl\n            result[lvl] = self.local_iteration_trimmed_mean_per_lvl(\n                data_from_opener, shared_state[lvl], layer_used, mask, refit\n            )\n            trim_ratio = get_trim_ratio(adata.uns[\"num_replicates\"][lvl])\n            result[lvl][\"trim_ratio\"] = trim_ratio\n        return result\n\n    result.update(\n        self.local_iteration_trimmed_mean_per_lvl(\n            data_from_opener,\n            shared_state,\n            layer_used,\n            mask=np.ones(adata.n_obs, dtype=bool),\n            refit=refit,\n        )\n    )\n    if mode == \"cooks\":\n        result[\"trim_ratio\"] = 0.125\n    else:\n        result[\"trim_ratio\"] = trim_ratio\n\n    return result\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.substeps.LocalIterationTrimmedMean.local_iteration_trimmed_mean_per_lvl","title":"<code>local_iteration_trimmed_mean_per_lvl(data_from_opener, shared_state, layer_used, mask, refit=False)</code>","text":"<p>Local iteration of the trimmed mean algo.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used, all the necessary info is stored in the local adata.</p> required <code>shared_state</code> <code>dict</code> <p>Dictionary with the following keys:    - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.    - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.</p> required <code>layer_used</code> <code>str</code> <p>Name of the layer used to compute the trimmed mean.</p> required <code>mask</code> <code>ndarray</code> <p>Mask to filter values used in the quantile computation.</p> required <code>refit</code> <code>bool</code> <p>If true, the function will use the refit adata to compute the trimmed mean.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the following keys:     - num_strictly_above: np.ndarray[int] of size (n_genes,2)     - upper_bounds_thresholds: np.ndarray of size (n_genes,2)     - lower_bounds_thresholds: np.ndarray of size (n_genes,2)     - n_samples: int</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/substeps.py</code> <pre><code>def local_iteration_trimmed_mean_per_lvl(\n    self, data_from_opener, shared_state, layer_used, mask, refit: bool = False\n) -&gt; dict:\n    \"\"\"Local iteration of the trimmed mean algo.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used, all the necessary info is stored in the local adata.\n\n    shared_state : dict\n        Dictionary with the following keys:\n           - upper_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n           - lower_bounds_thresholds : np.ndarray of size (n_genes,2). Not used.\n\n    layer_used : str\n        Name of the layer used to compute the trimmed mean.\n\n    mask : np.ndarray\n        Mask to filter values used in the quantile computation.\n\n    refit : bool\n        If true, the function will use the refit adata to compute the trimmed mean.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the following keys:\n            - num_strictly_above: np.ndarray[int] of size (n_genes,2)\n            - upper_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - lower_bounds_thresholds: np.ndarray of size (n_genes,2)\n            - n_samples: int\n    \"\"\"\n    if refit:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # We don't need to pass the thresholds in the share states as it's always the\n    # mean of the upper and lower bounds.\n    threshold = (\n        shared_state[\"upper_bounds_thresholds\"]\n        + shared_state[\"lower_bounds_thresholds\"]\n    ) / 2\n    local_adata_filtered = adata[mask]\n    # Array of size (n_genes, 2) containing the number of samples above the\n    # thresholds.\n    num_strictly_above = (\n        local_adata_filtered.layers[layer_used][..., None] &gt; threshold[None, ...]\n    ).sum(axis=0)\n\n    return {\n        \"num_strictly_above\": num_strictly_above,\n        \"upper_bounds_thresholds\": shared_state[\"upper_bounds_thresholds\"],\n        \"lower_bounds_thresholds\": shared_state[\"lower_bounds_thresholds\"],\n        \"n_samples\": local_adata_filtered.n_obs,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.utils","title":"<code>utils</code>","text":""},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.utils.get_scale","title":"<code>get_scale(x)</code>","text":"<p>Get the scale based on the number of cells.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The number of cells.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The scale used to compute the dispersion during cook distance calculation.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/utils.py</code> <pre><code>def get_scale(x):\n    \"\"\"Get the scale based on the number of cells.\n\n    Parameters\n    ----------\n    x : float\n        The number of cells.\n\n    Returns\n    -------\n    float\n        The scale used to compute the dispersion during cook distance calculation.\n    \"\"\"\n    scales = (2.04, 1.86, 1.51)\n    return scales[trimfn(x)]\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.utils.get_trim_ratio","title":"<code>get_trim_ratio(x)</code>","text":"<p>Get the trim ratio based on the number of cells.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The number of cells.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The trim ratio.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/utils.py</code> <pre><code>def get_trim_ratio(x):\n    \"\"\"Get the trim ratio based on the number of cells.\n\n    Parameters\n    ----------\n    x : float\n        The number of cells.\n\n    Returns\n    -------\n    float\n        The trim ratio.\n    \"\"\"\n    trimratio = (1 / 3, 1 / 4, 1 / 8)\n    return trimratio[trimfn(x)]\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#fedpydeseq2.core.fed_algorithms.compute_trimmed_mean.utils.trimfn","title":"<code>trimfn(x)</code>","text":"<p>Determine the use-case of the trim ratio and scale based on cell counts.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The number of cells.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the trim ratio and scale to use.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/compute_trimmed_mean/utils.py</code> <pre><code>def trimfn(x: float) -&gt; int:\n    \"\"\"Determine the use-case of the trim ratio and scale based on cell counts.\n\n    Parameters\n    ----------\n    x : float\n        The number of cells.\n\n    Returns\n    -------\n    int\n        The index of the trim ratio and scale to use.\n    \"\"\"\n    return 2 if x &gt;= 23.5 else 1 if x &gt;= 3.5 else 0\n</code></pre>"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/compute_trimmed_mean/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the normed counts will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the normed counts will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 1 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{min}}_{g,l})^{(k)}\\). Each center Server 2 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 2 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k (Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k (Z^{\\texttt{min}}_{g,l})^{(k)}\\). Server Center 3 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 3 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 4 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 4 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 5 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 5 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the normed counts across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\). Each center Server 6 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center"},{"location":"api/core/federated_algorithms/compute_trimmed_mean/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. If use_lvl is \\(\\texttt{True}\\), then one trimmed mean of the normed counts will be computed for each gene and for each level of the design matrix (that is the trimmed mean will be performed on the samples corresponding to the same level of the design matrix across all centers). If use_lvl is \\(\\texttt{False}\\), then one trimmed mean of the normed counts will be computed for each gene and for all samples. This parameter is set to \\(\\texttt{True}\\) if there is at least one level of the design matrix with more than 3 replicates across all centers, and to \\(\\texttt{False}\\) otherwise. Each center Server 1 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Let \\(\\mathcal{L}_{\\geq 3}\\) bet the set of levels \\(1 \\leq l \\leq L\\) of the design matrix with at least \\(3\\) replicates across all centers. Let \\(\\mathcal{I}_{k,l}\\) denote the set of sample indices \\(1 \\leq i \\leq n_k\\) in center \\(k\\) whose line in the design corresponds to level \\(l\\). Dictionary with two keys.  \"max_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the maximum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"min_values\", which is a numpy array of shape \\(G\\) containing, for each gene \\(g\\), the minimum value of the center normed counts for \\(g\\) on samples in \\(\\mathcal{I}_{k,l}\\), denoted with \\((Z^{\\texttt{min}}_{g,l})^{(k)}\\). Each center Server 2 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 2 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain an upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the maximum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\max_k (Z^{\\texttt{max}}_{g,l})^{(k)}\\).  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), will contain a lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). Initialized for both ratios as the minimum of the normed counts of gene \\(g\\) across all samples in level \\(l\\), computed as \\(\\min_k (Z^{\\texttt{min}}_{g,l})^{(k)}\\). Server Center 3 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 3 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the average of the upper and lower  bounds on the \\(r\\)-quantile value of gene \\(g\\).  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"trim_ratio\" is the trim ration, defining the upper and lower ratios whose quantile we want to compute. Denoted with \\(r_{\\texttt{trim}}\\), equal to \\(0.125\\). Each center Server 4 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Server Center 4 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with two keys.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated upper bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and for each ratio \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), contains an updated lower bound of the \\(r\\)-quantile value of the normed counts of gene \\(g\\) across samples in level \\(l\\). For a given gene \\(g\\) and \\(r\\)-quantile, it is updated as either its previous value or the average of the previous upper and lower bounds across samples in level \\(l\\), depending on the number of samples in the level whose normed counts value is strictly above this average. This number of samples can be computed from the previous shared states. Server Center 5 use_lvl bool \\(()\\) Whether or not to use the levels of the design matrix to compute the trimmed mean. Passed on without modification. Each center Server 5 \\(l \\in \\mathcal{L}_{\\geq 3}\\) dict Dictionary with the following keys.  \"trimmed_local_sum\" is a numpy array of shape \\((G,)\\). For each gene \\(g\\) and each ration \\(r \\in \\{r_{\\texttt{trim}}, 1 - r_{\\texttt{trim}}\\}\\), we approximate the \\(r\\)-quantile by taking the average of the running upper and lower bounds. The trimmed local sum is computed as the sum of the normed counts across all samples in level \\(l\\) and center \\(k\\), whose value is strictly above the approximation of the \\(r_{\\texttt{trim}}\\)-quantile and less or equal to the approximation of the \\(1-r_{\\texttt{trim}}\\) quantile.  \"n_samples\" is the number of samples in level \\(l\\) and center \\(k\\).  \"num_strictly_above\" is a numpy array of shape \\((G, 2)\\). For each gene \\(g\\) and each ratio \\(r\\), contains the number of samples in the center and in the level whose normed counts value for gene \\(g\\) is strictly above the approximation of the \\(r\\)-quantile.  \"upper_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\), simply passed on from the previous state.  \"lower_bounds_thresholds\" is a numpy array of shape \\((G, 2)\\) simply passed on from the previous state. \"trim_ratio\", equal to \\(0.125\\). Each center Server 6 trimmed_mean_normed_counts DataFrame \\((G, \\|\\mathcal{L}_{\\geq 3}\\|)\\) For each gene \\(g\\) and each level \\(l \\in \\mathcal{L}_{\\geq 3}\\), the corresponding entry \\(\\overline{Z}^{\\texttt{trim}}_{g,l}\\) is the approximation of the trimmed mean of the normed counts for gene \\(g\\) and samples whose line in the design corresponds to level \\(l\\) with trim ratio \\(r_{\\texttt{trim}}\\), computed by summing the trimmed_local_sums and dividing by the sum of the local n_samples for the corresponding gene and level. Server Center"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>fit_dispersions</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#api","title":"API","text":""},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.dispersions_grid_search","title":"<code>dispersions_grid_search</code>","text":"<p>Main module to compute dispersions by minimizing the MLE using a grid search.</p>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.dispersions_grid_search.ComputeDispersionsGridSearch","title":"<code>ComputeDispersionsGridSearch</code>","text":"<p>               Bases: <code>AggGridUpdate</code>, <code>LocGridLoss</code></p> <p>Mixin class to implement the computation of genewise dispersions.</p> <p>The switch between genewise and MAP dispersions is done by setting the <code>fit_mode</code> argument in the <code>fit_dispersions</code> to either \"MLE\" or \"MAP\".</p> <p>Methods:</p> Name Description <code>fit_dispersions</code> <p>A method to fit dispersions using grid search.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/dispersions_grid_search.py</code> <pre><code>class ComputeDispersionsGridSearch(\n    AggGridUpdate,\n    LocGridLoss,\n):\n    \"\"\"Mixin class to implement the computation of genewise dispersions.\n\n    The switch between genewise and MAP dispersions is done by setting the `fit_mode`\n    argument in the `fit_dispersions` to either \"MLE\" or \"MAP\".\n\n    Methods\n    -------\n    fit_dispersions\n        A method to fit dispersions using grid search.\n    \"\"\"\n\n    grid_batch_size: int\n    grid_depth: int\n    grid_length: int\n\n    @log_organisation_method\n    def fit_dispersions(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        shared_state,\n        round_idx,\n        clean_models,\n        fit_mode: Literal[\"MLE\", \"MAP\"] = \"MLE\",\n        refit_mode: bool = False,\n    ):\n        \"\"\"Fit dispersions using grid search.\n\n        Supports two modes: \"MLE\", to fit gene-wise dispersions, and \"MAP\", to fit\n        MAP dispersions and filter them to avoid shrinking the dispersions of genes\n        that are too far from the trend curve.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_state: dict, optional\n            If the fit_mode is \"MLE\", it is None.\n            If the fit_mode is \"MAP\", it contains the output of the trend fitting,\n            that is a dictionary with a \"fitted_dispersion\" field containing\n            the fitted dispersions from the trend curve, a \"prior_disp_var\" field\n            containing the prior variance of the dispersions, and a \"_squared_logres\"\n            field containing the squared residuals of the trend fitting.\n\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            Whether to clean the models after the computation.\n\n        fit_mode: str\n            If \"MLE\", gene-wise dispersions are fitted independently, and\n            `\"genewise_dispersions\"` fields are populated. If \"MAP\", prior\n            regularization is applied, `\"MAP_dispersions\"` fields are populated.\n\n        refit_mode: bool\n            Whether to run on `refit_adata`s instead of `local_adata`s (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        shared_state: dict or list[dict]\n            A dictionary containing:\n            - \"genewise_dispersions\": The MLE dispersions, to be stored locally at\n            - \"lower_log_bounds\": log lower bounds for the grid search (only used in\n            internal loop),\n            - \"upper_log_bounds\": log upper bounds for the grid search (only used in\n            internal loop).\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        start_loop()\n        for iteration in range(self.grid_depth):\n            start_iteration(iteration)\n            # Compute local loss summands at all grid points.\n            local_states, shared_states, round_idx = local_step(\n                local_method=self.local_grid_loss,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                input_local_states=local_states,\n                input_shared_state=shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Compute local grid loss summands.\",\n                round_idx=round_idx,\n                clean_models=clean_models,\n                method_params={\n                    \"prior_reg\": fit_mode == \"MAP\",\n                    \"refit_mode\": refit_mode,\n                },\n            )\n\n            # Aggregate local summands and refine the search interval.\n            shared_state, round_idx = aggregation_step(\n                aggregation_method=self.global_grid_update,\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                input_shared_states=shared_states,\n                description=\"Perform a global grid search update.\",\n                round_idx=round_idx,\n                clean_models=clean_models,\n                method_params={\n                    \"prior_reg\": fit_mode == \"MAP\",\n                    \"dispersions_param_name\": (\n                        \"genewise_dispersions\"\n                        if fit_mode == \"MLE\"\n                        else \"MAP_dispersions\"\n                    ),\n                },\n            )\n            end_iteration()\n        end_loop()\n\n        return local_states, shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.dispersions_grid_search.ComputeDispersionsGridSearch.fit_dispersions","title":"<code>fit_dispersions(train_data_nodes, aggregation_node, local_states, shared_state, round_idx, clean_models, fit_mode='MLE', refit_mode=False)</code>","text":"<p>Fit dispersions using grid search.</p> <p>Supports two modes: \"MLE\", to fit gene-wise dispersions, and \"MAP\", to fit MAP dispersions and filter them to avoid shrinking the dispersions of genes that are too far from the trend curve.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>shared_state</code> <p>If the fit_mode is \"MLE\", it is None. If the fit_mode is \"MAP\", it contains the output of the trend fitting, that is a dictionary with a \"fitted_dispersion\" field containing the fitted dispersions from the trend curve, a \"prior_disp_var\" field containing the prior variance of the dispersions, and a \"_squared_logres\" field containing the squared residuals of the trend fitting.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>Whether to clean the models after the computation.</p> required <code>fit_mode</code> <code>Literal['MLE', 'MAP']</code> <p>If \"MLE\", gene-wise dispersions are fitted independently, and <code>\"genewise_dispersions\"</code> fields are populated. If \"MAP\", prior regularization is applied, <code>\"MAP_dispersions\"</code> fields are populated.</p> <code>'MLE'</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>shared_state</code> <code>dict or list[dict]</code> <p>A dictionary containing: - \"genewise_dispersions\": The MLE dispersions, to be stored locally at - \"lower_log_bounds\": log lower bounds for the grid search (only used in internal loop), - \"upper_log_bounds\": log upper bounds for the grid search (only used in internal loop).</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/dispersions_grid_search.py</code> <pre><code>@log_organisation_method\ndef fit_dispersions(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    shared_state,\n    round_idx,\n    clean_models,\n    fit_mode: Literal[\"MLE\", \"MAP\"] = \"MLE\",\n    refit_mode: bool = False,\n):\n    \"\"\"Fit dispersions using grid search.\n\n    Supports two modes: \"MLE\", to fit gene-wise dispersions, and \"MAP\", to fit\n    MAP dispersions and filter them to avoid shrinking the dispersions of genes\n    that are too far from the trend curve.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_state: dict, optional\n        If the fit_mode is \"MLE\", it is None.\n        If the fit_mode is \"MAP\", it contains the output of the trend fitting,\n        that is a dictionary with a \"fitted_dispersion\" field containing\n        the fitted dispersions from the trend curve, a \"prior_disp_var\" field\n        containing the prior variance of the dispersions, and a \"_squared_logres\"\n        field containing the squared residuals of the trend fitting.\n\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        Whether to clean the models after the computation.\n\n    fit_mode: str\n        If \"MLE\", gene-wise dispersions are fitted independently, and\n        `\"genewise_dispersions\"` fields are populated. If \"MAP\", prior\n        regularization is applied, `\"MAP_dispersions\"` fields are populated.\n\n    refit_mode: bool\n        Whether to run on `refit_adata`s instead of `local_adata`s (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    shared_state: dict or list[dict]\n        A dictionary containing:\n        - \"genewise_dispersions\": The MLE dispersions, to be stored locally at\n        - \"lower_log_bounds\": log lower bounds for the grid search (only used in\n        internal loop),\n        - \"upper_log_bounds\": log upper bounds for the grid search (only used in\n        internal loop).\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    start_loop()\n    for iteration in range(self.grid_depth):\n        start_iteration(iteration)\n        # Compute local loss summands at all grid points.\n        local_states, shared_states, round_idx = local_step(\n            local_method=self.local_grid_loss,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            input_local_states=local_states,\n            input_shared_state=shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local grid loss summands.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\n                \"prior_reg\": fit_mode == \"MAP\",\n                \"refit_mode\": refit_mode,\n            },\n        )\n\n        # Aggregate local summands and refine the search interval.\n        shared_state, round_idx = aggregation_step(\n            aggregation_method=self.global_grid_update,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=shared_states,\n            description=\"Perform a global grid search update.\",\n            round_idx=round_idx,\n            clean_models=clean_models,\n            method_params={\n                \"prior_reg\": fit_mode == \"MAP\",\n                \"dispersions_param_name\": (\n                    \"genewise_dispersions\"\n                    if fit_mode == \"MLE\"\n                    else \"MAP_dispersions\"\n                ),\n            },\n        )\n        end_iteration()\n    end_loop()\n\n    return local_states, shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.substeps","title":"<code>substeps</code>","text":"<p>Module to implement the substeps to fit dispersions with MLE.</p> <p>This module contains all the substeps to fit dispersions using a grid search.</p>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.substeps.AggGridUpdate","title":"<code>AggGridUpdate</code>","text":"<p>Mixin to compute global MLE grid updates.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/substeps.py</code> <pre><code>class AggGridUpdate:\n    \"\"\"Mixin to compute global MLE grid updates.\"\"\"\n\n    min_disp: float\n    grid_batch_size: int\n    num_jobs: int\n    joblib_backend: str\n\n    @remote\n    @log_remote\n    def global_grid_update(\n        self,\n        shared_states,\n        prior_reg: bool = False,\n        dispersions_param_name: str = \"genewise_dispersions\",\n    ) -&gt; dict:\n        \"\"\"Aggregate local MLE summands on a grid and update global dispersion.\n\n        Also sets new search intervals for recursion.\n\n        Parameters\n        ----------\n        shared_states : list\n            List of local states dictionaries, with:\n            - \"nll\": local negative log-likelihoods (n_genes x grid_length),\n            - \"CR_summand\": local Cox-Reid adjustment summands\n            (n_params x n_params x n_genes x grid_length),\n            - \"grid\": grid of dispersions that were evaluated (n_genes x grid_length),\n            - \"max_disp\": global upper bound on dispersions.\n            - \"reg\": prior regularization to add for MAP dispersions\n              (only if prior_reg is True).\n\n        prior_reg : bool\n            Whether to include prior regularization, for MAP estimation\n            (default: False).\n\n\n        dispersions_param_name : str\n            Name of the dispersion parameter to update. Dispersions will be saved under\n            this name. (default: \"genewise_dispersions\").\n\n        Returns\n        -------\n        dict\n            Keys:\n            - dispersions_param_name: updated dispersions (n_genes),\n            - \"lower_log_bounds\": updated lower log bounds (n_genes),\n            - \"upper_log_bounds\": updated upper log bounds (n_genes).\n        \"\"\"\n        nll = sum([state[\"nll\"] for state in shared_states])\n        global_CR_summand = sum([state[\"CR_summand\"] for state in shared_states])\n\n        # Compute (batched) global losses\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(\n                n_jobs=self.num_jobs,\n            )(\n                delayed(global_grid_cr_loss)(\n                    nll=nll[i : i + self.grid_batch_size],\n                    cr_grid=global_CR_summand[i : i + self.grid_batch_size],\n                )\n                for i in range(0, len(nll), self.grid_batch_size)\n            )\n\n        if len(res) == 0:\n            global_losses = np.zeros((0, nll.shape[1]))\n        else:\n            global_losses = np.concatenate(res, axis=0)\n\n        if prior_reg:\n            global_losses += shared_states[0][\"reg\"]\n\n        # For each gene, find the argmin alpha, and the new search interval\n        grids = shared_states[0][\"grid\"]\n        # min_idx of shape n_genes\n        min_idx = np.argmin(global_losses, axis=1)\n        # delta of shape n_genes\n        alpha = grids[np.arange(len(grids)), min_idx]\n\n        # Compute the new bounds\n        # Note: the grid should be in log space\n        delta_grid = np.log(grids[:, 1]) - np.log(grids[:, 0])\n        log_grid_lower_bounds = np.maximum(\n            np.log(self.min_disp), np.log(alpha) - delta_grid\n        )\n        log_grid_upper_bounds = np.minimum(\n            np.log(shared_states[0][\"max_disp\"]), np.log(alpha) + delta_grid\n        )\n\n        # Set the dispersions of all-zero genes to NaN\n        alpha[~shared_states[0][\"non_zero\"]] = np.NaN\n\n        return {\n            dispersions_param_name: alpha,\n            \"lower_log_bounds\": log_grid_lower_bounds,\n            \"upper_log_bounds\": log_grid_upper_bounds,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.substeps.AggGridUpdate.global_grid_update","title":"<code>global_grid_update(shared_states, prior_reg=False, dispersions_param_name='genewise_dispersions')</code>","text":"<p>Aggregate local MLE summands on a grid and update global dispersion.</p> <p>Also sets new search intervals for recursion.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list</code> <p>List of local states dictionaries, with: - \"nll\": local negative log-likelihoods (n_genes x grid_length), - \"CR_summand\": local Cox-Reid adjustment summands (n_params x n_params x n_genes x grid_length), - \"grid\": grid of dispersions that were evaluated (n_genes x grid_length), - \"max_disp\": global upper bound on dispersions. - \"reg\": prior regularization to add for MAP dispersions   (only if prior_reg is True).</p> required <code>prior_reg</code> <code>bool</code> <p>Whether to include prior regularization, for MAP estimation (default: False).</p> <code>False</code> <code>dispersions_param_name</code> <code>str</code> <p>Name of the dispersion parameter to update. Dispersions will be saved under this name. (default: \"genewise_dispersions\").</p> <code>'genewise_dispersions'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Keys: - dispersions_param_name: updated dispersions (n_genes), - \"lower_log_bounds\": updated lower log bounds (n_genes), - \"upper_log_bounds\": updated upper log bounds (n_genes).</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/substeps.py</code> <pre><code>@remote\n@log_remote\ndef global_grid_update(\n    self,\n    shared_states,\n    prior_reg: bool = False,\n    dispersions_param_name: str = \"genewise_dispersions\",\n) -&gt; dict:\n    \"\"\"Aggregate local MLE summands on a grid and update global dispersion.\n\n    Also sets new search intervals for recursion.\n\n    Parameters\n    ----------\n    shared_states : list\n        List of local states dictionaries, with:\n        - \"nll\": local negative log-likelihoods (n_genes x grid_length),\n        - \"CR_summand\": local Cox-Reid adjustment summands\n        (n_params x n_params x n_genes x grid_length),\n        - \"grid\": grid of dispersions that were evaluated (n_genes x grid_length),\n        - \"max_disp\": global upper bound on dispersions.\n        - \"reg\": prior regularization to add for MAP dispersions\n          (only if prior_reg is True).\n\n    prior_reg : bool\n        Whether to include prior regularization, for MAP estimation\n        (default: False).\n\n\n    dispersions_param_name : str\n        Name of the dispersion parameter to update. Dispersions will be saved under\n        this name. (default: \"genewise_dispersions\").\n\n    Returns\n    -------\n    dict\n        Keys:\n        - dispersions_param_name: updated dispersions (n_genes),\n        - \"lower_log_bounds\": updated lower log bounds (n_genes),\n        - \"upper_log_bounds\": updated upper log bounds (n_genes).\n    \"\"\"\n    nll = sum([state[\"nll\"] for state in shared_states])\n    global_CR_summand = sum([state[\"CR_summand\"] for state in shared_states])\n\n    # Compute (batched) global losses\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(\n            n_jobs=self.num_jobs,\n        )(\n            delayed(global_grid_cr_loss)(\n                nll=nll[i : i + self.grid_batch_size],\n                cr_grid=global_CR_summand[i : i + self.grid_batch_size],\n            )\n            for i in range(0, len(nll), self.grid_batch_size)\n        )\n\n    if len(res) == 0:\n        global_losses = np.zeros((0, nll.shape[1]))\n    else:\n        global_losses = np.concatenate(res, axis=0)\n\n    if prior_reg:\n        global_losses += shared_states[0][\"reg\"]\n\n    # For each gene, find the argmin alpha, and the new search interval\n    grids = shared_states[0][\"grid\"]\n    # min_idx of shape n_genes\n    min_idx = np.argmin(global_losses, axis=1)\n    # delta of shape n_genes\n    alpha = grids[np.arange(len(grids)), min_idx]\n\n    # Compute the new bounds\n    # Note: the grid should be in log space\n    delta_grid = np.log(grids[:, 1]) - np.log(grids[:, 0])\n    log_grid_lower_bounds = np.maximum(\n        np.log(self.min_disp), np.log(alpha) - delta_grid\n    )\n    log_grid_upper_bounds = np.minimum(\n        np.log(shared_states[0][\"max_disp\"]), np.log(alpha) + delta_grid\n    )\n\n    # Set the dispersions of all-zero genes to NaN\n    alpha[~shared_states[0][\"non_zero\"]] = np.NaN\n\n    return {\n        dispersions_param_name: alpha,\n        \"lower_log_bounds\": log_grid_lower_bounds,\n        \"upper_log_bounds\": log_grid_upper_bounds,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.substeps.LocGridLoss","title":"<code>LocGridLoss</code>","text":"<p>Mixin to compute local MLE summands on a grid.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/substeps.py</code> <pre><code>class LocGridLoss:\n    \"\"\"Mixin to compute local MLE summands on a grid.\"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n    grid_batch_size: int\n    grid_length: int\n    min_disp: float\n    num_jobs: int\n    joblib_backend: str\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def local_grid_loss(\n        self,\n        data_from_opener,\n        shared_state,\n        prior_reg: bool = False,\n        refit_mode: bool = False,\n    ) -&gt; dict:\n        \"\"\"Compute local MLE losses and Cox-Reid summands on a grid.\n\n        Parameters\n        ----------\n        data_from_opener : ad.AnnData\n            Not used.\n\n        shared_state : dict, optional\n            Shared states with the previous search intervals \"lower_log_bounds\" and\n            \"upper_log_bounds\", except at initial step where it is None in the case\n            of gene-wise dispersions, or contains the output of the trend fitting\n            in the case of MAP dispersions.\n\n        prior_reg : bool\n            Whether to include prior regularization, for MAP estimation\n            (default: False).\n\n        refit_mode : bool\n            Whether to run on `refit_adata`s instead of `local_adata`s (default: False).\n\n        Returns\n        -------\n        dict\n            Keys:\n            - \"nll\": local negative log-likelihoods (n_genes x grid_length),\n            - \"CR_summand\": local Cox-Reid adjustment summands\n            (n_params x n_params x n_genes x grid_length),\n            - \"grid\": grid of dispersions to evaluate (n_genes x grid_length),\n            - \"n_samples\": number of samples in the local dataset,\n            - \"max_disp\": global upper bound on dispersions.\n            - \"non_zero\": mask of all zero genes.\n            - \"reg\": quadratic regularization term for MAP estimation (only if\n              `prior_reg=True`).\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # If we are fitting MAP dispersions and this is the first iteration, we need\n        # to save the results of the trend curve fitting.\n        # In refit mode, we can use the results from the previous iteration.\n        if not refit_mode:\n            if prior_reg and (\"fitted_dispersions\" not in self.local_adata.varm):\n                self.local_adata.varm[\"fitted_dispersions\"] = shared_state[\n                    \"fitted_dispersions\"\n                ]\n                self.local_adata.uns[\"trend_coeffs\"] = shared_state[\"trend_coeffs\"]\n                self.local_adata.uns[\"prior_disp_var\"] = shared_state[\"prior_disp_var\"]\n                self.local_adata.uns[\"_squared_logres\"] = shared_state[\n                    \"_squared_logres\"\n                ]\n                self.local_adata.uns[\"disp_function_type\"] = shared_state[\n                    \"disp_function_type\"\n                ]\n                self.local_adata.uns[\"mean_disp\"] = shared_state[\"mean_disp\"]\n\n        # Compute log space grids\n        if (shared_state is not None) and (\"lower_log_bounds\" in shared_state):\n            # Get the bounds from the previous iteration. Each gene has its own bounds.\n            min_log_alpha = shared_state[\"lower_log_bounds\"]  # ndarray (n_genes)\n            max_log_alpha = shared_state[\"upper_log_bounds\"]  # ndarray (n_genes)\n            grid = np.exp(np.linspace(min_log_alpha, max_log_alpha, self.grid_length)).T\n            # of size n_genes x grid_length\n        else:\n            # At first iteration, all genes get the same grid\n            min_log_alpha = np.log(self.min_disp)  # float\n            max_log_alpha = np.log(adata.uns[\"max_disp\"])  # float\n            grid = np.exp(np.linspace(min_log_alpha, max_log_alpha, self.grid_length))\n            # of size n_genes x grid_length\n            grid = np.repeat(grid[None, :], adata.n_vars, axis=0)\n\n        design = adata.obsm[\"design_matrix\"].values\n        n_params = design.shape[1]\n\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(\n                n_jobs=self.num_jobs,\n            )(\n                delayed(local_grid_summands)(\n                    counts=adata.X[:, i : i + self.grid_batch_size],\n                    design=design,\n                    mu=adata.layers[\"_mu_hat\"][:, i : i + self.grid_batch_size],\n                    alpha_grid=grid[i : i + self.grid_batch_size, :],\n                )\n                for i in range(0, adata.n_vars, self.grid_batch_size)\n            )\n            if len(res) == 0:\n                nll = np.zeros((0, self.grid_length))\n                CR_summand = np.zeros(\n                    (0, self.grid_length, n_params, n_params),\n                )\n            else:\n                nll = np.vstack([x[0] for x in res])\n                CR_summand = np.vstack([x[1] for x in res])\n\n            result_shared_state = {\n                \"nll\": nll,\n                \"CR_summand\": CR_summand,\n                \"grid\": grid,\n                \"max_disp\": adata.uns[\"max_disp\"],\n                \"non_zero\": adata.varm[\"non_zero\"],\n            }\n\n            if prior_reg:\n                reg = (\n                    np.log(grid) - np.log(adata.varm[\"fitted_dispersions\"])[:, None]\n                ) ** 2 / (2 * adata.uns[\"prior_disp_var\"])\n\n                result_shared_state[\"reg\"] = reg\n\n        return result_shared_state\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#fedpydeseq2.core.fed_algorithms.dispersions_grid_search.substeps.LocGridLoss.local_grid_loss","title":"<code>local_grid_loss(data_from_opener, shared_state, prior_reg=False, refit_mode=False)</code>","text":"<p>Compute local MLE losses and Cox-Reid summands on a grid.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>Shared states with the previous search intervals \"lower_log_bounds\" and \"upper_log_bounds\", except at initial step where it is None in the case of gene-wise dispersions, or contains the output of the trend fitting in the case of MAP dispersions.</p> required <code>prior_reg</code> <code>bool</code> <p>Whether to include prior regularization, for MAP estimation (default: False).</p> <code>False</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Keys: - \"nll\": local negative log-likelihoods (n_genes x grid_length), - \"CR_summand\": local Cox-Reid adjustment summands (n_params x n_params x n_genes x grid_length), - \"grid\": grid of dispersions to evaluate (n_genes x grid_length), - \"n_samples\": number of samples in the local dataset, - \"max_disp\": global upper bound on dispersions. - \"non_zero\": mask of all zero genes. - \"reg\": quadratic regularization term for MAP estimation (only if   <code>prior_reg=True</code>).</p> Source code in <code>fedpydeseq2/core/fed_algorithms/dispersions_grid_search/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef local_grid_loss(\n    self,\n    data_from_opener,\n    shared_state,\n    prior_reg: bool = False,\n    refit_mode: bool = False,\n) -&gt; dict:\n    \"\"\"Compute local MLE losses and Cox-Reid summands on a grid.\n\n    Parameters\n    ----------\n    data_from_opener : ad.AnnData\n        Not used.\n\n    shared_state : dict, optional\n        Shared states with the previous search intervals \"lower_log_bounds\" and\n        \"upper_log_bounds\", except at initial step where it is None in the case\n        of gene-wise dispersions, or contains the output of the trend fitting\n        in the case of MAP dispersions.\n\n    prior_reg : bool\n        Whether to include prior regularization, for MAP estimation\n        (default: False).\n\n    refit_mode : bool\n        Whether to run on `refit_adata`s instead of `local_adata`s (default: False).\n\n    Returns\n    -------\n    dict\n        Keys:\n        - \"nll\": local negative log-likelihoods (n_genes x grid_length),\n        - \"CR_summand\": local Cox-Reid adjustment summands\n        (n_params x n_params x n_genes x grid_length),\n        - \"grid\": grid of dispersions to evaluate (n_genes x grid_length),\n        - \"n_samples\": number of samples in the local dataset,\n        - \"max_disp\": global upper bound on dispersions.\n        - \"non_zero\": mask of all zero genes.\n        - \"reg\": quadratic regularization term for MAP estimation (only if\n          `prior_reg=True`).\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # If we are fitting MAP dispersions and this is the first iteration, we need\n    # to save the results of the trend curve fitting.\n    # In refit mode, we can use the results from the previous iteration.\n    if not refit_mode:\n        if prior_reg and (\"fitted_dispersions\" not in self.local_adata.varm):\n            self.local_adata.varm[\"fitted_dispersions\"] = shared_state[\n                \"fitted_dispersions\"\n            ]\n            self.local_adata.uns[\"trend_coeffs\"] = shared_state[\"trend_coeffs\"]\n            self.local_adata.uns[\"prior_disp_var\"] = shared_state[\"prior_disp_var\"]\n            self.local_adata.uns[\"_squared_logres\"] = shared_state[\n                \"_squared_logres\"\n            ]\n            self.local_adata.uns[\"disp_function_type\"] = shared_state[\n                \"disp_function_type\"\n            ]\n            self.local_adata.uns[\"mean_disp\"] = shared_state[\"mean_disp\"]\n\n    # Compute log space grids\n    if (shared_state is not None) and (\"lower_log_bounds\" in shared_state):\n        # Get the bounds from the previous iteration. Each gene has its own bounds.\n        min_log_alpha = shared_state[\"lower_log_bounds\"]  # ndarray (n_genes)\n        max_log_alpha = shared_state[\"upper_log_bounds\"]  # ndarray (n_genes)\n        grid = np.exp(np.linspace(min_log_alpha, max_log_alpha, self.grid_length)).T\n        # of size n_genes x grid_length\n    else:\n        # At first iteration, all genes get the same grid\n        min_log_alpha = np.log(self.min_disp)  # float\n        max_log_alpha = np.log(adata.uns[\"max_disp\"])  # float\n        grid = np.exp(np.linspace(min_log_alpha, max_log_alpha, self.grid_length))\n        # of size n_genes x grid_length\n        grid = np.repeat(grid[None, :], adata.n_vars, axis=0)\n\n    design = adata.obsm[\"design_matrix\"].values\n    n_params = design.shape[1]\n\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(\n            n_jobs=self.num_jobs,\n        )(\n            delayed(local_grid_summands)(\n                counts=adata.X[:, i : i + self.grid_batch_size],\n                design=design,\n                mu=adata.layers[\"_mu_hat\"][:, i : i + self.grid_batch_size],\n                alpha_grid=grid[i : i + self.grid_batch_size, :],\n            )\n            for i in range(0, adata.n_vars, self.grid_batch_size)\n        )\n        if len(res) == 0:\n            nll = np.zeros((0, self.grid_length))\n            CR_summand = np.zeros(\n                (0, self.grid_length, n_params, n_params),\n            )\n        else:\n            nll = np.vstack([x[0] for x in res])\n            CR_summand = np.vstack([x[1] for x in res])\n\n        result_shared_state = {\n            \"nll\": nll,\n            \"CR_summand\": CR_summand,\n            \"grid\": grid,\n            \"max_disp\": adata.uns[\"max_disp\"],\n            \"non_zero\": adata.varm[\"non_zero\"],\n        }\n\n        if prior_reg:\n            reg = (\n                np.log(grid) - np.log(adata.varm[\"fitted_dispersions\"])[:, None]\n            ) ** 2 / (2 * adata.uns[\"prior_disp_var\"])\n\n            result_shared_state[\"reg\"] = reg\n\n    return result_shared_state\n</code></pre>"},{"location":"api/core/federated_algorithms/dispersions_grid_search/dispersions_grid_search/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 1 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 1 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 1 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 1 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 1 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 2 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 2 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 2 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 3 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 3 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 3 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 3 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 3 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 4 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 4 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 4 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center"},{"location":"api/core/federated_algorithms/dispersions_grid_search/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 1 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 1 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between \\(\\texttt{min\\_disp}\\) \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 1 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between \\(\\texttt{min\\_disp}\\) and \\(\\max(\\texttt{max\\_disp},n)\\) of size \\(N_{\\texttt{gs}}\\). Each center Server 1 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 1 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 2 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 2 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 2 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center 3 nll nparray \\((G, N_{\\texttt{gs}})\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the negative log likelihood of the negative binomial GLM with fixed mean parameters \\(\\mu^{(k)}_{ig}\\) over the set of local samples and dispersion parameter \\(\\alpha_g\\). If the number of unique levels of all factors is equal to the number of parameters \\(p\\), then \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\texttt{nan}\\) for zero genes, and as the minimum of \\(\\gamma^{(k)}_{i}~(X^{(k)})^{\\top} G^{-1}\\Phi_g\\) and \\(\\texttt{min\\_mu}:=0.5\\) (for the definition of \\(G\\) and \\(\\Phi_g\\), see step 8). Otherwise, \\(\\mu^{(k)}_{ig}\\) is defined as \\(\\gamma^{(k)}_{i}~\\exp(X^{(k)}_{i} \\cdot \\beta_g)\\) on non-zero genes, where \\(\\beta_g\\) is the log fold change result of the IRLS algorithm, and as \\(\\texttt{nan}\\) on zero genes. Each center Server 3 CR_summand nparray \\((G, N_{\\texttt{gs}}, p, p)\\) For all genes \\(g\\) and all dispersion values \\(\\alpha_g\\) in the logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\), the matrix of the Cox-Reid regularization term pertaining to the samples in the center \\(k\\). This matrix is computed as \\(\\text{CR}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\), whose expression can be found in the description of the nll variable. Each center Server 3 grid nparray \\((G, N_{\\texttt{gs}})\\) The logarithmic grid between the exponential of the lower log bound and of the upper log bound of size \\(N_{\\texttt{gs}}\\). Each center Server 3 max_disp int The effective maximum value of the dispersion parameter \\(\\max(\\texttt{max\\_disp},n)\\). Each center Server 3 non_zero nparray \\((G,)\\) A boolean array indicating which genes have non-zero counts in at least one center. Each center Server 4 genewise_dispersions nparray \\((G,)\\) For each gene \\(g\\), the current estimate of the dispersion parameter \\(\\alpha_g\\). This estimate is computed by first computing the global nll (summing all local nlls)  as well as the global Cox-Reid regularization term, which is half the log determinant of the sum of the local Cox-Reid matrices. The Cox-Reid regularized nll per gene and per dispersion in the grid is obtained by summing the regularization term and the nll. Finally, for every gene, the dispersion parameter is estimated by taking the minizer of this regularized nll on the grid of size \\(N_{\\texttt{gs}}\\). Server Center 4 lower_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the maximum of the log of the min dispersion and \\(\\alpha_g - \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as a lower bound for the next grid search for the dispersion parameter. Server Center 4 upper_log_bounds nparray \\((G,)\\) For each gene \\(g\\), the minimum of the log of the max dispersion and  \\(\\alpha_g + \\delta\\) where \\(\\delta\\) is the current mesh size of the grid and \\(\\alpha_g\\) is the current dispersion estimate. This value will be used as an upper bound for the next grid search for the dispersion parameter. Server Center"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>run_fed_irls</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#api","title":"API","text":"<p>Module which contains the Mixin in charge of performing FedIRLS.</p>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.fed_irls","title":"<code>fed_irls</code>","text":"<p>Module containing the ComputeLFC method.</p>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.fed_irls.FedIRLS","title":"<code>FedIRLS</code>","text":"<p>               Bases: <code>LocMakeIRLSSummands</code>, <code>AggMakeIRLSUpdate</code></p> <p>Mixin class to implement the LFC computation algorithm.</p> <p>The goal of this class is to implement the IRLS algorithm specifically applied to the negative binomial distribution, with fixed dispersion parameter (only the mean parameter, expressed as the exponential of the log fold changes times the design matrix, is estimated). This algorithm is caught with another method on the genes on which it fails.</p> <p>To the best of our knowledge, there is no explicit implementation of IRLS for the negative binomial in a federated setting. However, the steps of IRLS are akin to the ones of a Newton-Raphson algorithm, with the difference that the Hessian matrix is replaced by the Fisher information matrix.</p> <p>Let us recall the steps of the IRLS algorithm for one gene (this method then implements these iterations for all genes in parallell). We want to estimate the log fold changes :math:<code>\\beta</code> from the counts :math:<code>y</code> and the design matrix :math:<code>X</code>. The negative binomial likelihood is given by:</p> <p>.. math::     \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left( y_i \\log(\\mu_i) -     (y_i + \\alpha^{-1}) \\log(\\mu_i + \\alpha^{-1}) \\right) + \\text{const}(y, \\alpha)</p> <p>where :math:<code>\\mu_i = \\gamma_i\\exp(X_i \\cdot \\beta)</code> and :math:<code>\\alpha</code> is the dispersion parameter.</p> <p>Given an iterate :math:<code>\\beta_k</code>, the IRLS algorithm computes the next iterate :math:<code>\\beta_{k+1}</code> as follows.</p> <p>First, we compute the mean parameter :math:<code>\\mu_k</code> from the current iterate, using the formula of the log fold changes:</p> <p>.. math::     (\\mu_{k})_i = \\gamma_i \\exp(X_i \\cdot \\beta_k)</p> <p>In practice, we trim the values of :math:<code>\\mu_k</code> to a minimum value to ensure numerical stability.</p> <p>Then, we compute the weight matrix :math:<code>W_k</code> from the current iterate :math:<code>\\beta_k</code>, which is a diagonal matrix with diagonal elements:</p> <p>.. math::     (W_k){ii} = \\frac{\\mu}}{1 + \\mu_{k,i} \\alpha</p> <p>where :math:<code>\\alpha</code> is the dispersion parameter. This weight matrix is used to compute both the estimated variance (or hat matrix) and the feature vector :math:<code>z_k</code>:</p> <p>.. math::     z_k = \\log\\left(\\frac{\\mu_k}{\\gamma}\\right) + \\frac{y - \\mu_k}{\\mu_k}</p> <p>The estimated variance is given by:</p> <p>.. math::     H_k = X^T W_k X</p> <p>The update step is then given by:</p> <p>.. math::     \\beta_{k+1} = (H_k)^{-1} X^T W_k z_k</p> <p>This is akin to the Newton-Raphson algorithm, with the Hessian matrix replaced by the Fisher information, and the gradient replaced by the feature vector.</p> <p>Methods:</p> Name Description <code>run_fed_irls</code> <p>Run the IRLS algorithm.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/fed_irls.py</code> <pre><code>class FedIRLS(\n    LocMakeIRLSSummands,\n    AggMakeIRLSUpdate,\n):\n    r\"\"\"Mixin class to implement the LFC computation algorithm.\n\n    The goal of this class is to implement the IRLS algorithm specifically applied\n    to the negative binomial distribution, with fixed dispersion parameter (only\n    the mean parameter, expressed as the exponential of the log fold changes times\n    the design matrix, is estimated). This algorithm is caught with another method on\n    the genes on which it fails.\n\n    To the best of our knowledge, there is no explicit implementation of IRLS for the\n    negative binomial in a federated setting. However, the steps of IRLS are akin\n    to the ones of a Newton-Raphson algorithm, with the difference that the Hessian\n    matrix is replaced by the Fisher information matrix.\n\n    Let us recall the steps of the IRLS algorithm for one gene (this method then\n    implements these iterations for all genes in parallell).\n    We want to estimate the log fold changes :math:`\\beta` from the counts :math:`y`\n    and the design matrix :math:`X`. The negative binomial likelihood is given by:\n\n    .. math::\n        \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left( y_i \\log(\\mu_i) -\n        (y_i + \\alpha^{-1}) \\log(\\mu_i + \\alpha^{-1}) \\right) + \\text{const}(y, \\alpha)\n\n    where :math:`\\mu_i = \\gamma_i\\exp(X_i \\cdot \\beta)` and :math:`\\alpha` is\n    the dispersion parameter.\n\n    Given an iterate :math:`\\beta_k`, the IRLS algorithm computes the next iterate\n    :math:`\\beta_{k+1}` as follows.\n\n    First, we compute the mean parameter :math:`\\mu_k` from the current iterate, using\n    the formula of the log fold changes:\n\n    .. math::\n        (\\mu_{k})_i = \\gamma_i \\exp(X_i \\cdot \\beta_k)\n\n    In practice, we trim the values of :math:`\\mu_k` to a minimum value to ensure\n    numerical stability.\n\n    Then, we compute the weight matrix :math:`W_k` from the current iterate\n    :math:`\\beta_k`, which is a diagonal matrix with diagonal elements:\n\n    .. math::\n        (W_k)_{ii} = \\frac{\\mu_{k,i}}{1 + \\mu_{k,i} \\alpha}\n\n    where :math:`\\alpha` is the dispersion parameter.\n    This weight matrix is used to compute both the estimated variance (or hat matrix)\n    and the feature vector :math:`z_k`:\n\n    .. math::\n        z_k = \\log\\left(\\frac{\\mu_k}{\\gamma}\\right) + \\frac{y - \\mu_k}{\\mu_k}\n\n    The estimated variance is given by:\n\n    .. math::\n        H_k = X^T W_k X\n\n    The update step is then given by:\n\n    .. math::\n        \\beta_{k+1} = (H_k)^{-1} X^T W_k z_k\n\n    This is akin to the Newton-Raphson algorithm, with the\n    Hessian matrix replaced by the Fisher information, and the gradient replaced by the\n    feature vector.\n\n    Methods\n    -------\n    run_fed_irls\n        Run the IRLS algorithm.\n    \"\"\"\n\n    @log_organisation_method\n    def run_fed_irls(\n        self,\n        train_data_nodes: list,\n        aggregation_node: AggregationNode,\n        local_states: dict,\n        input_shared_state: dict,\n        round_idx: int,\n        clean_models: bool = True,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Run the IRLS algorithm.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        input_shared_state: dict\n            Shared state with the following keys:\n            - beta: ndarray\n                The current beta, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\\\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            If True, the models are cleaned.\n\n        refit_mode: bool\n            Whether to run on `refit_adata`s instead of `local_adata`s.\n            (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        global_irls_summands_nlls_shared_state: dict\n            Shared states containing the final IRLS results.\n            It contains nothing for now.\n            - beta: ndarray\n                The current beta, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\\\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        #### ---- Main training loop ---- #####\n\n        global_irls_summands_nlls_shared_state = input_shared_state\n\n        start_loop()\n        for iteration in range(self.irls_num_iter + 1):\n            start_iteration(iteration)\n            # ---- Compute local IRLS summands and nlls ---- #\n\n            (\n                local_states,\n                local_irls_summands_nlls_shared_states,\n                round_idx,\n            ) = local_step(\n                local_method=self.make_local_irls_summands_and_nlls,\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                round_idx=round_idx,\n                input_local_states=local_states,\n                input_shared_state=global_irls_summands_nlls_shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Compute local IRLS summands and nlls.\",\n                clean_models=clean_models,\n                method_params={\"refit_mode\": refit_mode},\n            )\n\n            # ---- Compute global IRLS update and nlls ---- #\n\n            global_irls_summands_nlls_shared_state, round_idx = aggregation_step(\n                aggregation_method=self.make_global_irls_update,\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                input_shared_states=local_irls_summands_nlls_shared_states,\n                round_idx=round_idx,\n                description=\"Update the log fold changes and nlls in IRLS.\",\n                clean_models=clean_models,\n            )\n            end_iteration()\n        end_loop()\n\n        return local_states, global_irls_summands_nlls_shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.fed_irls.FedIRLS.run_fed_irls","title":"<code>run_fed_irls(train_data_nodes, aggregation_node, local_states, input_shared_state, round_idx, clean_models=True, refit_mode=False)</code>","text":"<p>Run the IRLS algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <code>list</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <code>AggregationNode</code> <p>The aggregation node.</p> required <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> required <code>input_shared_state</code> <code>dict</code> <p>Shared state with the following keys: - beta: ndarray     The current beta, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape                (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> required <code>round_idx</code> <code>int</code> <p>The current round.</p> required <code>clean_models</code> <code>bool</code> <p>If True, the models are cleaned.</p> <code>True</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>global_irls_summands_nlls_shared_state</code> <code>dict</code> <p>Shared states containing the final IRLS results. It contains nothing for now. - beta: ndarray     The current beta, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape                (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/fed_irls.py</code> <pre><code>@log_organisation_method\ndef run_fed_irls(\n    self,\n    train_data_nodes: list,\n    aggregation_node: AggregationNode,\n    local_states: dict,\n    input_shared_state: dict,\n    round_idx: int,\n    clean_models: bool = True,\n    refit_mode: bool = False,\n):\n    \"\"\"Run the IRLS algorithm.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    input_shared_state: dict\n        Shared state with the following keys:\n        - beta: ndarray\n            The current beta, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\\\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        If True, the models are cleaned.\n\n    refit_mode: bool\n        Whether to run on `refit_adata`s instead of `local_adata`s.\n        (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    global_irls_summands_nlls_shared_state: dict\n        Shared states containing the final IRLS results.\n        It contains nothing for now.\n        - beta: ndarray\n            The current beta, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\\\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    #### ---- Main training loop ---- #####\n\n    global_irls_summands_nlls_shared_state = input_shared_state\n\n    start_loop()\n    for iteration in range(self.irls_num_iter + 1):\n        start_iteration(iteration)\n        # ---- Compute local IRLS summands and nlls ---- #\n\n        (\n            local_states,\n            local_irls_summands_nlls_shared_states,\n            round_idx,\n        ) = local_step(\n            local_method=self.make_local_irls_summands_and_nlls,\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=global_irls_summands_nlls_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local IRLS summands and nlls.\",\n            clean_models=clean_models,\n            method_params={\"refit_mode\": refit_mode},\n        )\n\n        # ---- Compute global IRLS update and nlls ---- #\n\n        global_irls_summands_nlls_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.make_global_irls_update,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=local_irls_summands_nlls_shared_states,\n            round_idx=round_idx,\n            description=\"Update the log fold changes and nlls in IRLS.\",\n            clean_models=clean_models,\n        )\n        end_iteration()\n    end_loop()\n\n    return local_states, global_irls_summands_nlls_shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.substeps","title":"<code>substeps</code>","text":"<p>Module to implement the substeps for the fitting of log fold changes.</p> <p>This module contains all these substeps as mixin classes.</p>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.substeps.AggMakeIRLSUpdate","title":"<code>AggMakeIRLSUpdate</code>","text":"<p>Mixin class to aggregate IRLS summands.</p> <p>Please refer to the method make_local_irls_summands_and_nlls for more.</p> <p>Attributes:</p> Name Type Description <code>num_jobs</code> <code>int</code> <p>The number of cpus to use.</p> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity of the joblib backend.</p> <code>joblib_backend</code> <code>str</code> <p>The backend to use for the joblib parallelization.</p> <code>irls_batch_size</code> <code>int</code> <p>The batch size to use for the IRLS algorithm.</p> <code>max_beta</code> <code>float</code> <p>The maximum value for the beta parameter.</p> <code>beta_tol</code> <code>float</code> <p>The tolerance for the beta parameter.</p> <code>irls_num_iter</code> <code>int</code> <p>The number of iterations for the IRLS algorithm.</p> <p>Methods:</p> Name Description <code>make_global_irls_update</code> <p>A remote method. Aggregates the local quantities to create the global IRLS update. It also updates the masks indicating which genes have diverged or converged according to the deviance.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/substeps.py</code> <pre><code>class AggMakeIRLSUpdate:\n    \"\"\"Mixin class to aggregate IRLS summands.\n\n    Please refer to the method make_local_irls_summands_and_nlls for more.\n\n    Attributes\n    ----------\n    num_jobs : int\n        The number of cpus to use.\n    joblib_verbosity : int\n        The verbosity of the joblib backend.\n    joblib_backend : str\n        The backend to use for the joblib parallelization.\n    irls_batch_size : int\n        The batch size to use for the IRLS algorithm.\n    max_beta : float\n        The maximum value for the beta parameter.\n    beta_tol : float\n        The tolerance for the beta parameter.\n    irls_num_iter : int\n        The number of iterations for the IRLS algorithm.\n\n    Methods\n    -------\n    make_global_irls_update\n        A remote method. Aggregates the local quantities to create\n        the global IRLS update. It also updates the masks indicating which genes\n        have diverged or converged according to the deviance.\n    \"\"\"\n\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n    max_beta: float\n    beta_tol: float\n    irls_num_iter: int\n\n    @remote\n    @log_remote\n    def make_global_irls_update(self, shared_states: list[dict]) -&gt; dict[str, Any]:\n        \"\"\"Make the summands for the IRLS algorithm.\n\n        The role of this function is twofold.\n\n        1) It computes the global_nll and updates the masks according to the deviance,\n        for the beta values that have been computed in the previous round.\n\n        2) It aggregates the local hat matrix and features to solve the linear system\n        and get the new beta values.\n\n        Parameters\n        ----------\n        shared_states: list[dict]\n            A list of dictionaries containing the following\n            keys:\n            - local_hat_matrix: ndarray\n                The local hat matrix, of shape (n_irls_genes, n_params, n_params).\n                n_irsl_genes is the number of genes that are still active (non zero\n                gene names on the irls_mask).\n            - local_features: ndarray\n                The local features, of shape (n_irls_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\\\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary containing all the necessary info to run IRLS.\n            It contains the following fields:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\\\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n        \"\"\"\n        # Load main params from the first state\n        beta = shared_states[0][\"beta\"]\n        irls_mask = shared_states[0][\"irls_mask\"]\n        irls_diverged_mask = shared_states[0][\"irls_diverged_mask\"]\n        global_nll = shared_states[0][\"global_nll\"]\n        round_number_irls = shared_states[0][\"round_number_irls\"]\n\n        # ---- Step 0: Aggregate the local hat matrix, features and global_nll ---- #\n\n        global_hat_matrix = sum([state[\"local_hat_matrix\"] for state in shared_states])\n        global_features = sum([state[\"local_features\"] for state in shared_states])\n        global_nll_on_irls_mask = sum([state[\"local_nll\"] for state in shared_states])\n\n        # ---- Step 1: update global_nll and masks ---- #\n\n        # The first round needs to be handled separately\n        if round_number_irls == 0:\n            # In that case, the irls_masks consists in all True values\n            # We only need set the initial global_nll\n            global_nll = global_nll_on_irls_mask\n\n        else:\n            old_global_nll = global_nll.copy()\n            old_irls_mask = irls_mask.copy()\n\n            global_nll[irls_mask] = global_nll_on_irls_mask\n\n            # Set the new masks with the dev ratio and beta values\n            deviance_ratio = np.abs(2 * global_nll - 2 * old_global_nll) / (\n                np.abs(2 * global_nll) + 0.1\n            )\n            irls_diverged_mask = irls_diverged_mask | (\n                np.abs(beta) &gt; self.max_beta\n            ).any(axis=1)\n\n            irls_mask = irls_mask &amp; (deviance_ratio &gt; self.beta_tol)\n            irls_mask = irls_mask &amp; ~irls_diverged_mask\n            new_mask_in_old_mask = (irls_mask &amp; old_irls_mask)[old_irls_mask]\n            global_hat_matrix = global_hat_matrix[new_mask_in_old_mask]\n            global_features = global_features[new_mask_in_old_mask]\n\n        if round_number_irls == self.irls_num_iter:\n            # In this case, we must prepare the switch to fed prox newton\n            return {\n                \"beta\": beta,\n                \"irls_diverged_mask\": irls_diverged_mask,\n                \"irls_mask\": irls_mask,\n                \"global_nll\": global_nll,\n                \"round_number_irls\": round_number_irls,\n            }\n\n        # ---- Step 2: Solve the system to compute beta ---- #\n\n        ridge_factor = np.diag(np.repeat(1e-6, global_hat_matrix.shape[1]))\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n                delayed(np.linalg.solve)(\n                    global_hat_matrix[i : i + self.irls_batch_size] + ridge_factor,\n                    global_features[i : i + self.irls_batch_size],\n                )\n                for i in range(0, len(global_hat_matrix), self.irls_batch_size)\n            )\n        if len(res) &gt; 0:\n            beta_hat = np.concatenate(res)\n        else:\n            beta_hat = np.zeros((0, global_hat_matrix.shape[1]))\n\n        # TODO :  it would be cleaner to pass an update, which is None at the first\n        #  round. That way we do not update beta in a different step its evaluation.\n\n        # Update the beta\n        beta[irls_mask] = beta_hat\n\n        round_number_irls = round_number_irls + 1\n\n        return {\n            \"beta\": beta,\n            \"irls_diverged_mask\": irls_diverged_mask,\n            \"irls_mask\": irls_mask,\n            \"global_nll\": global_nll,\n            \"round_number_irls\": round_number_irls,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.substeps.AggMakeIRLSUpdate.make_global_irls_update","title":"<code>make_global_irls_update(shared_states)</code>","text":"<p>Make the summands for the IRLS algorithm.</p> <p>The role of this function is twofold.</p> <p>1) It computes the global_nll and updates the masks according to the deviance, for the beta values that have been computed in the previous round.</p> <p>2) It aggregates the local hat matrix and features to solve the linear system and get the new beta values.</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>A list of dictionaries containing the following keys: - local_hat_matrix: ndarray     The local hat matrix, of shape (n_irls_genes, n_params, n_params).     n_irsl_genes is the number of genes that are still active (non zero     gene names on the irls_mask). - local_features: ndarray     The local features, of shape (n_irls_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape                (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing all the necessary info to run IRLS. It contains the following fields: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape                (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/substeps.py</code> <pre><code>@remote\n@log_remote\ndef make_global_irls_update(self, shared_states: list[dict]) -&gt; dict[str, Any]:\n    \"\"\"Make the summands for the IRLS algorithm.\n\n    The role of this function is twofold.\n\n    1) It computes the global_nll and updates the masks according to the deviance,\n    for the beta values that have been computed in the previous round.\n\n    2) It aggregates the local hat matrix and features to solve the linear system\n    and get the new beta values.\n\n    Parameters\n    ----------\n    shared_states: list[dict]\n        A list of dictionaries containing the following\n        keys:\n        - local_hat_matrix: ndarray\n            The local hat matrix, of shape (n_irls_genes, n_params, n_params).\n            n_irsl_genes is the number of genes that are still active (non zero\n            gene names on the irls_mask).\n        - local_features: ndarray\n            The local features, of shape (n_irls_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\\\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary containing all the necessary info to run IRLS.\n        It contains the following fields:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\\\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n    \"\"\"\n    # Load main params from the first state\n    beta = shared_states[0][\"beta\"]\n    irls_mask = shared_states[0][\"irls_mask\"]\n    irls_diverged_mask = shared_states[0][\"irls_diverged_mask\"]\n    global_nll = shared_states[0][\"global_nll\"]\n    round_number_irls = shared_states[0][\"round_number_irls\"]\n\n    # ---- Step 0: Aggregate the local hat matrix, features and global_nll ---- #\n\n    global_hat_matrix = sum([state[\"local_hat_matrix\"] for state in shared_states])\n    global_features = sum([state[\"local_features\"] for state in shared_states])\n    global_nll_on_irls_mask = sum([state[\"local_nll\"] for state in shared_states])\n\n    # ---- Step 1: update global_nll and masks ---- #\n\n    # The first round needs to be handled separately\n    if round_number_irls == 0:\n        # In that case, the irls_masks consists in all True values\n        # We only need set the initial global_nll\n        global_nll = global_nll_on_irls_mask\n\n    else:\n        old_global_nll = global_nll.copy()\n        old_irls_mask = irls_mask.copy()\n\n        global_nll[irls_mask] = global_nll_on_irls_mask\n\n        # Set the new masks with the dev ratio and beta values\n        deviance_ratio = np.abs(2 * global_nll - 2 * old_global_nll) / (\n            np.abs(2 * global_nll) + 0.1\n        )\n        irls_diverged_mask = irls_diverged_mask | (\n            np.abs(beta) &gt; self.max_beta\n        ).any(axis=1)\n\n        irls_mask = irls_mask &amp; (deviance_ratio &gt; self.beta_tol)\n        irls_mask = irls_mask &amp; ~irls_diverged_mask\n        new_mask_in_old_mask = (irls_mask &amp; old_irls_mask)[old_irls_mask]\n        global_hat_matrix = global_hat_matrix[new_mask_in_old_mask]\n        global_features = global_features[new_mask_in_old_mask]\n\n    if round_number_irls == self.irls_num_iter:\n        # In this case, we must prepare the switch to fed prox newton\n        return {\n            \"beta\": beta,\n            \"irls_diverged_mask\": irls_diverged_mask,\n            \"irls_mask\": irls_mask,\n            \"global_nll\": global_nll,\n            \"round_number_irls\": round_number_irls,\n        }\n\n    # ---- Step 2: Solve the system to compute beta ---- #\n\n    ridge_factor = np.diag(np.repeat(1e-6, global_hat_matrix.shape[1]))\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n            delayed(np.linalg.solve)(\n                global_hat_matrix[i : i + self.irls_batch_size] + ridge_factor,\n                global_features[i : i + self.irls_batch_size],\n            )\n            for i in range(0, len(global_hat_matrix), self.irls_batch_size)\n        )\n    if len(res) &gt; 0:\n        beta_hat = np.concatenate(res)\n    else:\n        beta_hat = np.zeros((0, global_hat_matrix.shape[1]))\n\n    # TODO :  it would be cleaner to pass an update, which is None at the first\n    #  round. That way we do not update beta in a different step its evaluation.\n\n    # Update the beta\n    beta[irls_mask] = beta_hat\n\n    round_number_irls = round_number_irls + 1\n\n    return {\n        \"beta\": beta,\n        \"irls_diverged_mask\": irls_diverged_mask,\n        \"irls_mask\": irls_mask,\n        \"global_nll\": global_nll,\n        \"round_number_irls\": round_number_irls,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.substeps.LocMakeIRLSSummands","title":"<code>LocMakeIRLSSummands</code>","text":"<p>Mixin to make the summands for the IRLS algorithm.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>The local AnnData object.</p> <code>num_jobs</code> <code>int</code> <p>The number of cpus to use.</p> <code>joblib_verbosity</code> <code>int</code> <p>The verbosity of the joblib backend.</p> <code>joblib_backend</code> <code>str</code> <p>The backend to use for the joblib parallelization.</p> <code>irls_batch_size</code> <code>int</code> <p>The batch size to use for the IRLS algorithm.</p> <code>min_mu</code> <code>float</code> <p>The minimum value for the mu parameter.</p> <code>irls_num_iter</code> <code>int</code> <p>The number of iterations for the IRLS algorithm.</p> <p>Methods:</p> Name Description <code>make_local_irls_summands_and_nlls</code> <p>A remote_data method. Makes the summands for the IRLS algorithm. It also passes on the necessary global quantities.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/substeps.py</code> <pre><code>class LocMakeIRLSSummands:\n    \"\"\"Mixin to make the summands for the IRLS algorithm.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        The local AnnData object.\n    num_jobs : int\n        The number of cpus to use.\n    joblib_verbosity : int\n        The verbosity of the joblib backend.\n    joblib_backend : str\n        The backend to use for the joblib parallelization.\n    irls_batch_size : int\n        The batch size to use for the IRLS algorithm.\n    min_mu : float\n        The minimum value for the mu parameter.\n    irls_num_iter : int\n        The number of iterations for the IRLS algorithm.\n\n    Methods\n    -------\n    make_local_irls_summands_and_nlls\n        A remote_data method. Makes the summands for the IRLS algorithm.\n        It also passes on the necessary global quantities.\n    \"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n    min_mu: float\n    irls_num_iter: int\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def make_local_irls_summands_and_nlls(\n        self,\n        data_from_opener: AnnData,\n        shared_state: dict[str, Any],\n        refit_mode: bool = False,\n    ):\n        \"\"\"Make the summands for the IRLS algorithm.\n\n        This functions does two main operations:\n\n        1) It computes the summands for the beta update.\n        2) It computes the local quantities to compute the global_nll\n        of the current beta\n\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            A dictionary containing the following\n            keys:\n            - beta: ndarray\n                The current beta, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta from the previous beta, of shape\\\n                (n_non_zero_genes,).\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n\n        refit_mode : bool\n            Whether to run on `refit_adata`s instead of `local_adata`s.\n            (default: False).\n\n        Returns\n        -------\n        dict\n            The state to share to the server.\n            It contains the following fields:\n            - beta: ndarray\n                The current beta, of shape (n_non_zero_genes, n_params).\n            - local_nll: ndarray\n                The local nll of the current beta, of shape (n_irls_genes,).\n            - local_hat_matrix: ndarray\n                The local hat matrix, of shape (n_irls_genes, n_params, n_params).\n                n_irsl_genes is the number of genes that are still active (non zero\n                gene names on the irls_mask).\n            - local_features: ndarray\n                The local features, of shape (n_irls_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if fed avg should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - irls_mask: ndarray\n                A boolean mask indicating if IRLS should be used for a given gene\n                (shape: (n_non_zero_genes,)).\n            - global_nll: ndarray\n                The global_nll of the current beta of shape\n                (n_non_zero_genes,).\n                This parameter is simply passed to the next shared state\n            - round_number_irls: int\n                The current round number of the IRLS algorithm.\n                This round number is not updated here.\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # Put all elements in the shared state in readable variables\n        beta = shared_state[\"beta\"]\n        irls_mask = shared_state[\"irls_mask\"]\n        irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n        global_nll = shared_state[\"global_nll\"]\n        round_number_irls = shared_state[\"round_number_irls\"]\n\n        # Get the quantitie stored in the adata\n        disp_param_name = adata.uns[\"_irls_disp_param_name\"]\n\n        # If this is the first round, save the beta init in a field of the local adata\n        if round_number_irls == 0:\n            adata.uns[\"_irls_beta_init\"] = beta.copy()\n\n        (\n            irls_gene_names,\n            design_matrix,\n            size_factors,\n            counts,\n            dispersions,\n            beta_genes,\n        ) = get_lfc_utils_from_gene_mask_adata(\n            adata, irls_mask, beta=beta, disp_param_name=disp_param_name\n        )\n\n        # ---- Compute the summands for the beta update and the local nll ---- #\n\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n                delayed(make_irls_update_summands_and_nll_batch)(\n                    design_matrix,\n                    size_factors,\n                    beta_genes[i : i + self.irls_batch_size],\n                    dispersions[i : i + self.irls_batch_size],\n                    counts[:, i : i + self.irls_batch_size],\n                    self.min_mu,\n                )\n                for i in range(0, len(beta_genes), self.irls_batch_size)\n            )\n\n        if len(res) == 0:\n            H = np.zeros((0, beta.shape[1], beta.shape[1]))\n            y = np.zeros((0, beta.shape[1]))\n            local_nll = np.zeros(0)\n        else:\n            H = np.concatenate([r[0] for r in res])\n            y = np.concatenate([r[1] for r in res])\n            local_nll = np.concatenate([r[2] for r in res])\n\n        # Create the shared state\n        return {\n            \"beta\": beta,\n            \"local_nll\": local_nll,\n            \"local_hat_matrix\": H,\n            \"local_features\": y,\n            \"irls_gene_names\": irls_gene_names,\n            \"irls_diverged_mask\": irls_diverged_mask,\n            \"irls_mask\": irls_mask,\n            \"global_nll\": global_nll,\n            \"round_number_irls\": round_number_irls,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.substeps.LocMakeIRLSSummands.make_local_irls_summands_and_nlls","title":"<code>make_local_irls_summands_and_nlls(data_from_opener, shared_state, refit_mode=False)</code>","text":"<p>Make the summands for the IRLS algorithm.</p> <p>This functions does two main operations:</p> <p>1) It computes the summands for the beta update. 2) It computes the local quantities to compute the global_nll of the current beta</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>A dictionary containing the following keys: - beta: ndarray     The current beta, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta from the previous beta, of shape                (n_non_zero_genes,). - round_number_irls: int     The current round number of the IRLS algorithm.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The state to share to the server. It contains the following fields: - beta: ndarray     The current beta, of shape (n_non_zero_genes, n_params). - local_nll: ndarray     The local nll of the current beta, of shape (n_irls_genes,). - local_hat_matrix: ndarray     The local hat matrix, of shape (n_irls_genes, n_params, n_params).     n_irsl_genes is the number of genes that are still active (non zero     gene names on the irls_mask). - local_features: ndarray     The local features, of shape (n_irls_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if fed avg should be used for a given gene     (shape: (n_non_zero_genes,)). - irls_mask: ndarray     A boolean mask indicating if IRLS should be used for a given gene     (shape: (n_non_zero_genes,)). - global_nll: ndarray     The global_nll of the current beta of shape     (n_non_zero_genes,).     This parameter is simply passed to the next shared state - round_number_irls: int     The current round number of the IRLS algorithm.     This round number is not updated here.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef make_local_irls_summands_and_nlls(\n    self,\n    data_from_opener: AnnData,\n    shared_state: dict[str, Any],\n    refit_mode: bool = False,\n):\n    \"\"\"Make the summands for the IRLS algorithm.\n\n    This functions does two main operations:\n\n    1) It computes the summands for the beta update.\n    2) It computes the local quantities to compute the global_nll\n    of the current beta\n\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        A dictionary containing the following\n        keys:\n        - beta: ndarray\n            The current beta, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta from the previous beta, of shape\\\n            (n_non_zero_genes,).\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n\n    refit_mode : bool\n        Whether to run on `refit_adata`s instead of `local_adata`s.\n        (default: False).\n\n    Returns\n    -------\n    dict\n        The state to share to the server.\n        It contains the following fields:\n        - beta: ndarray\n            The current beta, of shape (n_non_zero_genes, n_params).\n        - local_nll: ndarray\n            The local nll of the current beta, of shape (n_irls_genes,).\n        - local_hat_matrix: ndarray\n            The local hat matrix, of shape (n_irls_genes, n_params, n_params).\n            n_irsl_genes is the number of genes that are still active (non zero\n            gene names on the irls_mask).\n        - local_features: ndarray\n            The local features, of shape (n_irls_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if fed avg should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - irls_mask: ndarray\n            A boolean mask indicating if IRLS should be used for a given gene\n            (shape: (n_non_zero_genes,)).\n        - global_nll: ndarray\n            The global_nll of the current beta of shape\n            (n_non_zero_genes,).\n            This parameter is simply passed to the next shared state\n        - round_number_irls: int\n            The current round number of the IRLS algorithm.\n            This round number is not updated here.\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # Put all elements in the shared state in readable variables\n    beta = shared_state[\"beta\"]\n    irls_mask = shared_state[\"irls_mask\"]\n    irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n    global_nll = shared_state[\"global_nll\"]\n    round_number_irls = shared_state[\"round_number_irls\"]\n\n    # Get the quantitie stored in the adata\n    disp_param_name = adata.uns[\"_irls_disp_param_name\"]\n\n    # If this is the first round, save the beta init in a field of the local adata\n    if round_number_irls == 0:\n        adata.uns[\"_irls_beta_init\"] = beta.copy()\n\n    (\n        irls_gene_names,\n        design_matrix,\n        size_factors,\n        counts,\n        dispersions,\n        beta_genes,\n    ) = get_lfc_utils_from_gene_mask_adata(\n        adata, irls_mask, beta=beta, disp_param_name=disp_param_name\n    )\n\n    # ---- Compute the summands for the beta update and the local nll ---- #\n\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n            delayed(make_irls_update_summands_and_nll_batch)(\n                design_matrix,\n                size_factors,\n                beta_genes[i : i + self.irls_batch_size],\n                dispersions[i : i + self.irls_batch_size],\n                counts[:, i : i + self.irls_batch_size],\n                self.min_mu,\n            )\n            for i in range(0, len(beta_genes), self.irls_batch_size)\n        )\n\n    if len(res) == 0:\n        H = np.zeros((0, beta.shape[1], beta.shape[1]))\n        y = np.zeros((0, beta.shape[1]))\n        local_nll = np.zeros(0)\n    else:\n        H = np.concatenate([r[0] for r in res])\n        y = np.concatenate([r[1] for r in res])\n        local_nll = np.concatenate([r[2] for r in res])\n\n    # Create the shared state\n    return {\n        \"beta\": beta,\n        \"local_nll\": local_nll,\n        \"local_hat_matrix\": H,\n        \"local_features\": y,\n        \"irls_gene_names\": irls_gene_names,\n        \"irls_diverged_mask\": irls_diverged_mask,\n        \"irls_mask\": irls_mask,\n        \"global_nll\": global_nll,\n        \"round_number_irls\": round_number_irls,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.utils","title":"<code>utils</code>","text":"<p>Module to implement the utilities of the IRLS algorithm.</p> <p>Most of these functions have the _batch suffix, which means that they are vectorized to work over batches of genes in the parallel_backend file in the same module.</p>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#fedpydeseq2.core.fed_algorithms.fed_irls.utils.make_irls_update_summands_and_nll_batch","title":"<code>make_irls_update_summands_and_nll_batch(design_matrix, size_factors, beta, dispersions, counts, min_mu)</code>","text":"<p>Make the summands for the IRLS algorithm for a given set of genes.</p> <p>Parameters:</p> Name Type Description Default <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <code>beta</code> <code>ndarray</code> <p>The log fold change matrix, of shape (batch_size, n_params).</p> required <code>dispersions</code> <code>ndarray</code> <p>The dispersions, of shape (batch_size).</p> required <code>counts</code> <code>ndarray</code> <p>The counts, of shape (n_obs,batch_size).</p> required <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability.</p> required <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>The H matrix, of shape (batch_size, n_params, n_params).</p> <code>y</code> <code>ndarray</code> <p>The y vector, of shape (batch_size, n_params).</p> <code>nll</code> <code>ndarray</code> <p>The negative binomial negative log-likelihood, of shape (batch_size).</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_irls/utils.py</code> <pre><code>def make_irls_update_summands_and_nll_batch(\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n    beta: np.ndarray,\n    dispersions: np.ndarray,\n    counts: np.ndarray,\n    min_mu: float,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Make the summands for the IRLS algorithm for a given set of genes.\n\n    Parameters\n    ----------\n    design_matrix : ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : ndarray\n        The size factors, of shape (n_obs).\n    beta : ndarray\n        The log fold change matrix, of shape (batch_size, n_params).\n    dispersions : ndarray\n        The dispersions, of shape (batch_size).\n    counts : ndarray\n        The counts, of shape (n_obs,batch_size).\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n\n    Returns\n    -------\n    H : ndarray\n        The H matrix, of shape (batch_size, n_params, n_params).\n    y : ndarray\n        The y vector, of shape (batch_size, n_params).\n    nll : ndarray\n        The negative binomial negative log-likelihood, of shape (batch_size).\n    \"\"\"\n    max_limit = np.log(1e100)\n    design_matrix_time_beta_T = design_matrix @ beta.T\n    mask_nan = design_matrix_time_beta_T &gt; max_limit\n\n    # In order to avoid overflow and np.inf, we replace all big values in the\n    # design_matrix_time_beta_T with 0., then we carry the computation normally, and\n    # we modify the final quantity with their true value for the inputs were\n    # exp_design_matrix_time_beta_T should have taken values &gt;&gt; 1\n    exp_design_matrix_time_beta_T = np.zeros(\n        design_matrix_time_beta_T.shape, dtype=design_matrix_time_beta_T.dtype\n    )\n    exp_design_matrix_time_beta_T[~mask_nan] = np.exp(\n        design_matrix_time_beta_T[~mask_nan]\n    )\n    mu = size_factors[:, None] * exp_design_matrix_time_beta_T\n\n    mu = np.maximum(mu, min_mu)\n\n    W = mu / (1.0 + mu * dispersions[None, :])\n\n    dispersions_broadcast = np.broadcast_to(\n        dispersions, (mu.shape[0], dispersions.shape[0])\n    )\n    W[mask_nan] = 1.0 / dispersions_broadcast[mask_nan]\n\n    z = np.log(mu / size_factors[:, None]) + (counts - mu) / mu\n    z[mask_nan] = design_matrix_time_beta_T[mask_nan] - 1.0\n\n    H = (design_matrix.T[:, :, None] * W).transpose(2, 0, 1) @ design_matrix[None, :, :]\n    y = (design_matrix.T @ (W * z)).T\n\n    mu[mask_nan] = np.inf\n    nll = grid_nb_nll(counts, mu, dispersions, mask_nan)\n\n    return H, y, nll\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_irls/fed_irls/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 0 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 0 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 0 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 0 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 0 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 1 round_number_irls int The current round number of the IRLS algorithm. Simply passed on. Each center Server 1 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood at the \\(\\beta_g\\) parameter. Simply passed on. Each center Server 1 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Simply passed on. Each center Server 1 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on. Each center Server 1 irls_gene_names Index \\((G_{\\texttt{act}},)\\) The gene names of the active genes. Each center Server 1 local_features nparray \\((G_{\\texttt{act}}, p)\\) For each active gene \\(g\\), the projected features \\((X^{(k)})^{\\top} W^{(k)}_g \\phi^{(k)}_{g}\\), where \\(\\phi^{(k)}_{ig} = \\log\\left(\\frac{\\mu^{(k)}_{ig}}{\\gamma_{i}}\\right) + \\frac{Y^{(k)}_{ig} - \\mu^{(k)}_{ig}}{\\mu^{(k)}_{ig}}\\) and \\(\\phi^{(k)}_{g}\\) is the vector of \\((\\phi^{(k)}_{ig})_{1 \\leq i \\leq n_k}\\). Each center Server 1 local_hat_matrix nparray \\((G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 local_nll nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data. Each center Server 1 beta nparray \\((G_{\\texttt{nz}}, p)\\) The log fold change \\(\\beta\\) parameter. Not modified. Each center Server 2 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 2 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 2 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 2 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 2 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center"},{"location":"api/core/federated_algorithms/fed_irls/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 0 beta nparray \\((G_{\\texttt{nz}}, p)\\) The initial value of the \\(\\beta\\) parameter. If the Gramm matrix is full rank, is set to \\(\\beta_g = G^{-1}(\\sum_k\\Phi^{(k)}_{g})\\) for all genes \\(g\\). Otherwise, it is set as a weighed average of the normed log means, i.e., \\(\\beta_g = \\sum_k \\tfrac{n_k}{n}~\\overline{\\log(Z)}^{(k)}_{g}\\), where \\(\\overline{\\log(Z)}^{(k)}_{g} = \\tfrac{1}{n_k}\\sum_{i=1}^{n_k}{\\log(Z^{(k)}_{ig})}\\) is computed locally. Server Center 0 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Initialized to \\(\\texttt{False}\\). Server Center 0 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Initialized to \\(\\texttt{True}\\). Server Center 0 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM, initialized at \\(1000\\) for all genes. Server Center 0 round_number_irls int The current round number of the IRLS algorithm. Initialized at \\(0\\). Server Center 1 round_number_irls int The current round number of the IRLS algorithm. Simply passed on. Each center Server 1 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log likelihood at the \\(\\beta_g\\) parameter. Simply passed on. Each center Server 1 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Simply passed on. Each center Server 1 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on. Each center Server 1 irls_gene_names Index \\((G_{\\texttt{act}},)\\) The gene names of the active genes. Each center Server 1 local_features nparray \\((G_{\\texttt{act}}, p)\\) For each active gene \\(g\\), the projected features \\((X^{(k)})^{\\top} W^{(k)}_g \\phi^{(k)}_{g}\\), where \\(\\phi^{(k)}_{ig} = \\log\\left(\\frac{\\mu^{(k)}_{ig}}{\\gamma_{i}}\\right) + \\frac{Y^{(k)}_{ig} - \\mu^{(k)}_{ig}}{\\mu^{(k)}_{ig}}\\) and \\(\\phi^{(k)}_{g}\\) is the vector of \\((\\phi^{(k)}_{ig})_{1 \\leq i \\leq n_k}\\). Each center Server 1 local_hat_matrix nparray \\((G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the hat matrix  \\(H^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(\\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene for sample \\(i\\) for parameter \\(\\beta\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 local_nll nparray \\((G_{\\texttt{act}},)\\) For each gene \\(g\\), the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data. Each center Server 1 beta nparray \\((G_{\\texttt{nz}}, p)\\) The log fold change \\(\\beta\\) parameter. Not modified. Each center Server 2 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 2 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 2 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 2 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 2 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/","title":"Workflow graph","text":"<p>The workflow graph below illustrates the sequence of operations in the design matrix construction process. It shows how data flows between local centers and the aggregation server during the <code>run_fed_PQN</code> function.</p> <p></p> <p>For a detailed breakdown of the shared states and their contents at each step, please refer to the table below.</p>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#api","title":"API","text":"<p>Necessary mixin and utils for prox newton method.</p>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.fed_PQN","title":"<code>fed_PQN</code>","text":""},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.fed_PQN.FedProxQuasiNewton","title":"<code>FedProxQuasiNewton</code>","text":"<p>               Bases: <code>LocMakeFedPQNFisherGradientNLL</code>, <code>AggChooseStepComputeAscentDirection</code></p> <p>Mixin class to implement a Prox Newton method for box constraints.</p> <p>It implements the method presented here: https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf More context can be found here https://optml.mit.edu/papers/sksChap.pdf</p> <p>Methods:</p> Name Description <code>run_fed_PQN</code> <p>The method to run the Prox Quasi Newton algorithm. It relies on the methods inherited from the LocMakeFedPQNFisherGradientNLL and AggChooseStepComputeAscentDirection classes.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/fed_PQN.py</code> <pre><code>class FedProxQuasiNewton(\n    LocMakeFedPQNFisherGradientNLL, AggChooseStepComputeAscentDirection\n):\n    \"\"\"Mixin class to implement a Prox Newton method for box constraints.\n\n    It implements the method presented here:\n    https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf\n    More context can be found here\n    https://optml.mit.edu/papers/sksChap.pdf\n\n    Methods\n    -------\n    run_fed_PQN\n        The method to run the Prox Quasi Newton algorithm.\n        It relies on the methods inherited from the LocMakeFedPQNFisherGradientNLL and\n        AggChooseStepComputeAscentDirection classes.\n    \"\"\"\n\n    PQN_num_iters: int\n\n    @log_organisation_method\n    def run_fed_PQN(\n        self,\n        train_data_nodes,\n        aggregation_node,\n        local_states,\n        PQN_shared_state,\n        first_iteration_mode: Literal[\"irls_catch\"] | None,\n        round_idx,\n        clean_models,\n        refit_mode: bool = False,\n    ):\n        \"\"\"Run the Prox Quasi Newton  algorithm.\n\n        Parameters\n        ----------\n        train_data_nodes: list\n            List of TrainDataNode.\n\n        aggregation_node: AggregationNode\n            The aggregation node.\n\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        PQN_shared_state: dict\n            The input shared state.\n            The requirements for this shared state are defined in the\n            LocMakeFedPQNFisherGradientNLL class and depend on the\n            first_iteration_mode.\n\n        first_iteration_mode: Optional[Literal[\"irls_catch\"]]\n            The first iteration mode.\n            This defines the input requirements for the algorithm, and is passed\n            to the make_local_fisher_gradient_nll method at the first iteration.\n\n        round_idx: int\n            The current round.\n\n        clean_models: bool\n            If True, the models are cleaned.\n\n        refit_mode: bool\n            Whether to run on `refit_adata`s instead of `local_adata`s.\n            (default: False).\n\n        Returns\n        -------\n        local_states: dict\n            Local states. Required to propagate intermediate results.\n\n        irls_final_shared_states: dict\n            Shared states containing the final IRLS results.\n            It contains nothing for now.\n\n        round_idx: int\n            The updated round index.\n        \"\"\"\n        #### ---- Main training loop ---- #####\n\n        start_loop()\n        for pqn_iter in range(self.PQN_num_iters + 1):\n            start_iteration(pqn_iter)\n            # ---- Compute local IRLS summands and nlls ---- #\n\n            (\n                local_states,\n                local_fisher_gradient_nlls_shared_states,\n                round_idx,\n            ) = local_step(\n                local_method=self.make_local_fisher_gradient_nll,\n                method_params={\n                    \"first_iteration_mode\": (\n                        first_iteration_mode if pqn_iter == 0 else None\n                    ),\n                    \"refit_mode\": refit_mode,\n                },\n                train_data_nodes=train_data_nodes,\n                output_local_states=local_states,\n                round_idx=round_idx,\n                input_local_states=local_states,\n                input_shared_state=PQN_shared_state,\n                aggregation_id=aggregation_node.organization_id,\n                description=\"Compute local Prox Newton summands and nlls.\",\n                clean_models=clean_models,\n            )\n\n            # ---- Compute global IRLS update and nlls ---- #\n\n            PQN_shared_state, round_idx = aggregation_step(\n                aggregation_method=self.choose_step_and_compute_ascent_direction,\n                train_data_nodes=train_data_nodes,\n                aggregation_node=aggregation_node,\n                input_shared_states=local_fisher_gradient_nlls_shared_states,\n                round_idx=round_idx,\n                description=\"Update the log fold changes and nlls in IRLS.\",\n                clean_models=clean_models,\n            )\n            end_iteration()\n        end_loop()\n\n        #### ---- End of training ---- ####\n\n        return local_states, PQN_shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.fed_PQN.FedProxQuasiNewton.run_fed_PQN","title":"<code>run_fed_PQN(train_data_nodes, aggregation_node, local_states, PQN_shared_state, first_iteration_mode, round_idx, clean_models, refit_mode=False)</code>","text":"<p>Run the Prox Quasi Newton  algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>train_data_nodes</code> <p>List of TrainDataNode.</p> required <code>aggregation_node</code> <p>The aggregation node.</p> required <code>local_states</code> <p>Local states. Required to propagate intermediate results.</p> required <code>PQN_shared_state</code> <p>The input shared state. The requirements for this shared state are defined in the LocMakeFedPQNFisherGradientNLL class and depend on the first_iteration_mode.</p> required <code>first_iteration_mode</code> <code>Literal['irls_catch'] | None</code> <p>The first iteration mode. This defines the input requirements for the algorithm, and is passed to the make_local_fisher_gradient_nll method at the first iteration.</p> required <code>round_idx</code> <p>The current round.</p> required <code>clean_models</code> <p>If True, the models are cleaned.</p> required <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>local_states</code> <code>dict</code> <p>Local states. Required to propagate intermediate results.</p> <code>irls_final_shared_states</code> <code>dict</code> <p>Shared states containing the final IRLS results. It contains nothing for now.</p> <code>round_idx</code> <code>int</code> <p>The updated round index.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/fed_PQN.py</code> <pre><code>@log_organisation_method\ndef run_fed_PQN(\n    self,\n    train_data_nodes,\n    aggregation_node,\n    local_states,\n    PQN_shared_state,\n    first_iteration_mode: Literal[\"irls_catch\"] | None,\n    round_idx,\n    clean_models,\n    refit_mode: bool = False,\n):\n    \"\"\"Run the Prox Quasi Newton  algorithm.\n\n    Parameters\n    ----------\n    train_data_nodes: list\n        List of TrainDataNode.\n\n    aggregation_node: AggregationNode\n        The aggregation node.\n\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    PQN_shared_state: dict\n        The input shared state.\n        The requirements for this shared state are defined in the\n        LocMakeFedPQNFisherGradientNLL class and depend on the\n        first_iteration_mode.\n\n    first_iteration_mode: Optional[Literal[\"irls_catch\"]]\n        The first iteration mode.\n        This defines the input requirements for the algorithm, and is passed\n        to the make_local_fisher_gradient_nll method at the first iteration.\n\n    round_idx: int\n        The current round.\n\n    clean_models: bool\n        If True, the models are cleaned.\n\n    refit_mode: bool\n        Whether to run on `refit_adata`s instead of `local_adata`s.\n        (default: False).\n\n    Returns\n    -------\n    local_states: dict\n        Local states. Required to propagate intermediate results.\n\n    irls_final_shared_states: dict\n        Shared states containing the final IRLS results.\n        It contains nothing for now.\n\n    round_idx: int\n        The updated round index.\n    \"\"\"\n    #### ---- Main training loop ---- #####\n\n    start_loop()\n    for pqn_iter in range(self.PQN_num_iters + 1):\n        start_iteration(pqn_iter)\n        # ---- Compute local IRLS summands and nlls ---- #\n\n        (\n            local_states,\n            local_fisher_gradient_nlls_shared_states,\n            round_idx,\n        ) = local_step(\n            local_method=self.make_local_fisher_gradient_nll,\n            method_params={\n                \"first_iteration_mode\": (\n                    first_iteration_mode if pqn_iter == 0 else None\n                ),\n                \"refit_mode\": refit_mode,\n            },\n            train_data_nodes=train_data_nodes,\n            output_local_states=local_states,\n            round_idx=round_idx,\n            input_local_states=local_states,\n            input_shared_state=PQN_shared_state,\n            aggregation_id=aggregation_node.organization_id,\n            description=\"Compute local Prox Newton summands and nlls.\",\n            clean_models=clean_models,\n        )\n\n        # ---- Compute global IRLS update and nlls ---- #\n\n        PQN_shared_state, round_idx = aggregation_step(\n            aggregation_method=self.choose_step_and_compute_ascent_direction,\n            train_data_nodes=train_data_nodes,\n            aggregation_node=aggregation_node,\n            input_shared_states=local_fisher_gradient_nlls_shared_states,\n            round_idx=round_idx,\n            description=\"Update the log fold changes and nlls in IRLS.\",\n            clean_models=clean_models,\n        )\n        end_iteration()\n    end_loop()\n\n    #### ---- End of training ---- ####\n\n    return local_states, PQN_shared_state, round_idx\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.substeps","title":"<code>substeps</code>","text":""},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.substeps.AggChooseStepComputeAscentDirection","title":"<code>AggChooseStepComputeAscentDirection</code>","text":"<p>Mixin class to compute the right ascent direction.</p> <p>An ascent direction is a direction that is positively correlated to the gradient. This direction will be used to compute the next iterate in the proximal quasi newton algorithm. As our aim will be to mimimize the negative log likelihood, we will move in the opposite direction, that is in the direction of minus the ascent direction.</p> <p>Attributes:</p> Name Type Description <code>num_jobs</code> <code>int</code> <p>The number of cpus to use.</p> <code>joblib_verbosity</code> <code>int</code> <p>The joblib verbosity.</p> <code>joblib_backend</code> <code>str</code> <p>The backend to use for the IRLS algorithm.</p> <code>irls_batch_size</code> <code>int</code> <p>The batch size to use for the IRLS algorithm.</p> <code>max_beta</code> <code>float</code> <p>The maximum value for the beta parameter.</p> <code>beta_tol</code> <code>float</code> <p>The tolerance for the beta parameter.</p> <code>PQN_num_iters_ls</code> <code>int</code> <p>The number of iterations to use for the line search.</p> <code>PQN_c1</code> <code>float</code> <p>The c1 parameter for the line search.</p> <code>PQN_ftol</code> <code>float</code> <p>The ftol parameter for the line search.</p> <code>PQN_num_iters</code> <code>int</code> <p>The number of iterations to use for the proximal quasi newton algorithm.</p> <p>Methods:</p> Name Description <code>choose_step_and_compute_ascent_direction</code> <p>A remote method. Choose the best step size and compute the next ascent direction.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/substeps.py</code> <pre><code>class AggChooseStepComputeAscentDirection:\n    \"\"\"Mixin class to compute the right ascent direction.\n\n    An ascent direction is a direction that is positively correlated to the gradient.\n    This direction will be used to compute the next iterate in the proximal quasi newton\n    algorithm. As our aim will be to mimimize the negative log likelihood, we will\n    move in the opposite direction, that is in the direction of minus the\n    ascent direction.\n\n    Attributes\n    ----------\n    num_jobs : int\n        The number of cpus to use.\n    joblib_verbosity : int\n        The joblib verbosity.\n    joblib_backend : str\n        The backend to use for the IRLS algorithm.\n    irls_batch_size : int\n        The batch size to use for the IRLS algorithm.\n    max_beta : float\n        The maximum value for the beta parameter.\n    beta_tol : float\n        The tolerance for the beta parameter.\n    PQN_num_iters_ls : int\n        The number of iterations to use for the line search.\n    PQN_c1 : float\n        The c1 parameter for the line search.\n    PQN_ftol : float\n        The ftol parameter for the line search.\n    PQN_num_iters : int\n        The number of iterations to use for the proximal quasi newton algorithm.\n\n    Methods\n    -------\n    choose_step_and_compute_ascent_direction\n        A remote method.\n        Choose the best step size and compute the next ascent direction.\n    \"\"\"\n\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n    max_beta: float\n    beta_tol: float\n    PQN_num_iters_ls: int\n    PQN_c1: float\n    PQN_ftol: float\n    PQN_num_iters: int\n\n    @remote\n    @log_remote\n    def choose_step_and_compute_ascent_direction(\n        self, shared_states: list[dict]\n    ) -&gt; dict[str, Any]:\n        \"\"\"Choose best step size and compute next ascent direction.\n\n        By \"ascent direction\", we mean the direction that is positively correlated\n        with the gradient.\n\n        The role of this function is twofold.\n\n        1) It chooses the best step size for each gene, and updates the beta values\n        as well as the nll values. This allows to define the next iterate.\n        Note that at the first iterate, it simply computes the nll, gradient and fisher\n        information at the current beta values, to define the next ascent direction.\n\n        2) For this new iterate (or the current one if we are at the first round),\n        it computes the gradient scaling matrix, which is used to scale the gradient\n        in the proximal newton algorithm. From this gradient scaling matrix, and the\n        gradient, it computes the ascent direction (and the newton decrement).\n\n\n        Parameters\n        ----------\n        shared_states: list[dict]\n            A list of dictionaries containing the following\n            keys:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - local_nll: ndarray\n                The local nll, of shape (n_genes,), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask).\n            - local_fisher: ndarray\n                The local fisher matrix, of shape (n_genes, n_params, n_params).\n            - local_gradient: ndarray\n                The local gradient, of shape (n_genes, n_params).\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the prox newton\n                algorithm, of shape (n_non_zero_genes,).\n            - PQN_mask: ndarray\n                A boolean mask indicating if the gene should be used for the\n                proximal newton step, of shape (n_non_zero_genes,).\n            - global_reg_nll: ndarray\n                The global regularized nll, of shape (n_non_zero_genes,).\n            - newton_decrement_on_mask: Optional[ndarray]\n                The newton decrement, of shape (n_ngenes,).\n                This is None at the first round of the prox newton algorithm.\n            - round_number_PQN: int\n                The current round number of the prox newton algorithm.\n            - ascent_direction_on_mask: Optional[ndarray]\n                The ascent direction, of shape (n_genes, n_params), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n\n        Returns\n        -------\n        dict[str, Any]\n            A dictionary containing all the necessary info to run the method.\n            If we are not at the last iteration, it contains the following fields:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - PQN_mask: ndarray\n                A boolean mask indicating if the gene should be used for the\n                proximal newton step.\n                It is of shape (n_non_zero_genes,)\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the prox newton\n                algorithm. It is of shape (n_non_zero_genes,)\n            - ascent_direction_on_mask: np.ndarray\n                The ascent direction, of shape (n_genes, n_params), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask).\n            - newton_decrement_on_mask: np.ndarray\n                The newton decrement, of shape (n_ngenes,).\n            - round_number_PQN: int\n                The current round number of the prox newton algorithm.\n            - global_reg_nll: ndarray\n                The global regularized nll, of shape (n_non_zero_genes,).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n            If we are at the last iteration, it contains the following fields:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the prox newton\n                algorithm. It is of shape (n_non_zero_genes,)\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n        \"\"\"\n        # We use the following naming convention: when we say \"on mask\", we mean\n        # that we restrict the quantity to the genes that are active in the proximal\n        # newton\n        # algorithm. We therefore need to ensure that these quantities are readjusted\n        # when we change the proximal quasi newton mask.\n\n        # Load main params from the first state\n        beta = shared_states[0][\"beta\"]\n        PQN_diverged_mask = shared_states[0][\"PQN_diverged_mask\"]\n        PQN_mask = shared_states[0][\"PQN_mask\"]\n        reg_nll = shared_states[0][\"global_reg_nll\"]\n        ascent_direction_on_mask = shared_states[0][\"ascent_direction_on_mask\"]\n        newton_decrement_on_mask = shared_states[0][\"newton_decrement_on_mask\"]\n        round_number_PQN = shared_states[0][\"round_number_PQN\"]\n\n        reg_parameter = 1e-6\n\n        # ---- Step 0: Aggregate the nll, gradient and fisher info ---- #\n\n        new_fisher_options_on_mask = sum(\n            [state[\"local_fisher\"] for state in shared_states]\n        )\n\n        new_gradient_options_on_mask = sum(\n            [state[\"local_gradient\"] for state in shared_states]\n        )\n        new_reg_nll_options_on_mask = sum(\n            [state[\"local_nll\"] for state in shared_states]\n        )\n\n        # ---- Step 1: Add the regularization term ---- #\n\n        # ---- Step 1a: Compute the new beta options ---- #\n\n        # In order to regularize, we have to compute the beta values at which\n        # the nll, gradient and fisher informations were evaluated in the local steps.\n\n        beta_on_mask = beta[PQN_mask]\n\n        if round_number_PQN == 0:\n            # In this case, there is no line search, and only\n            # beta is considered in the local steps.\n            new_beta_options_on_mask = beta_on_mask[None, :]\n\n        else:\n            # In this case, there is a line search, and we have to\n            # compute the new beta options\n            assert ascent_direction_on_mask is not None\n            step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n            new_beta_options_on_mask = np.clip(\n                beta_on_mask[None, :, :]\n                - step_sizes[:, None, None] * ascent_direction_on_mask[None, :, :],\n                -self.max_beta,\n                self.max_beta,\n            )\n\n        # ---- Step 1b: Add the regularization ---- #\n\n        # Add a regularization term to fisher info\n\n        if new_fisher_options_on_mask is not None:\n            # Add the regularization term to construct the Fisher info with prior\n            # from the Fisher info without prior\n            cross_term = (\n                new_gradient_options_on_mask[:, :, :, None]\n                @ new_beta_options_on_mask[:, :, None, :]\n            )\n            beta_term = (\n                new_beta_options_on_mask[:, :, :, None]\n                @ new_beta_options_on_mask[:, :, None, :]\n            )\n            new_fisher_options_on_mask += (\n                reg_parameter * (cross_term + cross_term.transpose(0, 1, 3, 2))\n                + reg_parameter**2 * beta_term\n            )\n\n            # Furthermore, add a ridge term to the Fisher info for numerical stability\n            # This factor decreases log linearly between and initial and final reg\n            # The decreasing factor is to ensure that the first steps correspond to\n            # gradient descent steps, as we are too far from the optimum\n            # to use the Fisher info.\n            # Note that other schemes seem to work as well: 1 for 20 iterations then\n            # 1e-6\n            # 1 for 20 iterations then 1e-2 (to confirm), or 1 for 20 iterations and\n            # then\n            # 1/n_samples.\n            initial_reg_fisher = 1\n            final_reg_fisher = 1e-6\n            reg_fisher = initial_reg_fisher * (\n                final_reg_fisher / initial_reg_fisher\n            ) ** (round_number_PQN / self.PQN_num_iters)\n\n            new_fisher_options_on_mask = (\n                new_fisher_options_on_mask\n                + np.diag(np.repeat(reg_fisher, new_fisher_options_on_mask.shape[-1]))[\n                    None, None, :, :\n                ]\n            )\n\n        # Add regularization term to gradient\n        new_gradient_options_on_mask += reg_parameter * new_beta_options_on_mask\n\n        # Add regularization term to the nll\n        new_reg_nll_options_on_mask += (\n            0.5 * reg_parameter * np.sum(new_beta_options_on_mask**2, axis=2)\n        )\n\n        # ---- Step 2: Compute best step size, and new values for this step size ---- #\n\n        # This is only done if we are not at the first round of the prox newton\n        # algorithm, as the first rounds serves only to evaluate the nll, gradient\n        # and fisher info at the current beta values, and compute the first\n        # ascent direction.\n\n        if round_number_PQN &gt; 0:\n            # ---- Step 2a: See which step sizes pass the selection criteria ---- #\n\n            assert reg_nll is not None\n            reg_nll_on_mask = reg_nll[PQN_mask]\n\n            obj_diff_options_on_mask = (\n                reg_nll_on_mask[None, :] - new_reg_nll_options_on_mask\n            )  # of shape n_steps, n_PQN_genes\n\n            step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n\n            # Condition 1: Armijo condition\n            # This condition is also called the first Wolfe condition.\n            # Reference https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n            admissible_step_size_options_mask = (\n                obj_diff_options_on_mask\n                &gt;= self.PQN_c1 * step_sizes[:, None] * newton_decrement_on_mask[None, :]\n            )\n\n            # ---- Step 2b: Identify genes that have diverged, and remove them ---- #\n\n            # For each gene, we check if there is at least one step size that satisfies\n            # the selection criteria. If there is none, we consider that the gene has\n            # diverged: we remove all such genes from the PQN_mask\n            # and add them to the PQN_diverged_mask.\n\n            diverged_gene_mask_in_current_PQN_mask = np.all(\n                ~admissible_step_size_options_mask, axis=0\n            )\n\n            # Remove these diverged genes for which we cannot find\n            # a correct step size\n\n            PQN_diverged_mask[PQN_mask] = diverged_gene_mask_in_current_PQN_mask\n            PQN_mask[PQN_mask] = ~diverged_gene_mask_in_current_PQN_mask\n\n            # Restrict all the quantities defined on the prox newton\n            # mask to the new prox newton mask\n\n            obj_diff_options_on_mask = obj_diff_options_on_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask\n            ]\n            reg_nll_on_mask = reg_nll_on_mask[~diverged_gene_mask_in_current_PQN_mask]\n            beta_on_mask = beta_on_mask[~diverged_gene_mask_in_current_PQN_mask]\n            admissible_step_size_options_mask = admissible_step_size_options_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask\n            ]\n            new_reg_nll_options_on_mask = new_reg_nll_options_on_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask\n            ]\n            new_gradient_options_on_mask = new_gradient_options_on_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask, :\n            ]\n            new_beta_options_on_mask = new_beta_options_on_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask, :\n            ]\n\n            new_fisher_options_on_mask = new_fisher_options_on_mask[\n                :, ~diverged_gene_mask_in_current_PQN_mask, :, :\n            ]\n\n            # ---- Step 2c: Find the best step size for each gene ---- #\n\n            # Here, we find the best step size for each gene that satisfies the\n            # selection criteria (i.e. the largest).\n            # We do this by finding the first index for which\n            # the admissible step size mask is True.\n            # We then create the new beta, gradient, fisher info and reg nll by\n            # taking the option corresponding to the best step size\n\n            new_step_size_index = np.argmax(admissible_step_size_options_mask, axis=0)\n            arange_PQN = np.arange(len(new_step_size_index))\n\n            new_beta_on_mask = new_beta_options_on_mask[new_step_size_index, arange_PQN]\n            new_gradient_on_mask = new_gradient_options_on_mask[\n                new_step_size_index, arange_PQN\n            ]\n            new_fisher_on_mask = new_fisher_options_on_mask[\n                new_step_size_index, arange_PQN\n            ]\n\n            obj_diff_on_mask = obj_diff_options_on_mask[new_step_size_index, arange_PQN]\n\n            new_reg_nll_on_mask = new_reg_nll_options_on_mask[\n                new_step_size_index, arange_PQN\n            ]\n\n            # ---- Step 2d: Update the beta values and the reg_nll values ---- #\n\n            beta[PQN_mask] = new_beta_on_mask\n            reg_nll[PQN_mask] = new_reg_nll_on_mask\n\n            # ---- Step 2e: Check for convergence of the method ---- #\n\n            convergence_mask = (\n                np.abs(obj_diff_on_mask)\n                / (\n                    np.maximum(\n                        np.maximum(\n                            np.abs(new_reg_nll_on_mask),\n                            np.abs(reg_nll_on_mask),\n                        ),\n                        1,\n                    )\n                )\n                &lt; self.PQN_ftol\n            )\n\n            # ---- Step 2f: Remove converged genes from the mask ---- #\n            PQN_mask[PQN_mask] = ~convergence_mask\n\n            # If we reach the max number of iterations, we stop\n            if round_number_PQN == self.PQN_num_iters:\n                # In this case, we are finished.\n                return {\n                    \"beta\": beta,\n                    \"PQN_diverged_mask\": PQN_diverged_mask | PQN_mask,\n                    \"irls_diverged_mask\": shared_states[0][\"irls_diverged_mask\"],\n                }\n\n            # We restrict all quantities to the new mask\n\n            new_gradient_on_mask = new_gradient_on_mask[~convergence_mask]\n            new_beta_on_mask = new_beta_on_mask[~convergence_mask]\n            new_fisher_on_mask = new_fisher_on_mask[~convergence_mask]\n\n            # Note, this is the old beta\n            beta_on_mask = beta_on_mask[~convergence_mask]\n\n        else:\n            # In this case, we are at the first round of the prox newton algorithm\n            # In this case, we simply instantiate the new values to the first\n            # values that were computed in the local steps, to be able to compute\n            # the first ascent direction.\n            beta_on_mask = None\n            new_gradient_on_mask = new_gradient_options_on_mask[0]\n            new_beta_on_mask = new_beta_options_on_mask[0]\n            new_fisher_on_mask = new_fisher_options_on_mask[0]\n\n            # Set the nll\n            reg_nll[PQN_mask] = new_reg_nll_options_on_mask[0]\n\n        # ---- Step 3: Compute the gradient scaling matrix ---- #\n\n        gradient_scaling_matrix_on_mask = compute_gradient_scaling_matrix_fisher(\n            fisher=new_fisher_on_mask,\n            backend=self.joblib_backend,\n            num_jobs=self.num_jobs,\n            joblib_verbosity=self.joblib_verbosity,\n            batch_size=self.irls_batch_size,\n        )\n\n        # ---- Step 4: Compute the ascent direction and the newton decrement ---- #\n\n        (\n            ascent_direction_on_mask,\n            newton_decrement_on_mask,\n        ) = compute_ascent_direction_decrement(\n            gradient_scaling_matrix=gradient_scaling_matrix_on_mask,\n            gradient=new_gradient_on_mask,\n            beta=new_beta_on_mask,\n            max_beta=self.max_beta,\n        )\n\n        round_number_PQN += 1\n\n        return {\n            \"beta\": beta,\n            \"PQN_mask\": PQN_mask,\n            \"PQN_diverged_mask\": PQN_diverged_mask,\n            \"ascent_direction_on_mask\": ascent_direction_on_mask,\n            \"newton_decrement_on_mask\": newton_decrement_on_mask,\n            \"round_number_PQN\": round_number_PQN,\n            \"global_reg_nll\": reg_nll,\n            \"irls_diverged_mask\": shared_states[0][\"irls_diverged_mask\"],\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.substeps.AggChooseStepComputeAscentDirection.choose_step_and_compute_ascent_direction","title":"<code>choose_step_and_compute_ascent_direction(shared_states)</code>","text":"<p>Choose best step size and compute next ascent direction.</p> <p>By \"ascent direction\", we mean the direction that is positively correlated with the gradient.</p> <p>The role of this function is twofold.</p> <p>1) It chooses the best step size for each gene, and updates the beta values as well as the nll values. This allows to define the next iterate. Note that at the first iterate, it simply computes the nll, gradient and fisher information at the current beta values, to define the next ascent direction.</p> <p>2) For this new iterate (or the current one if we are at the first round), it computes the gradient scaling matrix, which is used to scale the gradient in the proximal newton algorithm. From this gradient scaling matrix, and the gradient, it computes the ascent direction (and the newton decrement).</p> <p>Parameters:</p> Name Type Description Default <code>shared_states</code> <code>list[dict]</code> <p>A list of dictionaries containing the following keys: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - local_nll: ndarray     The local nll, of shape (n_genes,), where     n_genes is the current number of genes that are active (True     in the PQN_mask). - local_fisher: ndarray     The local fisher matrix, of shape (n_genes, n_params, n_params). - local_gradient: ndarray     The local gradient, of shape (n_genes, n_params). - PQN_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the prox newton     algorithm, of shape (n_non_zero_genes,). - PQN_mask: ndarray     A boolean mask indicating if the gene should be used for the     proximal newton step, of shape (n_non_zero_genes,). - global_reg_nll: ndarray     The global regularized nll, of shape (n_non_zero_genes,). - newton_decrement_on_mask: Optional[ndarray]     The newton decrement, of shape (n_ngenes,).     This is None at the first round of the prox newton algorithm. - round_number_PQN: int     The current round number of the prox newton algorithm. - ascent_direction_on_mask: Optional[ndarray]     The ascent direction, of shape (n_genes, n_params), where     n_genes is the current number of genes that are active (True     in the PQN_mask). - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing all the necessary info to run the method. If we are not at the last iteration, it contains the following fields: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - PQN_mask: ndarray     A boolean mask indicating if the gene should be used for the     proximal newton step.     It is of shape (n_non_zero_genes,) - PQN_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the prox newton     algorithm. It is of shape (n_non_zero_genes,) - ascent_direction_on_mask: np.ndarray     The ascent direction, of shape (n_genes, n_params), where     n_genes is the current number of genes that are active (True     in the PQN_mask). - newton_decrement_on_mask: np.ndarray     The newton decrement, of shape (n_ngenes,). - round_number_PQN: int     The current round number of the prox newton algorithm. - global_reg_nll: ndarray     The global regularized nll, of shape (n_non_zero_genes,). - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm. If we are at the last iteration, it contains the following fields: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - PQN_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the prox newton     algorithm. It is of shape (n_non_zero_genes,) - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/substeps.py</code> <pre><code>@remote\n@log_remote\ndef choose_step_and_compute_ascent_direction(\n    self, shared_states: list[dict]\n) -&gt; dict[str, Any]:\n    \"\"\"Choose best step size and compute next ascent direction.\n\n    By \"ascent direction\", we mean the direction that is positively correlated\n    with the gradient.\n\n    The role of this function is twofold.\n\n    1) It chooses the best step size for each gene, and updates the beta values\n    as well as the nll values. This allows to define the next iterate.\n    Note that at the first iterate, it simply computes the nll, gradient and fisher\n    information at the current beta values, to define the next ascent direction.\n\n    2) For this new iterate (or the current one if we are at the first round),\n    it computes the gradient scaling matrix, which is used to scale the gradient\n    in the proximal newton algorithm. From this gradient scaling matrix, and the\n    gradient, it computes the ascent direction (and the newton decrement).\n\n\n    Parameters\n    ----------\n    shared_states: list[dict]\n        A list of dictionaries containing the following\n        keys:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - local_nll: ndarray\n            The local nll, of shape (n_genes,), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask).\n        - local_fisher: ndarray\n            The local fisher matrix, of shape (n_genes, n_params, n_params).\n        - local_gradient: ndarray\n            The local gradient, of shape (n_genes, n_params).\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the prox newton\n            algorithm, of shape (n_non_zero_genes,).\n        - PQN_mask: ndarray\n            A boolean mask indicating if the gene should be used for the\n            proximal newton step, of shape (n_non_zero_genes,).\n        - global_reg_nll: ndarray\n            The global regularized nll, of shape (n_non_zero_genes,).\n        - newton_decrement_on_mask: Optional[ndarray]\n            The newton decrement, of shape (n_ngenes,).\n            This is None at the first round of the prox newton algorithm.\n        - round_number_PQN: int\n            The current round number of the prox newton algorithm.\n        - ascent_direction_on_mask: Optional[ndarray]\n            The ascent direction, of shape (n_genes, n_params), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n\n    Returns\n    -------\n    dict[str, Any]\n        A dictionary containing all the necessary info to run the method.\n        If we are not at the last iteration, it contains the following fields:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - PQN_mask: ndarray\n            A boolean mask indicating if the gene should be used for the\n            proximal newton step.\n            It is of shape (n_non_zero_genes,)\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the prox newton\n            algorithm. It is of shape (n_non_zero_genes,)\n        - ascent_direction_on_mask: np.ndarray\n            The ascent direction, of shape (n_genes, n_params), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask).\n        - newton_decrement_on_mask: np.ndarray\n            The newton decrement, of shape (n_ngenes,).\n        - round_number_PQN: int\n            The current round number of the prox newton algorithm.\n        - global_reg_nll: ndarray\n            The global regularized nll, of shape (n_non_zero_genes,).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n        If we are at the last iteration, it contains the following fields:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the prox newton\n            algorithm. It is of shape (n_non_zero_genes,)\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n    \"\"\"\n    # We use the following naming convention: when we say \"on mask\", we mean\n    # that we restrict the quantity to the genes that are active in the proximal\n    # newton\n    # algorithm. We therefore need to ensure that these quantities are readjusted\n    # when we change the proximal quasi newton mask.\n\n    # Load main params from the first state\n    beta = shared_states[0][\"beta\"]\n    PQN_diverged_mask = shared_states[0][\"PQN_diverged_mask\"]\n    PQN_mask = shared_states[0][\"PQN_mask\"]\n    reg_nll = shared_states[0][\"global_reg_nll\"]\n    ascent_direction_on_mask = shared_states[0][\"ascent_direction_on_mask\"]\n    newton_decrement_on_mask = shared_states[0][\"newton_decrement_on_mask\"]\n    round_number_PQN = shared_states[0][\"round_number_PQN\"]\n\n    reg_parameter = 1e-6\n\n    # ---- Step 0: Aggregate the nll, gradient and fisher info ---- #\n\n    new_fisher_options_on_mask = sum(\n        [state[\"local_fisher\"] for state in shared_states]\n    )\n\n    new_gradient_options_on_mask = sum(\n        [state[\"local_gradient\"] for state in shared_states]\n    )\n    new_reg_nll_options_on_mask = sum(\n        [state[\"local_nll\"] for state in shared_states]\n    )\n\n    # ---- Step 1: Add the regularization term ---- #\n\n    # ---- Step 1a: Compute the new beta options ---- #\n\n    # In order to regularize, we have to compute the beta values at which\n    # the nll, gradient and fisher informations were evaluated in the local steps.\n\n    beta_on_mask = beta[PQN_mask]\n\n    if round_number_PQN == 0:\n        # In this case, there is no line search, and only\n        # beta is considered in the local steps.\n        new_beta_options_on_mask = beta_on_mask[None, :]\n\n    else:\n        # In this case, there is a line search, and we have to\n        # compute the new beta options\n        assert ascent_direction_on_mask is not None\n        step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n        new_beta_options_on_mask = np.clip(\n            beta_on_mask[None, :, :]\n            - step_sizes[:, None, None] * ascent_direction_on_mask[None, :, :],\n            -self.max_beta,\n            self.max_beta,\n        )\n\n    # ---- Step 1b: Add the regularization ---- #\n\n    # Add a regularization term to fisher info\n\n    if new_fisher_options_on_mask is not None:\n        # Add the regularization term to construct the Fisher info with prior\n        # from the Fisher info without prior\n        cross_term = (\n            new_gradient_options_on_mask[:, :, :, None]\n            @ new_beta_options_on_mask[:, :, None, :]\n        )\n        beta_term = (\n            new_beta_options_on_mask[:, :, :, None]\n            @ new_beta_options_on_mask[:, :, None, :]\n        )\n        new_fisher_options_on_mask += (\n            reg_parameter * (cross_term + cross_term.transpose(0, 1, 3, 2))\n            + reg_parameter**2 * beta_term\n        )\n\n        # Furthermore, add a ridge term to the Fisher info for numerical stability\n        # This factor decreases log linearly between and initial and final reg\n        # The decreasing factor is to ensure that the first steps correspond to\n        # gradient descent steps, as we are too far from the optimum\n        # to use the Fisher info.\n        # Note that other schemes seem to work as well: 1 for 20 iterations then\n        # 1e-6\n        # 1 for 20 iterations then 1e-2 (to confirm), or 1 for 20 iterations and\n        # then\n        # 1/n_samples.\n        initial_reg_fisher = 1\n        final_reg_fisher = 1e-6\n        reg_fisher = initial_reg_fisher * (\n            final_reg_fisher / initial_reg_fisher\n        ) ** (round_number_PQN / self.PQN_num_iters)\n\n        new_fisher_options_on_mask = (\n            new_fisher_options_on_mask\n            + np.diag(np.repeat(reg_fisher, new_fisher_options_on_mask.shape[-1]))[\n                None, None, :, :\n            ]\n        )\n\n    # Add regularization term to gradient\n    new_gradient_options_on_mask += reg_parameter * new_beta_options_on_mask\n\n    # Add regularization term to the nll\n    new_reg_nll_options_on_mask += (\n        0.5 * reg_parameter * np.sum(new_beta_options_on_mask**2, axis=2)\n    )\n\n    # ---- Step 2: Compute best step size, and new values for this step size ---- #\n\n    # This is only done if we are not at the first round of the prox newton\n    # algorithm, as the first rounds serves only to evaluate the nll, gradient\n    # and fisher info at the current beta values, and compute the first\n    # ascent direction.\n\n    if round_number_PQN &gt; 0:\n        # ---- Step 2a: See which step sizes pass the selection criteria ---- #\n\n        assert reg_nll is not None\n        reg_nll_on_mask = reg_nll[PQN_mask]\n\n        obj_diff_options_on_mask = (\n            reg_nll_on_mask[None, :] - new_reg_nll_options_on_mask\n        )  # of shape n_steps, n_PQN_genes\n\n        step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n\n        # Condition 1: Armijo condition\n        # This condition is also called the first Wolfe condition.\n        # Reference https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf\n        admissible_step_size_options_mask = (\n            obj_diff_options_on_mask\n            &gt;= self.PQN_c1 * step_sizes[:, None] * newton_decrement_on_mask[None, :]\n        )\n\n        # ---- Step 2b: Identify genes that have diverged, and remove them ---- #\n\n        # For each gene, we check if there is at least one step size that satisfies\n        # the selection criteria. If there is none, we consider that the gene has\n        # diverged: we remove all such genes from the PQN_mask\n        # and add them to the PQN_diverged_mask.\n\n        diverged_gene_mask_in_current_PQN_mask = np.all(\n            ~admissible_step_size_options_mask, axis=0\n        )\n\n        # Remove these diverged genes for which we cannot find\n        # a correct step size\n\n        PQN_diverged_mask[PQN_mask] = diverged_gene_mask_in_current_PQN_mask\n        PQN_mask[PQN_mask] = ~diverged_gene_mask_in_current_PQN_mask\n\n        # Restrict all the quantities defined on the prox newton\n        # mask to the new prox newton mask\n\n        obj_diff_options_on_mask = obj_diff_options_on_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask\n        ]\n        reg_nll_on_mask = reg_nll_on_mask[~diverged_gene_mask_in_current_PQN_mask]\n        beta_on_mask = beta_on_mask[~diverged_gene_mask_in_current_PQN_mask]\n        admissible_step_size_options_mask = admissible_step_size_options_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask\n        ]\n        new_reg_nll_options_on_mask = new_reg_nll_options_on_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask\n        ]\n        new_gradient_options_on_mask = new_gradient_options_on_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask, :\n        ]\n        new_beta_options_on_mask = new_beta_options_on_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask, :\n        ]\n\n        new_fisher_options_on_mask = new_fisher_options_on_mask[\n            :, ~diverged_gene_mask_in_current_PQN_mask, :, :\n        ]\n\n        # ---- Step 2c: Find the best step size for each gene ---- #\n\n        # Here, we find the best step size for each gene that satisfies the\n        # selection criteria (i.e. the largest).\n        # We do this by finding the first index for which\n        # the admissible step size mask is True.\n        # We then create the new beta, gradient, fisher info and reg nll by\n        # taking the option corresponding to the best step size\n\n        new_step_size_index = np.argmax(admissible_step_size_options_mask, axis=0)\n        arange_PQN = np.arange(len(new_step_size_index))\n\n        new_beta_on_mask = new_beta_options_on_mask[new_step_size_index, arange_PQN]\n        new_gradient_on_mask = new_gradient_options_on_mask[\n            new_step_size_index, arange_PQN\n        ]\n        new_fisher_on_mask = new_fisher_options_on_mask[\n            new_step_size_index, arange_PQN\n        ]\n\n        obj_diff_on_mask = obj_diff_options_on_mask[new_step_size_index, arange_PQN]\n\n        new_reg_nll_on_mask = new_reg_nll_options_on_mask[\n            new_step_size_index, arange_PQN\n        ]\n\n        # ---- Step 2d: Update the beta values and the reg_nll values ---- #\n\n        beta[PQN_mask] = new_beta_on_mask\n        reg_nll[PQN_mask] = new_reg_nll_on_mask\n\n        # ---- Step 2e: Check for convergence of the method ---- #\n\n        convergence_mask = (\n            np.abs(obj_diff_on_mask)\n            / (\n                np.maximum(\n                    np.maximum(\n                        np.abs(new_reg_nll_on_mask),\n                        np.abs(reg_nll_on_mask),\n                    ),\n                    1,\n                )\n            )\n            &lt; self.PQN_ftol\n        )\n\n        # ---- Step 2f: Remove converged genes from the mask ---- #\n        PQN_mask[PQN_mask] = ~convergence_mask\n\n        # If we reach the max number of iterations, we stop\n        if round_number_PQN == self.PQN_num_iters:\n            # In this case, we are finished.\n            return {\n                \"beta\": beta,\n                \"PQN_diverged_mask\": PQN_diverged_mask | PQN_mask,\n                \"irls_diverged_mask\": shared_states[0][\"irls_diverged_mask\"],\n            }\n\n        # We restrict all quantities to the new mask\n\n        new_gradient_on_mask = new_gradient_on_mask[~convergence_mask]\n        new_beta_on_mask = new_beta_on_mask[~convergence_mask]\n        new_fisher_on_mask = new_fisher_on_mask[~convergence_mask]\n\n        # Note, this is the old beta\n        beta_on_mask = beta_on_mask[~convergence_mask]\n\n    else:\n        # In this case, we are at the first round of the prox newton algorithm\n        # In this case, we simply instantiate the new values to the first\n        # values that were computed in the local steps, to be able to compute\n        # the first ascent direction.\n        beta_on_mask = None\n        new_gradient_on_mask = new_gradient_options_on_mask[0]\n        new_beta_on_mask = new_beta_options_on_mask[0]\n        new_fisher_on_mask = new_fisher_options_on_mask[0]\n\n        # Set the nll\n        reg_nll[PQN_mask] = new_reg_nll_options_on_mask[0]\n\n    # ---- Step 3: Compute the gradient scaling matrix ---- #\n\n    gradient_scaling_matrix_on_mask = compute_gradient_scaling_matrix_fisher(\n        fisher=new_fisher_on_mask,\n        backend=self.joblib_backend,\n        num_jobs=self.num_jobs,\n        joblib_verbosity=self.joblib_verbosity,\n        batch_size=self.irls_batch_size,\n    )\n\n    # ---- Step 4: Compute the ascent direction and the newton decrement ---- #\n\n    (\n        ascent_direction_on_mask,\n        newton_decrement_on_mask,\n    ) = compute_ascent_direction_decrement(\n        gradient_scaling_matrix=gradient_scaling_matrix_on_mask,\n        gradient=new_gradient_on_mask,\n        beta=new_beta_on_mask,\n        max_beta=self.max_beta,\n    )\n\n    round_number_PQN += 1\n\n    return {\n        \"beta\": beta,\n        \"PQN_mask\": PQN_mask,\n        \"PQN_diverged_mask\": PQN_diverged_mask,\n        \"ascent_direction_on_mask\": ascent_direction_on_mask,\n        \"newton_decrement_on_mask\": newton_decrement_on_mask,\n        \"round_number_PQN\": round_number_PQN,\n        \"global_reg_nll\": reg_nll,\n        \"irls_diverged_mask\": shared_states[0][\"irls_diverged_mask\"],\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.substeps.LocMakeFedPQNFisherGradientNLL","title":"<code>LocMakeFedPQNFisherGradientNLL</code>","text":"<p>Mixin to compute local values, gradient and Fisher information of the NLL.</p> <p>Attributes:</p> Name Type Description <code>local_adata</code> <code>AnnData</code> <p>The local AnnData.</p> <code>num_jobs</code> <code>int</code> <p>The number of cpus to use.</p> <code>joblib_verbosity</code> <code>int</code> <p>The joblib verbosity.</p> <code>joblib_backend</code> <code>str</code> <p>The backend to use for the IRLS algorithm.</p> <code>irls_batch_size</code> <code>int</code> <p>The batch size to use for the IRLS algorithm.</p> <code>max_beta</code> <code>float</code> <p>The maximum value for the beta parameter.</p> <code>PQN_num_iters_ls</code> <code>int</code> <p>The number of iterations to use for the line search.</p> <code>PQN_min_mu</code> <code>float</code> <p>The min_mu parameter for the Proximal Quasi Newton algorithm.</p> <p>Methods:</p> Name Description <code>make_local_fisher_gradient_nll</code> <p>A remote_data method. Make the local nll, gradient and fisher matrix.</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/substeps.py</code> <pre><code>class LocMakeFedPQNFisherGradientNLL:\n    \"\"\"Mixin to compute local values, gradient and Fisher information of the NLL.\n\n    Attributes\n    ----------\n    local_adata : AnnData\n        The local AnnData.\n    num_jobs : int\n        The number of cpus to use.\n    joblib_verbosity : int\n        The joblib verbosity.\n    joblib_backend : str\n        The backend to use for the IRLS algorithm.\n    irls_batch_size : int\n        The batch size to use for the IRLS algorithm.\n    max_beta : float\n        The maximum value for the beta parameter.\n    PQN_num_iters_ls : int\n        The number of iterations to use for the line search.\n    PQN_min_mu : float\n        The min_mu parameter for the Proximal Quasi Newton algorithm.\n\n    Methods\n    -------\n    make_local_fisher_gradient_nll\n        A remote_data method.\n        Make the local nll, gradient and fisher matrix.\n    \"\"\"\n\n    local_adata: AnnData\n    refit_adata: AnnData\n    num_jobs: int\n    joblib_verbosity: int\n    joblib_backend: str\n    irls_batch_size: int\n    max_beta: float\n    PQN_num_iters_ls: int\n    PQN_min_mu: float\n\n    @remote_data\n    @log_remote_data\n    @reconstruct_adatas\n    def make_local_fisher_gradient_nll(\n        self,\n        data_from_opener: AnnData,\n        shared_state: dict[str, Any],\n        first_iteration_mode: Literal[\"irls_catch\"] | None = None,\n        refit_mode: bool = False,\n    ):\n        r\"\"\"Make the local nll, gradient and fisher information matrix.\n\n        Given an ascent direction :math:`d` (an ascent direction being positively\n        correlated to the gradient of the starting point) and a starting point\n        :math:`beta`, this function\n        computes the nll, gradient and Fisher information at the points\n        :math:`beta + t * d`,\n        for :math:`t` in step_sizes\n        (step sizes are :math:`0.5^i` for :math:`i` in :math:`0,...,19`.\n\n\n        Moreover, if the iteration is the first one, the step sizes are not used,\n        and instead, the nll, gradient and fisher information are computed at the\n        current beta values.\n\n        Parameters\n        ----------\n        data_from_opener : AnnData\n            Not used.\n\n        shared_state : dict\n            A dictionary containing the following\n            keys:\n            - PQN_mask: ndarray\n                A boolean mask indicating if the gene should be used for the\n                proximal newton step.\n                It is of shape (n_non_zero_genes,)\n                Used, but not modified.\n            - round_number_PQN: int\n                The current round number of the prox newton algorithm.\n                Used but not modified.\n            - ascent_direction_on_mask: Optional[ndarray]\n                The ascent direction, of shape (n_genes, n_params), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask).\n                Used but not modified.\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n                Used but not modified.\n            - global_reg_nll: ndarray\n                The global regularized nll, of shape (n_non_zero_genes,).\n                Not used and not modified.\n            - newton_decrement_on_mask: Optional[ndarray]\n                The newton decrement, of shape (n_ngenes,).\n                It is None at the first round of the prox newton algorithm.\n                Not used and not modified.\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n                Not used and not modified.\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the prox newton\n                algorithm.\n                Not used and not modified.\n\n        first_iteration_mode : Optional[Literal[\"irls_catch\"]]\n            For the first iteration, this function behaves differently. If\n            first_iteration_mode is None, then we are not at the first iteration.\n            If first_iteration_mode is not None, the function will expect a\n            different shared state than the one described above, and will construct\n            the initial shared state from it.\n            If first_iteration_mode is \"irls_catch\", then we assume that\n            we are using the PQN algorithm as a method to catch IRLS when it fails\n            The function will expect a\n            shared state that contains the following fields:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n            - irls_mask : ndarray\n                The mask of genes that were still active for the IRLS algorithm.\n\n        refit_mode : bool\n            Whether to run on `refit_adata`s instead of `local_adata`s.\n            (default: False).\n\n        Returns\n        -------\n        dict\n            The state to share to the server.\n            It contains the following fields:\n            - beta: ndarray\n                The log fold changes, of shape (n_non_zero_genes, n_params).\n            - local_nll: ndarray\n                The local nll, of shape (n_step_sizes, n_genes,), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask). n_step_sizes is the number of step sizes\n                considered, which is `PQN_num_iters_ls` if we are not at the\n                first round, and 1 otherwise.\n                This is created during this step.\n            - local_fisher: ndarray\n                The local fisher matrix,\n                of shape (n_step_sizes, n_genes, n_params, n_params).\n                This is created during this step.\n            - local_gradient: ndarray\n                The local gradient, of shape (n_step_sizes, n_genes, n_params).\n                This is created during this step.\n            - PQN_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the prox newton\n                algorithm.\n            - PQN_mask: ndarray\n                A boolean mask indicating if the gene should be used for the\n                proximal newton step, of shape (n_non_zero_genes,).\n            - global_reg_nll: ndarray\n                The global regularized nll, of shape (n_non_zero_genes,).\n            - newton_decrement_on_mask: Optional[ndarray]\n                The newton decrement, of shape (n_ngenes,).\n                This is None at the first round of the prox newton algorithm.\n            - round_number_PQN: int\n                The current round number of the prox newton algorithm.\n            - irls_diverged_mask: ndarray\n                A boolean mask indicating if the gene has diverged in the IRLS\n                algorithm.\n            - ascent_direction_on_mask: Optional[ndarray]\n                The ascent direction, of shape (n_genes, n_params), where\n                n_genes is the current number of genes that are active (True\n                in the PQN_mask).\n\n        Raises\n        ------\n        ValueError\n            If first_iteration_mode is not None or \"irls_catch\".\n        \"\"\"\n        if refit_mode:\n            adata = self.refit_adata\n        else:\n            adata = self.local_adata\n\n        # Distinguish between the first iteration and the rest\n        if first_iteration_mode is not None and first_iteration_mode == \"irls_catch\":\n            beta = shared_state[\"beta\"]\n            irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n            irls_mask = shared_state[\"irls_mask\"]\n            PQN_mask = irls_mask | irls_diverged_mask\n            irls_diverged_mask = PQN_mask.copy()\n            round_number_PQN = 0\n            ascent_direction_on_mask = None\n            newton_decrement_on_mask = None\n            PQN_diverged_mask = np.zeros_like(irls_mask, dtype=bool)\n            global_reg_nll = np.nan * np.ones_like(irls_mask, dtype=float)\n        elif first_iteration_mode is None:\n            # If we are not at the first iteration, we use the shared state\n            PQN_mask = shared_state[\"PQN_mask\"]\n            round_number_PQN = shared_state[\"round_number_PQN\"]\n            ascent_direction_on_mask = shared_state[\"ascent_direction_on_mask\"]\n            beta = shared_state[\"beta\"]\n            PQN_diverged_mask = shared_state[\"PQN_diverged_mask\"]\n            newton_decrement_on_mask = shared_state[\"newton_decrement_on_mask\"]\n            global_reg_nll = shared_state[\"global_reg_nll\"]\n            irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n        else:\n            raise ValueError(\"first_iteration_mode should be None or 'irls_catch'\")\n\n        if round_number_PQN == 0:\n            # Sanity check that this is the first round of fed prox\n            beta[PQN_mask] = adata.uns[\"_irls_beta_init\"][PQN_mask]\n            step_sizes: np.ndarray | None = None\n\n        else:\n            step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n\n        # Get the quantities stored in the adata\n        disp_param_name = adata.uns[\"_irls_disp_param_name\"]\n\n        (\n            PQN_gene_names,\n            design_matrix,\n            size_factors,\n            counts,\n            dispersions,\n            beta_on_mask,\n        ) = get_lfc_utils_from_gene_mask_adata(\n            adata,\n            PQN_mask,\n            disp_param_name,\n            beta=beta,\n        )\n\n        # ---- Compute local nll, gradient and Fisher information ---- #\n\n        with parallel_backend(self.joblib_backend):\n            res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n                delayed(make_fisher_gradient_nll_step_sizes_batch)(\n                    design_matrix=design_matrix,\n                    size_factors=size_factors,\n                    beta=beta_on_mask[i : i + self.irls_batch_size],\n                    dispersions=dispersions[i : i + self.irls_batch_size],\n                    counts=counts[:, i : i + self.irls_batch_size],\n                    ascent_direction=(\n                        ascent_direction_on_mask[i : i + self.irls_batch_size]\n                        if ascent_direction_on_mask is not None\n                        else None\n                    ),\n                    step_sizes=step_sizes,\n                    beta_min=-self.max_beta,\n                    beta_max=self.max_beta,\n                    min_mu=self.PQN_min_mu,\n                )\n                for i in range(0, len(beta_on_mask), self.irls_batch_size)\n            )\n\n        n_step_sizes = len(step_sizes) if step_sizes is not None else 1\n        if len(res) == 0:\n            H = np.zeros((n_step_sizes, 0, beta.shape[1], beta.shape[1]))\n            gradient = np.zeros((n_step_sizes, 0, beta.shape[1]))\n            local_nll = np.zeros((n_step_sizes, 0))\n        else:\n            H = np.concatenate([r[0] for r in res], axis=1)\n            gradient = np.concatenate([r[1] for r in res], axis=1)\n            local_nll = np.concatenate([r[2] for r in res], axis=1)\n\n        # Create the shared state\n        return {\n            \"beta\": beta,\n            \"local_nll\": local_nll,\n            \"local_fisher\": H,\n            \"local_gradient\": gradient,\n            \"PQN_diverged_mask\": PQN_diverged_mask,\n            \"PQN_mask\": PQN_mask,\n            \"global_reg_nll\": global_reg_nll,\n            \"newton_decrement_on_mask\": newton_decrement_on_mask,\n            \"round_number_PQN\": round_number_PQN,\n            \"irls_diverged_mask\": irls_diverged_mask,\n            \"ascent_direction_on_mask\": ascent_direction_on_mask,\n        }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.substeps.LocMakeFedPQNFisherGradientNLL.make_local_fisher_gradient_nll","title":"<code>make_local_fisher_gradient_nll(data_from_opener, shared_state, first_iteration_mode=None, refit_mode=False)</code>","text":"<p>Make the local nll, gradient and fisher information matrix.</p> <p>Given an ascent direction :math:<code>d</code> (an ascent direction being positively correlated to the gradient of the starting point) and a starting point :math:<code>beta</code>, this function computes the nll, gradient and Fisher information at the points :math:<code>beta + t * d</code>, for :math:<code>t</code> in step_sizes (step sizes are :math:<code>0.5^i</code> for :math:<code>i</code> in :math:<code>0,...,19</code>.</p> <p>Moreover, if the iteration is the first one, the step sizes are not used, and instead, the nll, gradient and fisher information are computed at the current beta values.</p> <p>Parameters:</p> Name Type Description Default <code>data_from_opener</code> <code>AnnData</code> <p>Not used.</p> required <code>shared_state</code> <code>dict</code> <p>A dictionary containing the following keys: - PQN_mask: ndarray     A boolean mask indicating if the gene should be used for the     proximal newton step.     It is of shape (n_non_zero_genes,)     Used, but not modified. - round_number_PQN: int     The current round number of the prox newton algorithm.     Used but not modified. - ascent_direction_on_mask: Optional[ndarray]     The ascent direction, of shape (n_genes, n_params), where     n_genes is the current number of genes that are active (True     in the PQN_mask).     Used but not modified. - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params).     Used but not modified. - global_reg_nll: ndarray     The global regularized nll, of shape (n_non_zero_genes,).     Not used and not modified. - newton_decrement_on_mask: Optional[ndarray]     The newton decrement, of shape (n_ngenes,).     It is None at the first round of the prox newton algorithm.     Not used and not modified. - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm.     Not used and not modified. - PQN_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the prox newton     algorithm.     Not used and not modified.</p> required <code>first_iteration_mode</code> <code>Optional[Literal['irls_catch']]</code> <p>For the first iteration, this function behaves differently. If first_iteration_mode is None, then we are not at the first iteration. If first_iteration_mode is not None, the function will expect a different shared state than the one described above, and will construct the initial shared state from it. If first_iteration_mode is \"irls_catch\", then we assume that we are using the PQN algorithm as a method to catch IRLS when it fails The function will expect a shared state that contains the following fields: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm. - irls_mask : ndarray     The mask of genes that were still active for the IRLS algorithm.</p> <code>None</code> <code>refit_mode</code> <code>bool</code> <p>Whether to run on <code>refit_adata</code>s instead of <code>local_adata</code>s. (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The state to share to the server. It contains the following fields: - beta: ndarray     The log fold changes, of shape (n_non_zero_genes, n_params). - local_nll: ndarray     The local nll, of shape (n_step_sizes, n_genes,), where     n_genes is the current number of genes that are active (True     in the PQN_mask). n_step_sizes is the number of step sizes     considered, which is <code>PQN_num_iters_ls</code> if we are not at the     first round, and 1 otherwise.     This is created during this step. - local_fisher: ndarray     The local fisher matrix,     of shape (n_step_sizes, n_genes, n_params, n_params).     This is created during this step. - local_gradient: ndarray     The local gradient, of shape (n_step_sizes, n_genes, n_params).     This is created during this step. - PQN_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the prox newton     algorithm. - PQN_mask: ndarray     A boolean mask indicating if the gene should be used for the     proximal newton step, of shape (n_non_zero_genes,). - global_reg_nll: ndarray     The global regularized nll, of shape (n_non_zero_genes,). - newton_decrement_on_mask: Optional[ndarray]     The newton decrement, of shape (n_ngenes,).     This is None at the first round of the prox newton algorithm. - round_number_PQN: int     The current round number of the prox newton algorithm. - irls_diverged_mask: ndarray     A boolean mask indicating if the gene has diverged in the IRLS     algorithm. - ascent_direction_on_mask: Optional[ndarray]     The ascent direction, of shape (n_genes, n_params), where     n_genes is the current number of genes that are active (True     in the PQN_mask).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If first_iteration_mode is not None or \"irls_catch\".</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/substeps.py</code> <pre><code>@remote_data\n@log_remote_data\n@reconstruct_adatas\ndef make_local_fisher_gradient_nll(\n    self,\n    data_from_opener: AnnData,\n    shared_state: dict[str, Any],\n    first_iteration_mode: Literal[\"irls_catch\"] | None = None,\n    refit_mode: bool = False,\n):\n    r\"\"\"Make the local nll, gradient and fisher information matrix.\n\n    Given an ascent direction :math:`d` (an ascent direction being positively\n    correlated to the gradient of the starting point) and a starting point\n    :math:`beta`, this function\n    computes the nll, gradient and Fisher information at the points\n    :math:`beta + t * d`,\n    for :math:`t` in step_sizes\n    (step sizes are :math:`0.5^i` for :math:`i` in :math:`0,...,19`.\n\n\n    Moreover, if the iteration is the first one, the step sizes are not used,\n    and instead, the nll, gradient and fisher information are computed at the\n    current beta values.\n\n    Parameters\n    ----------\n    data_from_opener : AnnData\n        Not used.\n\n    shared_state : dict\n        A dictionary containing the following\n        keys:\n        - PQN_mask: ndarray\n            A boolean mask indicating if the gene should be used for the\n            proximal newton step.\n            It is of shape (n_non_zero_genes,)\n            Used, but not modified.\n        - round_number_PQN: int\n            The current round number of the prox newton algorithm.\n            Used but not modified.\n        - ascent_direction_on_mask: Optional[ndarray]\n            The ascent direction, of shape (n_genes, n_params), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask).\n            Used but not modified.\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n            Used but not modified.\n        - global_reg_nll: ndarray\n            The global regularized nll, of shape (n_non_zero_genes,).\n            Not used and not modified.\n        - newton_decrement_on_mask: Optional[ndarray]\n            The newton decrement, of shape (n_ngenes,).\n            It is None at the first round of the prox newton algorithm.\n            Not used and not modified.\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n            Not used and not modified.\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the prox newton\n            algorithm.\n            Not used and not modified.\n\n    first_iteration_mode : Optional[Literal[\"irls_catch\"]]\n        For the first iteration, this function behaves differently. If\n        first_iteration_mode is None, then we are not at the first iteration.\n        If first_iteration_mode is not None, the function will expect a\n        different shared state than the one described above, and will construct\n        the initial shared state from it.\n        If first_iteration_mode is \"irls_catch\", then we assume that\n        we are using the PQN algorithm as a method to catch IRLS when it fails\n        The function will expect a\n        shared state that contains the following fields:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n        - irls_mask : ndarray\n            The mask of genes that were still active for the IRLS algorithm.\n\n    refit_mode : bool\n        Whether to run on `refit_adata`s instead of `local_adata`s.\n        (default: False).\n\n    Returns\n    -------\n    dict\n        The state to share to the server.\n        It contains the following fields:\n        - beta: ndarray\n            The log fold changes, of shape (n_non_zero_genes, n_params).\n        - local_nll: ndarray\n            The local nll, of shape (n_step_sizes, n_genes,), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask). n_step_sizes is the number of step sizes\n            considered, which is `PQN_num_iters_ls` if we are not at the\n            first round, and 1 otherwise.\n            This is created during this step.\n        - local_fisher: ndarray\n            The local fisher matrix,\n            of shape (n_step_sizes, n_genes, n_params, n_params).\n            This is created during this step.\n        - local_gradient: ndarray\n            The local gradient, of shape (n_step_sizes, n_genes, n_params).\n            This is created during this step.\n        - PQN_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the prox newton\n            algorithm.\n        - PQN_mask: ndarray\n            A boolean mask indicating if the gene should be used for the\n            proximal newton step, of shape (n_non_zero_genes,).\n        - global_reg_nll: ndarray\n            The global regularized nll, of shape (n_non_zero_genes,).\n        - newton_decrement_on_mask: Optional[ndarray]\n            The newton decrement, of shape (n_ngenes,).\n            This is None at the first round of the prox newton algorithm.\n        - round_number_PQN: int\n            The current round number of the prox newton algorithm.\n        - irls_diverged_mask: ndarray\n            A boolean mask indicating if the gene has diverged in the IRLS\n            algorithm.\n        - ascent_direction_on_mask: Optional[ndarray]\n            The ascent direction, of shape (n_genes, n_params), where\n            n_genes is the current number of genes that are active (True\n            in the PQN_mask).\n\n    Raises\n    ------\n    ValueError\n        If first_iteration_mode is not None or \"irls_catch\".\n    \"\"\"\n    if refit_mode:\n        adata = self.refit_adata\n    else:\n        adata = self.local_adata\n\n    # Distinguish between the first iteration and the rest\n    if first_iteration_mode is not None and first_iteration_mode == \"irls_catch\":\n        beta = shared_state[\"beta\"]\n        irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n        irls_mask = shared_state[\"irls_mask\"]\n        PQN_mask = irls_mask | irls_diverged_mask\n        irls_diverged_mask = PQN_mask.copy()\n        round_number_PQN = 0\n        ascent_direction_on_mask = None\n        newton_decrement_on_mask = None\n        PQN_diverged_mask = np.zeros_like(irls_mask, dtype=bool)\n        global_reg_nll = np.nan * np.ones_like(irls_mask, dtype=float)\n    elif first_iteration_mode is None:\n        # If we are not at the first iteration, we use the shared state\n        PQN_mask = shared_state[\"PQN_mask\"]\n        round_number_PQN = shared_state[\"round_number_PQN\"]\n        ascent_direction_on_mask = shared_state[\"ascent_direction_on_mask\"]\n        beta = shared_state[\"beta\"]\n        PQN_diverged_mask = shared_state[\"PQN_diverged_mask\"]\n        newton_decrement_on_mask = shared_state[\"newton_decrement_on_mask\"]\n        global_reg_nll = shared_state[\"global_reg_nll\"]\n        irls_diverged_mask = shared_state[\"irls_diverged_mask\"]\n    else:\n        raise ValueError(\"first_iteration_mode should be None or 'irls_catch'\")\n\n    if round_number_PQN == 0:\n        # Sanity check that this is the first round of fed prox\n        beta[PQN_mask] = adata.uns[\"_irls_beta_init\"][PQN_mask]\n        step_sizes: np.ndarray | None = None\n\n    else:\n        step_sizes = 0.5 ** np.arange(self.PQN_num_iters_ls)\n\n    # Get the quantities stored in the adata\n    disp_param_name = adata.uns[\"_irls_disp_param_name\"]\n\n    (\n        PQN_gene_names,\n        design_matrix,\n        size_factors,\n        counts,\n        dispersions,\n        beta_on_mask,\n    ) = get_lfc_utils_from_gene_mask_adata(\n        adata,\n        PQN_mask,\n        disp_param_name,\n        beta=beta,\n    )\n\n    # ---- Compute local nll, gradient and Fisher information ---- #\n\n    with parallel_backend(self.joblib_backend):\n        res = Parallel(n_jobs=self.num_jobs, verbose=self.joblib_verbosity)(\n            delayed(make_fisher_gradient_nll_step_sizes_batch)(\n                design_matrix=design_matrix,\n                size_factors=size_factors,\n                beta=beta_on_mask[i : i + self.irls_batch_size],\n                dispersions=dispersions[i : i + self.irls_batch_size],\n                counts=counts[:, i : i + self.irls_batch_size],\n                ascent_direction=(\n                    ascent_direction_on_mask[i : i + self.irls_batch_size]\n                    if ascent_direction_on_mask is not None\n                    else None\n                ),\n                step_sizes=step_sizes,\n                beta_min=-self.max_beta,\n                beta_max=self.max_beta,\n                min_mu=self.PQN_min_mu,\n            )\n            for i in range(0, len(beta_on_mask), self.irls_batch_size)\n        )\n\n    n_step_sizes = len(step_sizes) if step_sizes is not None else 1\n    if len(res) == 0:\n        H = np.zeros((n_step_sizes, 0, beta.shape[1], beta.shape[1]))\n        gradient = np.zeros((n_step_sizes, 0, beta.shape[1]))\n        local_nll = np.zeros((n_step_sizes, 0))\n    else:\n        H = np.concatenate([r[0] for r in res], axis=1)\n        gradient = np.concatenate([r[1] for r in res], axis=1)\n        local_nll = np.concatenate([r[2] for r in res], axis=1)\n\n    # Create the shared state\n    return {\n        \"beta\": beta,\n        \"local_nll\": local_nll,\n        \"local_fisher\": H,\n        \"local_gradient\": gradient,\n        \"PQN_diverged_mask\": PQN_diverged_mask,\n        \"PQN_mask\": PQN_mask,\n        \"global_reg_nll\": global_reg_nll,\n        \"newton_decrement_on_mask\": newton_decrement_on_mask,\n        \"round_number_PQN\": round_number_PQN,\n        \"irls_diverged_mask\": irls_diverged_mask,\n        \"ascent_direction_on_mask\": ascent_direction_on_mask,\n    }\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.utils","title":"<code>utils</code>","text":"<p>Utility functions for the proximal Newton optimization.</p> <p>This optimization is used in the catching of the IRLS algorithm.</p>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.utils.compute_ascent_direction_decrement","title":"<code>compute_ascent_direction_decrement(gradient_scaling_matrix, gradient, beta, max_beta)</code>","text":"<p>Compute the ascent direction and decrement.</p> <p>We do this from the gradient scaling matrix, the gradient, the beta and the max beta, which embodies the box constraints.</p> <p>Please look at this paper for the precise references to the equations: https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf</p> <p>By ascent direction, we mean that the direction we compute is positively correlated with the gradient. As our aim is to minimize the function, we want to move in the opposite direction of the ascent direction, but it is simpler to compute the ascent direction to avoid sign errors.</p> <p>Parameters:</p> Name Type Description Default <code>gradient_scaling_matrix</code> <code>ndarray</code> <p>The gradient scaling matrix, of shape (n_genes, n_params, n_params).</p> required <code>gradient</code> <code>ndarray</code> <p>The gradient per gene, of shape (n_genes, n_params).</p> required <code>beta</code> <code>ndarray</code> <p>Beta on those genes, of shape (n_genes, n_params).</p> required <code>max_beta</code> <code>float</code> <p>The max absolute value for beta.</p> required <p>Returns:</p> Name Type Description <code>ascent_direction</code> <code>ndarray</code> <p>The new ascent direction, of shape (n_genes, n_params).</p> <code>newton_decrement</code> <code>ndarray</code> <p>The newton decrement associated to these ascent directions of shape (n_genes, )</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/utils.py</code> <pre><code>def compute_ascent_direction_decrement(\n    gradient_scaling_matrix: np.ndarray,\n    gradient: np.ndarray,\n    beta: np.ndarray,\n    max_beta: float,\n):\n    \"\"\"Compute the ascent direction and decrement.\n\n    We do this from the gradient scaling matrix, the gradient,\n    the beta and the max beta, which embodies the box constraints.\n\n    Please look at this paper for the precise references to the equations:\n    https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf\n\n    By ascent direction, we mean that the direction we compute is positively\n    correlated with the gradient. As our aim is to minimize the function,\n    we want to move in the opposite direction of the ascent direction, but\n    it is simpler to compute the ascent direction to avoid sign errors.\n\n    Parameters\n    ----------\n    gradient_scaling_matrix : np.ndarray\n        The gradient scaling matrix, of shape (n_genes, n_params, n_params).\n    gradient : np.ndarray\n        The gradient per gene, of shape (n_genes, n_params).\n    beta : np.ndarray\n        Beta on those genes, of shape (n_genes, n_params).\n    max_beta : float\n        The max absolute value for beta.\n\n    Returns\n    -------\n    ascent_direction : np.ndarray\n        The new ascent direction, of shape (n_genes, n_params).\n    newton_decrement : np.ndarray\n        The newton decrement associated to these ascent directions\n        of shape (n_genes, )\n    \"\"\"\n    # ---- Step 1: compute first index set ---- #\n    # See https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf\n    # equation 2.2\n\n    lower_binding = (beta &lt; -max_beta + 1e-14) &amp; (gradient &gt; 0)\n    upper_binding = (beta &gt; max_beta - 1e-14) &amp; (gradient &lt; 0)\n    first_index_mask = lower_binding | upper_binding  # of shape (n_genes, n_params)\n\n    # Set to zero the gradient scaling matrix on the first index\n\n    n_params = beta.shape[1]\n\n    gradient_scaling_matrix[\n        np.repeat(first_index_mask[:, :, None], repeats=n_params, axis=2)\n    ] = 0\n    gradient_scaling_matrix[\n        np.repeat(first_index_mask[:, None, :], repeats=n_params, axis=1)\n    ] = 0\n\n    ascent_direction = (gradient_scaling_matrix @ gradient[:, :, None]).squeeze(\n        axis=2\n    )  # of shape (n_genes, n_params)\n\n    # ---- Step 2: Compute the second index set ---- #\n    # See https://www.cs.utexas.edu/~inderjit/public_papers/pqnj_sisc10.pdf\n    # equation 2.3\n\n    lower_binding = (beta &lt; -max_beta + 1e-14) &amp; (ascent_direction &gt; 0)\n    upper_binding = (beta &gt; max_beta - 1e-14) &amp; (ascent_direction &lt; 0)\n    second_index_mask = lower_binding | upper_binding\n\n    # Set to zero the gradient scaling matrix on the second index\n\n    gradient_scaling_matrix[\n        np.repeat(second_index_mask[:, :, None], repeats=n_params, axis=2)\n    ] = 0\n    gradient_scaling_matrix[\n        np.repeat(second_index_mask[:, None, :], repeats=n_params, axis=1)\n    ] = 0\n\n    # ---- Step 3: Compute the ascent direction and Newton decrement ---- #\n\n    ascent_direction = gradient_scaling_matrix @ gradient[:, :, None]\n    newton_decrement = (gradient[:, None, :] @ ascent_direction).squeeze(axis=(1, 2))\n\n    ascent_direction = ascent_direction.squeeze(axis=2)\n\n    return ascent_direction, newton_decrement\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.utils.compute_gradient_scaling_matrix_fisher","title":"<code>compute_gradient_scaling_matrix_fisher(fisher, backend, num_jobs, joblib_verbosity, batch_size)</code>","text":"<p>Compute the gradient scaling matrix using the Fisher information.</p> <p>In this case, we simply invert the provided Fisher matrix to get the gradient scaling matrix.</p> <p>Parameters:</p> Name Type Description Default <code>fisher</code> <code>ndarray</code> <p>The Fisher matrix, of shape (n_genes, n_params, n_params)</p> required <code>backend</code> <code>str</code> <p>The backend to use for parallelization</p> required <code>num_jobs</code> <code>int</code> <p>The number of cpus to use</p> required <code>joblib_verbosity</code> <code>int</code> <p>The verbosity level of joblib</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for the computation</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The gradient scaling matrix, of shape (n_genes, n_params, n_params)</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/utils.py</code> <pre><code>def compute_gradient_scaling_matrix_fisher(\n    fisher: np.ndarray,\n    backend: str,\n    num_jobs: int,\n    joblib_verbosity: int,\n    batch_size: int,\n):\n    \"\"\"Compute the gradient scaling matrix using the Fisher information.\n\n    In this case, we simply invert the provided Fisher matrix to get the gradient\n    scaling matrix.\n\n    Parameters\n    ----------\n    fisher : ndarray\n        The Fisher matrix, of shape (n_genes, n_params, n_params)\n    backend : str\n        The backend to use for parallelization\n    num_jobs : int\n        The number of cpus to use\n    joblib_verbosity : int\n        The verbosity level of joblib\n    batch_size : int\n        The batch size to use for the computation\n\n    Returns\n    -------\n    ndarray\n        The gradient scaling matrix, of shape (n_genes, n_params, n_params)\n    \"\"\"\n    with parallel_backend(backend):\n        res = Parallel(n_jobs=num_jobs, verbose=joblib_verbosity)(\n            delayed(np.linalg.inv)(\n                fisher[i : i + batch_size],\n            )\n            for i in range(0, len(fisher), batch_size)\n        )\n    if len(res) &gt; 0:\n        gradient_scaling_matrix = np.concatenate(res)\n    else:\n        gradient_scaling_matrix = np.zeros_like(fisher)\n\n    return gradient_scaling_matrix\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#fedpydeseq2.core.fed_algorithms.fed_PQN.utils.make_fisher_gradient_nll_step_sizes_batch","title":"<code>make_fisher_gradient_nll_step_sizes_batch(design_matrix, size_factors, beta, dispersions, counts, ascent_direction, step_sizes, beta_min, beta_max, min_mu=0.5)</code>","text":"<p>Make local gradient, fisher matrix, and nll for multiple steps.</p> <p>Parameters:</p> Name Type Description Default <code>design_matrix</code> <code>ndarray</code> <p>The design matrix, of shape (n_obs, n_params).</p> required <code>size_factors</code> <code>ndarray</code> <p>The size factors, of shape (n_obs).</p> required <code>beta</code> <code>ndarray</code> <p>The log fold change matrix, of shape (batch_size, n_params).</p> required <code>dispersions</code> <code>ndarray</code> <p>The dispersions, of shape (batch_size).</p> required <code>counts</code> <code>ndarray</code> <p>The counts, of shape (n_obs,batch_size).</p> required <code>ascent_direction</code> <code>ndarray</code> <p>The ascent direction, of shape (batch_size, n_params).</p> required <code>step_sizes</code> <code>ndarray | None</code> <p>A list of step sizes to evaluate, of size (n_steps, ).</p> required <code>beta_min</code> <code>float | None</code> <p>The minimum value tolerated for beta.</p> required <code>beta_max</code> <code>float | None</code> <p>The maximum value tolerated for beta.</p> required <code>min_mu</code> <code>float</code> <p>Lower bound on estimated means, to ensure numerical stability.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>H</code> <code>Optional[ndarray]</code> <p>The Fisher information matrix, of shape (n_steps, batch_size, n_params, n_params).</p> <code>gradient</code> <code>ndarray</code> <p>The gradient, of shape (n_steps, batch_size, n_params).</p> <code>nll</code> <code>ndarray</code> <p>The nll evaluations on all steps, of size (n_steps, batch_size).</p> Source code in <code>fedpydeseq2/core/fed_algorithms/fed_PQN/utils.py</code> <pre><code>def make_fisher_gradient_nll_step_sizes_batch(\n    design_matrix: np.ndarray,\n    size_factors: np.ndarray,\n    beta: np.ndarray,\n    dispersions: np.ndarray,\n    counts: np.ndarray,\n    ascent_direction: np.ndarray | None,\n    step_sizes: np.ndarray | None,\n    beta_min: float | None,\n    beta_max: float | None,\n    min_mu: float = 0.5,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Make local gradient, fisher matrix, and nll for multiple steps.\n\n    Parameters\n    ----------\n    design_matrix : ndarray\n        The design matrix, of shape (n_obs, n_params).\n    size_factors : ndarray\n        The size factors, of shape (n_obs).\n    beta : ndarray\n        The log fold change matrix, of shape (batch_size, n_params).\n    dispersions : ndarray\n        The dispersions, of shape (batch_size).\n    counts : ndarray\n        The counts, of shape (n_obs,batch_size).\n    ascent_direction : np.ndarray\n        The ascent direction, of shape (batch_size, n_params).\n    step_sizes: np.ndarray\n        A list of step sizes to evaluate, of size (n_steps, ).\n    beta_min: float\n        The minimum value tolerated for beta.\n    beta_max: float\n        The maximum value tolerated for beta.\n    min_mu : float\n        Lower bound on estimated means, to ensure numerical stability.\n\n    Returns\n    -------\n    H : Optional[ndarray]\n        The Fisher information matrix, of shape\n        (n_steps, batch_size, n_params, n_params).\n    gradient : ndarray\n        The gradient, of shape (n_steps, batch_size, n_params).\n    nll : ndarray\n        The nll evaluations on all steps, of size (n_steps, batch_size).\n    \"\"\"\n    # If no ascent direction is provided, we do not need to compute the grid\n    # of beta values, but only the current beta value, where we unsqueeze the\n    # first dimension to make it compatible with the rest of the code\n    # This is the case when we are at the first iteration of the optimization\n    if ascent_direction is None and step_sizes is None:\n        beta_grid = np.clip(\n            beta[None, :, :],\n            beta_min,\n            beta_max,\n        )  # of shape (n_steps, batch_size, n_params)\n\n    # In this case, we compute the grid of beta values, by moving in the direction\n    # of the ascent direction, by the step sizes\n    else:\n        assert isinstance(step_sizes, np.ndarray) and isinstance(\n            ascent_direction, np.ndarray\n        )\n        beta_grid = np.clip(\n            beta[None, :, :] - step_sizes[:, None, None] * ascent_direction[None, :, :],\n            beta_min,\n            beta_max,\n        )  # of shape (n_steps, batch_size, n_params)\n\n    mu_grid = size_factors[None, None, :] * np.exp(\n        (design_matrix[None, None, :, :] @ beta_grid[:, :, :, None]).squeeze(axis=3)\n    )  # of shape (n_steps, batch_size, n_obs)\n    mu_grid = np.maximum(\n        mu_grid,\n        min_mu,\n    )\n\n    # --- Step 1: Compute the gradient ----#\n\n    gradient_term_1 = -(design_matrix.T @ counts).T[\n        None, :, :\n    ]  # shape (1, batch_size, n_params)\n    gradient_term_2 = (\n        design_matrix.T[None, None, :, :]\n        @ (\n            (1 / dispersions[None, :, None] + counts.T[None, :, :])\n            * mu_grid\n            / (1 / dispersions[None, :, None] + mu_grid)  # n_steps, batch_size, n_obs\n        )[:, :, :, None]\n    ).squeeze(\n        3\n    )  # Shape n_steps, batch_size, n_params\n    gradient = gradient_term_1 + gradient_term_2\n\n    # ---- Step 2: Compute the Fisher matrix  ----#\n\n    W = mu_grid / (1.0 + mu_grid * dispersions[None, :, None])\n    expanded_design = design_matrix[\n        None, None, :, :\n    ]  # of shape (1, 1, n_obs, n_params)\n    assert W is not None\n    H = (expanded_design * W[:, :, :, None]).transpose(0, 1, 3, 2) @ expanded_design\n    # H of size (n_steps, batch_size, n_params, n_params)\n\n    # Get the mu_grid\n    nll = mu_grid_nb_nll(counts, mu_grid, dispersions)\n\n    return H, gradient, nll\n</code></pre>"},{"location":"api/core/federated_algorithms/fed_pqn/fed_pqn/#table-with-shared-quantities-between-centers-and-server","title":"Table with shared quantities between centers and server","text":"ID Name Type Shape Description Computed by Sent to 0 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 0 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 0 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 0 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 0 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 1 ascent_direction_on_mask NoneType The ascent direction on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 1 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, initialized at \\(0\\). Each center Server 1 newton_decrement_on_mask NoneType The newton decrement on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 1 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, intialized at \\(\\texttt{np.nan}\\) since no nll has been computed at this stage. Each center Server 1 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). The genes which have diverged during the IRLS algorithm and those which have not converged at the end of the prescribed iterations are set to \\(\\texttt{True}\\). Each center Server 1 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on during the Fed Proximal Quasi-Newton algorithm, since the server is stateless. Each center Server 1 local_gradient nparray \\((1, G_{\\texttt{act}}, p)\\) For each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, i.e., \\(\\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g) =   -(X^{(k)})^{\\top} Y^{(k)}_{g} + (X^{(k)})^{\\top} \\left( \\frac{1}{\\alpha_g} + Y^{(k)}_{g} \\right) \\frac{\\mu^{(k)}_{g}}{\\frac{1}{\\alpha_g} + \\mu^{(k)}_{g}}\\), where \\(\\mu^{(k)}_{g}\\) and \\(Y^{(k)}_{g}\\) are both vectors in \\(\\mathbb{R}^{n_k}\\) and \\(\\alpha_g\\) is a scalar. TODO ref equation. Each center Server 1 local_fisher nparray \\((1, G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion, \\(n_k\\mathcal{I}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{gii} = \\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 local_nll nparray \\((1, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\). Each center Server 1 beta nparray \\((G_{\\texttt{nz}}, p)\\) For evert gene \\(g\\), the inital value of the \\(\\beta_g\\) parameter for the ProxQuasiNewton algorithm (PQN), which is either i) the value computed by IRLS if IRLS has converged on that gene (in that case, the gene will not be optimized), or ii) the initial value of the \\(\\beta_g\\) parameter for the IRLS algorithm, if IRLS has not converged on that gene. Each center Server 1 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Initialized to \\(\\texttt{False}\\). Each center Server 2 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 2 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 2 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 2 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes. It is computed as \\(\\nu_g = \\Delta_g  \\cdot \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) where \\(\\Delta_g\\) is the ascent direction on the active genes and \\(\\mathcal{L}^{\\lambda}\\) is the regularized log likelihood. Server Center 2 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes. To compute the ascent direction, we first compute the global Fisher information matrix for each gene \\(g\\), \\(n \\mathcal{I}_g = \\sum_k n_k \\mathcal{I}^{(k)}_{g}\\) and the global gradient \\(\\nabla_{\\beta} \\mathcal{L}(\\beta_g) = \\sum_k \\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g)\\). From these two quantities, we build the Fisher information and gradients of the regularized negative log likelihood, with \\(L^2\\) regularization \\(\\lambda = 10^{-6}\\). We add another regularization term to the Fisher matrix which depends on the iteration number. The goal is for the ascent direction to be close to the gradient for small iterations, and to be close to the real natural gradient descent update near the end of the optimization. The ascent direction is then computed as from the regularized Fisher and gradient (see main paper). Roughly, it is the solution of the linear system \\(n~\\mathcal{I}^{\\lambda}_g \\Delta_g = \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\), where certain components are dropped in to handle boundary conditions as the optimization is constrained to values of \\(\\beta_g\\) in \\([-\\texttt{max\\_beta},\\texttt{max\\_beta}]\\). Server Center 2 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Server Center 2 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which remains unchanged during the first iteration of the Proximal Quasi-Newton algorithm. Server Center 2 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Server Center 3 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 3 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 3 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 3 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 3 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 3 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 3 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 3 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 3 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 3 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 3 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 4 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 4 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 4 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 4 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. For more details, see description of 19. Server Center 4 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. Server Center 4 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. A gene \\(g\\) is considered to have diverged if for all step sizes \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and for the computed ascent direction \\(\\Delta_g\\), the Armijo condition is not satisfied, i.e., if \\(\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_g - \\delta \\Delta_g) \\leq \\delta ~ \\texttt{PQN\\_c1} ~ \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g) \\cdot \\Delta_g\\) where \\(\\mathcal{L}^{\\lambda}\\) is the regularized negative log likelihood, \\(\\texttt{PQN\\_c1}\\) is the Armijo parameter which can be set by the user (default is \\(10^{-4}\\)), and \\(\\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) is the gradient of the regularized negative log likelihood. Server Center 4 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). Genes that had converged with the relative error criterion or diverged (no stepsize satisfying the Armijo condition) have set to \\(\\texttt{False}\\). Server Center 4 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 5 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 5 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 5 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 5 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 5 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 5 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 5 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 5 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 5 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 5 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 5 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 6 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 6 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 6 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center"},{"location":"api/core/federated_algorithms/fed_pqn/shared/","title":"Shared","text":"ID Name Type Shape Description Computed by Sent to 0 beta nparray \\((G_{\\texttt{nz}}, p)\\) The updated value of the \\(\\beta\\) parameter after applying the IRLS update for the active genes. At the last step, is not updated. Server Center 0 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. A gene \\(g\\) is considered to have diverged if any component of \\(\\beta_g \\in \\mathbb{R}^p\\) has absolute value above the threshold \\(\\texttt{max\\_beta}\\). Server Center 0 irls_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the IRLS algorithm. Genes whose \\(\\beta\\) parameter has diverged are removed from the optimization. Moreover, the deviance ratio is computed for each active gene \\(g\\), as $\\frac{\\left| 2  \\mathcal{L}(\\beta_g) - 2 \\cdot \\mathcal{L}(\\beta_{g, \\text{prev}}) \\right|}{\\left| 2  \\mathcal{L}(\\beta) \\right| + 0.1} $ where \\(\\beta_{g, \\text{prev}}\\) is the value of \\(\\beta_{g}\\) at the previous iteration. If this deviance is smaller than the user inputed \\(\\texttt{beta\\_tol}\\), the gene is considered to have converged and is set to \\(\\texttt{False}\\) in the \\(\\texttt{irls\\_mask}\\), meaning it will not be optimized further. Server Center 0 global_nll nparray \\((G_{\\texttt{nz}},)\\) For each gene \\(g\\), the negative log-likelihood at the \\(\\beta_g\\) parameter, built by summing the local nlls from the centers. Server Center 0 round_number_irls int The current round number of the IRLS algorithm, incremented by \\(1\\) during the local step. Server Center 1 ascent_direction_on_mask NoneType The ascent direction on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 1 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, initialized at \\(0\\). Each center Server 1 newton_decrement_on_mask NoneType The newton decrement on the active genes, which is set to \\(\\texttt{None}\\) at the beginning. Each center Server 1 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, intialized at \\(\\texttt{np.nan}\\) since no nll has been computed at this stage. Each center Server 1 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). The genes which have diverged during the IRLS algorithm and those which have not converged at the end of the prescribed iterations are set to \\(\\texttt{True}\\). Each center Server 1 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Simply passed on during the Fed Proximal Quasi-Newton algorithm, since the server is stateless. Each center Server 1 local_gradient nparray \\((1, G_{\\texttt{act}}, p)\\) For each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, i.e., \\(\\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g) =   -(X^{(k)})^{\\top} Y^{(k)}_{g} + (X^{(k)})^{\\top} \\left( \\frac{1}{\\alpha_g} + Y^{(k)}_{g} \\right) \\frac{\\mu^{(k)}_{g}}{\\frac{1}{\\alpha_g} + \\mu^{(k)}_{g}}\\), where \\(\\mu^{(k)}_{g}\\) and \\(Y^{(k)}_{g}\\) are both vectors in \\(\\mathbb{R}^{n_k}\\) and \\(\\alpha_g\\) is a scalar. TODO ref equation. Each center Server 1 local_fisher nparray \\((1, G_{\\texttt{act}}, p, p)\\) For each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion, \\(n_k\\mathcal{I}^{(k)}_{g} = (X^{(k)})^{\\top} W^{(k)}_g X^{(k)}\\) where \\(W^{(k)}_g \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{gii} = \\frac{\\mu^{(k)}_{ig}}{1 + \\mu^{(k)}_{ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot \\beta_g)\\). Each center Server 1 local_nll nparray \\((1, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\). Each center Server 1 beta nparray \\((G_{\\texttt{nz}}, p)\\) For evert gene \\(g\\), the inital value of the \\(\\beta_g\\) parameter for the ProxQuasiNewton algorithm (PQN), which is either i) the value computed by IRLS if IRLS has converged on that gene (in that case, the gene will not be optimized), or ii) the initial value of the \\(\\beta_g\\) parameter for the IRLS algorithm, if IRLS has not converged on that gene. Each center Server 1 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Initialized to \\(\\texttt{False}\\). Each center Server 2 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 2 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 2 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 2 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes. It is computed as \\(\\nu_g = \\Delta_g  \\cdot \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) where \\(\\Delta_g\\) is the ascent direction on the active genes and \\(\\mathcal{L}^{\\lambda}\\) is the regularized log likelihood. Server Center 2 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes. To compute the ascent direction, we first compute the global Fisher information matrix for each gene \\(g\\), \\(n \\mathcal{I}_g = \\sum_k n_k \\mathcal{I}^{(k)}_{g}\\) and the global gradient \\(\\nabla_{\\beta} \\mathcal{L}(\\beta_g) = \\sum_k \\nabla_{\\beta} \\mathcal{L}^{(k)}(\\beta_g)\\). From these two quantities, we build the Fisher information and gradients of the regularized negative log likelihood, with \\(L^2\\) regularization \\(\\lambda = 10^{-6}\\). We add another regularization term to the Fisher matrix which depends on the iteration number. The goal is for the ascent direction to be close to the gradient for small iterations, and to be close to the real natural gradient descent update near the end of the optimization. The ascent direction is then computed as from the regularized Fisher and gradient (see main paper). Roughly, it is the solution of the linear system \\(n~\\mathcal{I}^{\\lambda}_g \\Delta_g = \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\), where certain components are dropped in to handle boundary conditions as the optimization is constrained to values of \\(\\beta_g\\) in \\([-\\texttt{max\\_beta},\\texttt{max\\_beta}]\\). Server Center 2 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Server Center 2 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which remains unchanged during the first iteration of the Proximal Quasi-Newton algorithm. Server Center 2 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Server Center 3 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 3 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 3 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 3 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 3 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 3 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 3 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 3 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 3 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 3 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 3 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 4 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed as the sum of the local negative log likelihoods with \\(L^2\\) regularization with \\(\\lambda = 10^{-6}\\). Server Center 4 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center 4 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, incremented by \\(1\\). Server Center 4 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. For more details, see description of 19. Server Center 4 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the new active genes \\(g\\), computed at the new \\(\\beta_g\\) iterate. Server Center 4 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. A gene \\(g\\) is considered to have diverged if for all step sizes \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and for the computed ascent direction \\(\\Delta_g\\), the Armijo condition is not satisfied, i.e., if \\(\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_g - \\delta \\Delta_g) \\leq \\delta ~ \\texttt{PQN\\_c1} ~ \\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g) \\cdot \\Delta_g\\) where \\(\\mathcal{L}^{\\lambda}\\) is the regularized negative log likelihood, \\(\\texttt{PQN\\_c1}\\) is the Armijo parameter which can be set by the user (default is \\(10^{-4}\\)), and \\(\\nabla_{\\beta} \\mathcal{L}^{\\lambda}(\\beta_g)\\) is the gradient of the regularized negative log likelihood. Server Center 4 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes). Genes that had converged with the relative error criterion or diverged (no stepsize satisfying the Armijo condition) have set to \\(\\texttt{False}\\). Server Center 4 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 5 round_number_PQN int The current round number of the Proximal Quasi-Newton algorithm, passed on without modification. Each center Server 5 newton_decrement_on_mask nparray \\((G_{\\texttt{act}},)\\) The newton decrement on the active genes, passed on without modification. Each center Server 5 global_reg_nll nparray \\((G_{\\texttt{nz}},)\\) The regularized negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, simply passed on. Each center Server 5 PQN_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes are currently being optimized by the Proximal Quasi-Newton algorithm (active genes),  passed on without modification. Each center Server 5 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\), which is simply passed on. Each center Server 5 local_gradient nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p)\\) For each potential step size \\(\\delta\\) and each gene \\(g\\), the gradient of the negative log likelihood of the negative binomial GLM with fixed dispersion at the \\(\\beta_g - \\delta \\Delta_g\\) parameter, computed on the local data. Each center Server 5 local_fisher nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}}, p, p)\\) For each potential step size \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\), and each gene \\(g\\), the Fisher information matrix of the negative binomial GLM with fixed dispersion computed at \\(\\beta_g - \\delta \\Delta_g\\). Formally, \\(n_k \\mathcal{I}^{(k)}_{\\delta,g} = (X^{(k)})^{\\top} W^{(k)}_{\\delta,g} X^{(k)}\\) where \\(W^{(k)}_{\\delta,g} \\in \\mathbb{R}^{n_k \\times n_k}\\) is the diagonal matrix with diagonal entries \\(W^{(k)}_{\\delta,gii} = \\frac{\\mu^{(k)}_{\\delta, ig}}{1 + \\mu^{(k)}_{\\delta, ig} \\alpha_g}\\) for \\(1 \\leq i \\leq n_k\\). \\(\\alpha_g\\) is the dispersion estimate of the gene and \\(\\mu^{(k)}_{\\delta,ig}\\) is the expected value of the gene \\(g\\) for sample \\(i\\) for parameter \\(\\beta_g - \\delta \\Delta_g\\), that is \\(\\gamma^{(k)}_{g} \\exp(X^{(k)}_{i} \\cdot (\\beta_g - \\delta \\Delta_g))\\). Each center Server 5 local_nll nparray \\((N_{\\texttt{ls}}, G_{\\texttt{act}})\\) The nll of the negative binomial GLM with fixed dispersion at the \\(\\beta_g\\) parameter, computed on the local data, for each gene \\(g\\) and for each potential next step \\(\\beta_g - \\delta \\Delta_g\\) for \\(\\delta \\in \\{1, 1/2, 1/4, ..., 1/2^{N_{\\texttt{ls}}-1}\\}\\). Each center Server 5 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Each center Server 5 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. Passed on without modification. Each center Server 5 ascent_direction_on_mask nparray \\((G_{\\texttt{act}}, p)\\) The ascent direction on the active genes, passed on without modification. Each center Server 6 PQN_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the Proximal Quasi-Newton algorithm to diverge. As this is the last step of the optimization, a gene \\(g\\) is considered to have diverged not only if the Armijo condition is never satisfied, but also if the convergence criterion is not met. A gene is said to converge if the relative difference between two successive iterates is smaller than a threshold value \\(\\texttt{PQN\\_ftol}\\), i.e., \\(\\left\\|\\mathcal{L}^{\\lambda}(\\beta_g) - \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}})\\right\\| \\leq \\texttt{PQN\\_ftol} \\max(\\mathcal{L}^{\\lambda}(\\beta_g), \\mathcal{L}^{\\lambda}(\\beta_{g,\\text{prev}}),1)\\). Server Center 6 beta nparray \\((G_{\\texttt{nz}}, p)\\) For each gene \\(g\\), the log fold change \\(\\beta_g\\). This value has been updated by the Proximal Quasi-Newton algorithm for all active genes where an admissible step size was found using the Armijo condition (see equation 2.5 of \\citep{kim2010tackling}). Server Center 6 irls_diverged_mask nparray \\((G_{\\texttt{nz}},)\\) A boolean array indicating which non zero genes have caused the IRLS algorithm to diverge. Passed on without modification. Server Center"},{"location":"generated/gallery/","title":"Tutorials","text":"<p> FedPyDESeq2 demo on the TCGA-LUAD dataset. </p> <p> Download all examples in Python source code: gallery_python.zip</p> <p> Download all examples in Jupyter notebooks: gallery_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/mg_execution_times/","title":"Computation times","text":"<p>38:33.045 total execution time for generated_gallery files:</p> <p>+----------------------------------------------------------+-----------+--------+ | plot_demo (docs/examples/plot_demo.py) | 38:33.045 | 0.0 MB | +----------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/plot_demo/","title":"FedPyDESeq2 demo on the TCGA-LUAD dataset.","text":"<p>Note</p> <p>Click here to download the full example code</p> <p>FedPyDESeq2 demo on the TCGA-LUAD dataset.</p> <p>This example demonstrates how to run a FedPyDESeq2 experiment on the TCGA-LUAD dataset from a single machine, using Substra's simulation mode.</p> <p>We will show how to perform a simple differential expression analysis, comparing samples with <code>\"Advanced\"</code> vs <code>\"Non-advanced\"</code> tumoral <code>stage</code>.</p> <pre><code>from pathlib import Path\n\nimport pandas as pd\nfrom fedpydeseq2_datasets.process_and_split_data import setup_tcga_dataset\nfrom IPython.display import display\n\nfrom fedpydeseq2.fedpydeseq2_pipeline import run_fedpydeseq2_experiment\n</code></pre>"},{"location":"generated/gallery/plot_demo/#dataset-setup","title":"Dataset setup","text":"<p>In a real federated setup, the data is distributed across multiple medical centers and must be registered with Substra beforehand. Hence, each center would have a folder containing two csvs (one fore the counts and one for the metadata), as well as an opener python file and a markdown readme file (see Substra's documentation on how to register a datasample). Then, we would only need pass the <code>dataset_datasample_keys path</code>.</p> <p>In this tutorial, however, we use FedPyDESeq2's simulation mode, which allows us to emulate a federated setup from a single machine.</p> <p>The simulation mode assumes the data to be organized in the following structure:</p> <pre><code>processed_data_path/\n\u251c\u2500\u2500 centers_data/\n\u2502   \u2514\u2500\u2500 tcga/\n\u2502       \u2514\u2500\u2500 {experiment_id}/\n\u2502           \u251c\u2500\u2500 center_0/\n\u2502           \u2502   \u251c\u2500\u2500 counts.csv\n\u2502           \u2502   \u2514\u2500\u2500 metadata.csv\n\u2502           \u251c\u2500\u2500 center_1/\n\u2502           \u2502   \u251c\u2500\u2500 counts.csv\n\u2502           \u2502   \u2514\u2500\u2500 metadata.csv\n\u2502           \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 pooled_data/\n    \u2514\u2500\u2500 tcga/\n        \u2514\u2500\u2500 {experiment_id}/\n            \u251c\u2500\u2500 counts.csv\n            \u2514\u2500\u2500 metadata.csv\n</code></pre> <p>In this tutorial, we have already downloaded the data in the <code>data/raw</code> directory.</p> <p>The <code>setup_tcga_dataset</code> function from <code>fedpydeseq2_datasets</code> will automatically organize the data in the <code>data/processed</code> directory.</p> <p>It will split the TCGA-LUAD dataset into 7 centers according to the geographical origin of the samples, as described in the FedPyDESeq2 paper.</p> <p>See also the <code>fedpydeseq2_datasets</code> repository for more details.</p> <pre><code>dataset_name = \"TCGA-LUAD\"\nraw_data_path = Path(\"data/raw\").resolve()\nprocessed_data_path = Path(\"data/processed\").resolve()\ndesign_factors = \"stage\"\n\nsetup_tcga_dataset(\n    raw_data_path,\n    processed_data_path,\n    dataset_name=dataset_name,\n    small_samples=False,\n    small_genes=False,\n    only_two_centers=False,\n    design_factors=design_factors,\n    force=True,\n)\n\nexperiment_id = \"TCGA-LUAD-stage\"\n</code></pre> <p>Out:</p> <pre><code>2025-06-24 07:42:11.627 | INFO     | fedpydeseq2_datasets.process_and_split_data:setup_tcga_dataset:144 - Setting up TCGA dataset: TCGA-LUAD-stage\n2025-06-24 07:42:11.627 | INFO     | fedpydeseq2_datasets.process_and_split_data:setup_tcga_dataset:150 - First center metadata does not exist or force=True. Setting up the dataset.\n2025-06-24 07:42:11.628 | INFO     | fedpydeseq2_datasets.process_and_split_data:_setup_tcga_dataset:282 - Processing the data for the TCGA dataset: TCGA-LUAD-stage\n2025-06-24 07:42:14.678 | INFO     | fedpydeseq2_datasets.process_and_split_data:_setup_tcga_dataset:302 - Saving the data for each center /home/runner/work/fedpydeseq2/fedpydeseq2/docs/examples/data/processed/centers_data/tcga/TCGA-LUAD-stage\n2025-06-24 07:42:53.220 | INFO     | fedpydeseq2_datasets.process_and_split_data:_setup_tcga_dataset:364 - Saving the pooled data at /home/runner/work/fedpydeseq2/fedpydeseq2/docs/examples/data/processed/pooled_data/tcga/TCGA-LUAD-stage\n</code></pre>"},{"location":"generated/gallery/plot_demo/#running-the-experiment","title":"Running the experiment","text":"<p>We can now run the experiment.</p> <p>Substra, the FL framework on which FedPyDESeq2 is built, supports a simulated mode which may be run locally from a single machine, which we will use here.</p> <p>Let's run our FedPyDESeq2 experiment. This may be done using the <code>run_fedpydeseq2_experiment</code> wrapper function, which takes the following parameters:</p> <ul> <li> <p><code>n_centers=7</code>: Our data is distributed across 7 different medical centers</p> </li> <li> <p><code>backend=\"subprocess\"</code> and <code>simulate=True</code>: We'll run the analysis locally on our   machine to simulate a federated setup, rather than in a real distributed environment</p> </li> <li> <p><code>register_data=True</code>: We'll register our dataset with Substra before analysis.   In the case of a real federated setup, this would be set to <code>False</code> if data was   already registered by Substra.</p> </li> <li> <p><code>asset_directory</code>: This directory should contain an opener.py file, containing an   Opener class, and datasets.description.md file. Here, we copy them from   <code>fedpydeseq2_datasets/assets/tcga</code></p> </li> <li> <p><code>centers_root_directory</code>: Where the processed data for each center is stored</p> </li> <li> <p><code>compute_plan_name</code>: We'll call this analysis \"Example-TCGA-LUAD-pipeline\"   in Substra</p> </li> <li> <p><code>dataset_name</code>: We're working with the TCGA-LUAD lung cancer dataset</p> </li> <li> <p><code>design_factors</code>: This should be a list of the design factors we wish to include in   our analysis. Here, we're studying how \"stage\" (the cancer stage) affects gene   expression</p> </li> <li> <p><code>ref_levels</code>: We're setting \"Non-advanced\" as our baseline cancer stage</p> </li> <li> <p><code>contrast</code>: This should be a list of three strings, of the form  <code>[\"factor\", \"alternative_level\", \"baseline_level\"]</code>. To compare gene expression   between \"Advanced\" vs \"Non-advanced\" stages, we set  <code>contrast=[\"stage\", \"Advanced\", \"Non-advanced\"]</code>.</p> </li> <li> <p><code>refit_cooks=True</code>: After finding outliers using Cook's distance, we'll refit the   model without them for more robust results</p> </li> </ul> <pre><code>fl_results = run_fedpydeseq2_experiment(\n    n_centers=7,\n    backend=\"subprocess\",\n    simulate=True,\n    register_data=True,\n    asset_directory=Path(\"assets/tcga\").resolve(),\n    centers_root_directory=processed_data_path\n    / \"centers_data\"\n    / \"tcga\"\n    / experiment_id,\n    compute_plan_name=\"Example-TCGA-LUAD-pipeline\",\n    dataset_name=\"TCGA-LUAD\",\n    design_factors=\"stage\",\n    ref_levels={\"stage\": \"Non-advanced\"},\n    contrast=[\"stage\", \"Advanced\", \"Non-advanced\"],\n    refit_cooks=True,\n)\n</code></pre> <p>Out:</p> <pre><code>2025-06-24 07:43:31.488 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:182 - Setting up organizations...\n2025-06-24 07:43:31.490 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:233 - Registering the datasets...\n2025-06-24 07:43:31.490 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg2MSP\n2025-06-24 07:43:31.491 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: e0f3b562-296f-45e4-9396-f1238eb58a13\n2025-06-24 07:43:31.492 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg3MSP\n2025-06-24 07:43:31.492 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: 6da35e45-07e0-45b3-b99f-cce57c2c8424\n2025-06-24 07:43:31.493 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg4MSP\n2025-06-24 07:43:31.493 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: 5e86bd61-cde7-48f9-a666-9b333e9a3a48\n2025-06-24 07:43:31.493 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg5MSP\n2025-06-24 07:43:31.494 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: dceea926-cdba-482e-8a14-b9c6719efed8\n2025-06-24 07:43:31.494 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg6MSP\n2025-06-24 07:43:31.495 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: b6c0ad4d-c167-4be8-951d-32dd4fa8fa18\n2025-06-24 07:43:31.495 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg7MSP\n2025-06-24 07:43:31.495 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: a10d950a-1b25-427d-a5fc-a2be88f7e159\n2025-06-24 07:43:31.496 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:290 - Adding dataset to client MyOrg8MSP\n2025-06-24 07:43:31.496 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:295 - Dataset added. Key: 71268858-2a9b-4598-8137-41c743826a2d\n2025-06-24 07:43:31.496 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:316 - Datasets registered.\n2025-06-24 07:43:31.496 | INFO     | fedpydeseq2.substra_utils.federated_experiment:run_federated_experiment:318 - Dataset keys: {'MyOrg2MSP': 'e0f3b562-296f-45e4-9396-f1238eb58a13', 'MyOrg3MSP': '6da35e45-07e0-45b3-b99f-cce57c2c8424', 'MyOrg4MSP': '5e86bd61-cde7-48f9-a666-9b333e9a3a48', 'MyOrg5MSP': 'dceea926-cdba-482e-8a14-b9c6719efed8', 'MyOrg6MSP': 'b6c0ad4d-c167-4be8-951d-32dd4fa8fa18', 'MyOrg7MSP': 'a10d950a-1b25-427d-a5fc-a2be88f7e159', 'MyOrg8MSP': '71268858-2a9b-4598-8137-41c743826a2d'}\n2025-06-24 07:43:55,740 - INFO - Simulating the execution of the compute plan.\n2025-06-24 07:43:55.741 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:66 - Building design matrices...\n2025-06-24 07:43:56.183 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:76 - Finished building design matrices.\n2025-06-24 07:43:56.183 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:82 - Computing size factors...\n2025-06-24 07:43:56.631 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:97 - Finished computing size factors.\n2025-06-24 07:43:56.632 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:101 - Running LFC and dispersions.\n2025-06-24 07:43:56.632 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:94 - Fit genewise dispersions...\n2025-06-24 07:49:06.711 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:108 - Finished fitting genewise dispersions.\n2025-06-24 07:49:06.712 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:112 - Compute dispersion prior...\n2025-06-24 07:49:09.886 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:125 - Finished computing dispersion prior.\n2025-06-24 07:49:09.886 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:145 - Fit MAP dispersions...\n2025-06-24 07:52:52.629 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:158 - Finished fitting MAP dispersions.\n2025-06-24 07:52:52.629 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:161 - Compute log fold changes...\n2025-06-24 07:56:43.133 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:171 - Finished computing log fold changes.\n2025-06-24 07:56:43.134 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:112 - Finished running LFC and dispersions.\n2025-06-24 07:56:43.134 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:114 - Computing Cook distances...\n2025-06-24 08:01:07.624 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:128 - Finished computing Cook distances.\n2025-06-24 08:01:07.624 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:132 - Refitting Cook outliers...\n2025-06-24 08:03:33.763 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:94 - Fit genewise dispersions...\n2025-06-24 08:11:23.314 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:108 - Finished fitting genewise dispersions.\n2025-06-24 08:11:26.877 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:145 - Fit MAP dispersions...\n2025-06-24 08:12:08.064 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:158 - Finished fitting MAP dispersions.\n2025-06-24 08:12:08.064 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:161 - Compute log fold changes...\n2025-06-24 08:19:42.188 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_lfc_dispersions.deseq2_lfc_dispersions:run_deseq2_lfc_dispersions:171 - Finished computing log fold changes.\n2025-06-24 08:19:46.047 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:164 - Finished refitting Cook outliers.\n2025-06-24 08:19:46.048 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:168 - Running DESeq2 statistics.\n2025-06-24 08:19:46.048 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:64 - Running Wald tests.\n2025-06-24 08:19:59.428 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:74 - Finished running Wald tests.\n2025-06-24 08:19:59.428 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:77 - Running Cook's filtering...\n2025-06-24 08:20:24.318 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:86 - Finished running Cook's filtering.\n2025-06-24 08:20:24.318 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:87 - Computing adjusted p-values...\n2025-06-24 08:20:30.188 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_stats.deseq2_stats:run_deseq2_stats:99 - Finished computing adjusted p-values.\n2025-06-24 08:20:30.188 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:178 - Finished running DESeq2 statistics.\n2025-06-24 08:20:30.188 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:182 - Saving pipeline results.\n2025-06-24 08:20:34.190 | INFO     | fedpydeseq2.core.deseq2_core.deseq2_full_pipe:run_deseq_pipe:191 - Finished saving pipeline results.\n2025-06-24 08:20:34,190 - INFO - Experiment summary saved to /tmp/tmpcmxzzy1b/2025_06_24_07_43_55_simu-15d0c308-b532-4be1-8a2a-c3bdb1312b18.json\n2025-06-24 08:20:34,191 - INFO - The compute plan has been simulated, its key is simu-15d0c308-b532-4be1-8a2a-c3bdb1312b18.\n</code></pre>"},{"location":"generated/gallery/plot_demo/#results","title":"Results","text":"<p>The results are then stored in a <code>fl_results</code> dictionary, which does not contain any individual sample information.</p> <pre><code>fl_results.keys()\n</code></pre> <p>Out:</p> <pre><code>dict_keys(['gene_names', 'MAP_dispersions', 'dispersions', 'genewise_dispersions', 'non_zero', 'fitted_dispersions', 'LFC', 'padj', 'p_values', 'wald_statistics', 'wald_se', 'replaced', 'refitted', 'prior_disp_var', '_squared_logres', 'contrast'])\n</code></pre> <p>We can then extract the results for our contrast of interest, and store them in a pandas DataFrame.</p> <pre><code>res_df = pd.DataFrame()\nres_df[\"LFC\"] = fl_results[\"LFC\"][\"stage_Advanced_vs_Non-advanced\"]\nres_df[\"pvalue\"] = fl_results[\"p_values\"]\nres_df[\"padj\"] = fl_results[\"padj\"]\n\nres_df = res_df.loc[fl_results[\"non_zero\"], :]\n</code></pre> <pre><code>display(res_df)\n</code></pre> <p>Out:</p> <pre><code>                      LFC    pvalue      padj\nENSG00000223972  0.338364  0.052181  0.253126\nENSG00000278267  0.038183  0.821323  0.937148\nENSG00000227232  0.104173  0.207068  0.516585\nENSG00000284332  0.174178  0.742368       NaN\nENSG00000243485  0.291516  0.192281  0.497940\n...                   ...       ...       ...\nENSG00000215506 -0.585729  0.145101       NaN\nENSG00000227629  0.074826  0.933744       NaN\nENSG00000231514  0.026989  0.853410  0.948483\nENSG00000237917  0.118084  0.668620  0.867462\nENSG00000235857 -1.069862  0.247121       NaN\n\n[57832 rows x 3 columns]\n</code></pre> <p>Total running time of the script: ( 38 minutes  33.045 seconds)</p> <p> Download Python source code: plot_demo.py</p> <p> Download Jupyter notebook: plot_demo.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"usage/contributing/","title":"Contributing","text":""},{"location":"usage/contributing/#setup","title":"Setup","text":""},{"location":"usage/contributing/#1-installing-the-repository-with-the-development-dependencies","title":"1- Installing the repository with the development dependencies","text":"<p>As in the installation page, start by creating a conda environment with the right dependencies.</p> <ol> <li> <p>Create a conda environment <pre><code>conda create -n fedpydeseq2 python=3.10 # or a higher python version\nconda activate fedpydeseq2\n</code></pre></p> </li> <li> <p>Add poetry <pre><code>conda install pip\npip install poetry==1.8.2\n</code></pre></p> </li> </ol> <p>and test the installation with <code>poetry --version</code>.</p> <ol> <li>Install the package with <code>testing,linting,docs</code> dependencies.</li> </ol> <p><code>cd</code> to the root of the repository and run</p> <pre><code>poetry install --with linting,testing,docs\n</code></pre>"},{"location":"usage/contributing/#2-install-pre-commit-hooks","title":"2 - Install <code>pre-commit</code> hooks","text":"<p>Still in the root of the repository, run</p> <p><code>pre-commit install</code></p> <p>You are now ready to contribute.</p>"},{"location":"usage/installation/","title":"Installation","text":""},{"location":"usage/installation/#1-create-a-conda-environment-with-python-310","title":"1 - Create a conda environment with python 3.10+","text":"<pre><code>conda create -n fedpydeseq2 python=3.10 # or a higher python version\nconda activate fedpydeseq2\n</code></pre>"},{"location":"usage/installation/#2-install-poetry","title":"2 - Install <code>poetry</code>","text":"<p>Run</p> <pre><code>conda install pip\npip install poetry==1.8.2\n</code></pre> <p>and test the installation with <code>poetry --version</code>.</p>"},{"location":"usage/installation/#3-install-the-package-and-its-dependencies-using-poetry","title":"3 - Install the package and its dependencies using <code>poetry</code>","text":"<p><code>cd</code> to the root of the repository and run</p> <pre><code>poetry install\n</code></pre> <p>If you want to install to contribute, please run</p> <pre><code>poetry install --with linting,testing\n</code></pre> <p>If you wish to manually create the documentation, please run</p> <pre><code>poetry install --with docs\n</code></pre> <p>If you want to run the paper experiments, please visit https://github.com/owkin/fedpydeseq2-paper-experiments.</p>"},{"location":"usage/installation_developers/","title":"Package installation for developers","text":""},{"location":"usage/installation_developers/#0-clone-the-repository","title":"0 - Clone the repository","text":"<p>Start by cloning this repository</p> <pre><code>git clone git@github.com:owkin/fedpydeseq2.git\n</code></pre>"},{"location":"usage/installation_developers/#1-create-a-conda-environment-with-python-310","title":"1 - Create a conda environment with python 3.10+","text":"<pre><code>conda create -n fedpydeseq2 python=3.11 # or a version compatible\nconda activate fedpydeseq2\n</code></pre>"},{"location":"usage/installation_developers/#2-install-poetry","title":"2 - Install <code>poetry</code>","text":"<p>Run</p> <pre><code>conda install pip\npip install poetry==1.8.2\n</code></pre> <p>and test the installation with <code>poetry --version</code>.</p>"},{"location":"usage/installation_developers/#3-install-the-package-and-its-dependencies-using-poetry","title":"3 - Install the package and its dependencies using <code>poetry</code>","text":"<p><code>cd</code> to the root of the repository and run</p> <pre><code>poetry install --with linting,testing\n</code></pre>"},{"location":"usage/installation_developers/#4-download-the-data-to-run-the-tests-on","title":"4 - Download the data to run the tests on","text":"<p>To download the data, <code>cd</code> to the root of the repository run this command.</p> <pre><code>fedpydeseq2-download-data --raw_data_output_path data/raw\n</code></pre> <p>This way, you create a <code>data/raw</code> subdirectory in the directory containing all the necessary data. If you want to modify the location of this raw data, you can in the following way. Run this command instead:</p> <pre><code>fedpydeseq2-download-data --raw_data_output_path MY_RAW_PATH\n</code></pre> <p>And create a file in the <code>tests</code> directory named <code>paths.json</code> containing - A <code>raw_data</code> field with the path to the raw data <code>MY_RAW_PATH</code> - An optional <code>assets_tcga</code> field with the path to the directory containing the <code>opener.py</code> file and its description (by default present in the fedpydeseq2_datasets module, so no need to specify this unless you need to modify the opener). - An optional <code>processed_data</code> field with the path to the directory where you want to save processed data. This is used if you want to run tests locally without reprocessing the data during each test session. Otherwise, the processed data will be saved in a temporary file during each test session. - An optional <code>default_logging_config</code> field with the path to the logging configuration used by default in tests. For more details on how logging works, please refer to this dedicated section - An optional <code>workflow_logging_config</code> field with the path to the logging configuration used in the logging tests.</p>"},{"location":"usage/logging/","title":"Logging","text":"<p>This section is useful for people who wish do get an in depth understanding of what each <code>remote</code> and <code>remote_data</code> functions have as inputs and outputs. This supposes a good understanding both of the code structure, of the <code>Substra</code> way of doing things, as well as that of the algorithm. The goal of the logging module is to precisely track the behaviour of functions in the <code>subprocess</code> mode (i.e., not in a real federated setting but in simulation).</p>"},{"location":"usage/logging/#overview","title":"Overview","text":"<p>Instructions on how to configure logging using JSON and INI files. The logging configuration is divided into two parts: 1. A JSON file that specifies the path to the INI file and additional logging options. 2. An INI file that defines the logger, handler, and formatter configurations.</p>"},{"location":"usage/logging/#json-configuration","title":"JSON Configuration","text":"<p>The JSON configuration file contains the following keys:</p> <ul> <li><code>logger_configuration_ini_path</code>: The path to the INI file that contains the logger configuration.</li> <li><code>generate_workflow</code>: A dictionary with the following keys:</li> <li><code>create_workflow</code>: A boolean indicating whether to create a workflow.</li> <li><code>workflow_file_path</code>: The path to the workflow file.</li> <li><code>clean_workflow_file</code>: A boolean indicating whether to clean the workflow file.</li> <li><code>log_shared_state_adata_content</code>: A boolean indicating whether to log the shared state adata content.</li> <li><code>log_shared_state_size</code>: A boolean indicating whether to log the shared state size.</li> </ul>"},{"location":"usage/logging/#example-json-configuration","title":"Example JSON Configuration","text":"<pre><code>{\n    \"logger_configuration_ini_path\": \"/path/to/logger_configuration_template.ini\",\n    \"generate_workflow\": {\n        \"create_workflow\": false,\n        \"workflow_file_path\": \"/path/to/workflow.txt\",\n        \"clean_workflow_file\": true\n    },\n    \"log_shared_state_adata_content\": false,\n    \"log_shared_state_size\": false\n}\n</code></pre>"},{"location":"usage/logging/#ini-configuration","title":"INI Configuration","text":"<p>The INI configuration file defines the loggers, handlers, and formatters. Below is an example of the INI file format:</p>"},{"location":"usage/logging/#example-ini-configuration","title":"Example INI Configuration","text":"<pre><code>; Description: This is a template for the configuration of the logger\n\n; Define the loggers\n[loggers]\nkeys=root\n\n; Define the handlers\n[handlers]\nkeys=consoleHandler\n\n; Define the formatters\n[formatters]\nkeys=sampleFormatter\n\n; Root logger configuration\n[logger_root]\nlevel=WARNING\nhandlers=consoleHandler\n\n; Console handler configuration\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=WARNING\nformatter=sampleFormatter\nargs=(sys.stdout,)\n\n; Sample formatter configuration\n[formatter_sampleFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n</code></pre>"},{"location":"usage/logging/#purpose","title":"Purpose","text":""},{"location":"usage/logging/#workflow","title":"Workflow","text":"<p>A workflow is simply a text file that logs the different shared states during subprocess mode execution. This feature is deactivated in other Substra backends. The workflow file helps in tracking the sequence of shared states and their transitions.</p>"},{"location":"usage/logging/#log_shared_state_adata_content","title":"log_shared_state_adata_content","text":"<p>The log_shared_state_adata_content flag determines whether to log the shared state keys and local adata keys to a file. This can be useful for debugging and tracking the data being processed.</p>"},{"location":"usage/logging/#log_shared_state_size","title":"log_shared_state_size","text":"<p>The log_shared_state_size flag determines whether to evaluate the size of the shared state. This is used for memory management purposes, helping to ensure that the application does not exceed memory limits.</p>"},{"location":"usage/logging/#usage","title":"Usage","text":"<ol> <li>Create a JSON configuration file with the required keys and values.</li> <li>Create an INI configuration file with the logger, handler, and formatter definitions.</li> <li>Ensure that the <code>logger_configuration_ini_path</code> in the JSON file points to the correct INI file path.</li> <li>Use the JSON configuration file to initialize the logging configuration in your application.</li> </ol> <p>By following these steps, you can configure logging in your application using JSON and INI files.</p>"},{"location":"usage/references/","title":"References","text":"<ol> <li> <p>Michael I Love, Wolfgang Huber, and Simon Anders. Moderated estimation of fold change and dispersion for rna-seq data with deseq2. Genome biology, 15(12):1\u201321, 2014. doi:10.1186/s13059-014-0550-8.\u00a0\u21a9</p> </li> <li> <p>Boris Muzellec, Maria Telenczuk, Vincent Cabeli, and Mathieu Andreux. Pydeseq2: a python package for bulk rna-seq differential expression analysis. Bioinformatics, 2023. doi:10.1093/bioinformatics/btad547.\u00a0\u21a9</p> </li> </ol>"}]}